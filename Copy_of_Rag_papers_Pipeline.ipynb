{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AreebAhmad-02/Real-time-rag-pipeline-for-rag-research-papers/blob/main/Copy_of_Rag_papers_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install arxiv -q -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48c586JBHwfQ",
        "outputId": "b8f9c0e4-f9d3-4383-9abb-7d9f9f0f3d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
{
  "cell_type": "markdown",
  "metadata": {
    "id": "HeadingID"
  },
  "source": [
    "# Getting Data Using Arxiv API for Past 2 Years of RAG Pipeline"
  ]
},
    {
      "cell_type": "code",
      "source": [
        "! pip install -q feedparser"
      ],
      "metadata": {
        "id": "R0WNjq06PJ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDgVcpZRF5J1",
        "outputId": "193b9dd2-7e6e-4d2d-a209-1ef5978f7bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "abs page link: http://arxiv.org/abs/2401.10286v3\n",
            "pdf link: http://arxiv.org/pdf/2401.10286v3\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: In previous studies, code-based models have consistently outperformed\n",
            "text-based models in reasoning-intensive scenarios. When generating our\n",
            "knowledge base for Retrieval-Augmented Generation (RAG), we observed that\n",
            "code-based models also perform exceptionally well in Chinese QA Pair Extraction\n",
            "task. Further, our experiments and the metrics we designed discovered that\n",
            "code-based models containing a certain amount of Chinese data achieve even\n",
            "better performance. Additionally, the capabilities of code-based English models\n",
            "in specified Chinese tasks offer a distinct perspective for discussion on the\n",
            "philosophical \"Chinese Room\" thought experiment.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.04206v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-06T18:01:29Z\n",
            "Title:  Explaining Autonomy: Enhancing Human-Robot Interaction through\n",
            "  Explanation Generation with Large Language Models\n",
            "Last Author:  Vicente Matellán-Olivera\n",
            "Authors:  David Sobrín-Hidalgo, Miguel A. González-Santamarta, Ángel M. Guerrero-Higueras, Francisco J. Rodríguez-Lera, Vicente Matellán-Olivera\n",
            "abs page link: http://arxiv.org/abs/2402.04206v1\n",
            "pdf link: http://arxiv.org/pdf/2402.04206v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 26 pages, 15 Figures, 11 Tables. This paper is a preprint of an\n",
            "  article submitted to the International Journal of Social Robotics\n",
            "Primary Category: cs.RO\n",
            "All Categories: cs.RO\n",
            "Abstract: This paper introduces a system designed to generate explanations for the\n",
            "actions performed by an autonomous robot in Human-Robot Interaction (HRI).\n",
            "Explainability in robotics, encapsulated within the concept of an eXplainable\n",
            "Autonomous Robot (XAR), is a growing research area. The work described in this\n",
            "paper aims to take advantage of the capabilities of Large Language Models\n",
            "(LLMs) in performing natural language processing tasks. This study focuses on\n",
            "the possibility of generating explanations using such models in combination\n",
            "with a Retrieval Augmented Generation (RAG) method to interpret data gathered\n",
            "from the logs of autonomous systems. In addition, this work also presents a\n",
            "formalization of the proposed explanation system. It has been evaluated through\n",
            "a navigation test from the European Robotics League (ERL), a Europe-wide social\n",
            "robotics competition. Regarding the obtained results, a validation\n",
            "questionnaire has been conducted to measure the quality of the explanations\n",
            "from the perspective of technical users. The results obtained during the\n",
            "experiment highlight the potential utility of LLMs in achieving explanatory\n",
            "capabilities in robots.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.09906v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-15T12:12:19Z\n",
            "Title:  Generative Representational Instruction Tuning\n",
            "Last Author:  Douwe Kiela\n",
            "Authors:  Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela\n",
            "abs page link: http://arxiv.org/abs/2402.09906v2\n",
            "pdf link: http://arxiv.org/pdf/2402.09906v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 66 pages (16 main), 25 figures, 34 tables\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: All text-based language problems can be reduced to either generation or\n",
            "embedding. Current models only perform well at one or the other. We introduce\n",
            "generative representational instruction tuning (GRIT) whereby a large language\n",
            "model is trained to handle both generative and embedding tasks by\n",
            "distinguishing between them through instructions. Compared to other open\n",
            "models, our resulting GritLM 7B sets a new state of the art on the Massive Text\n",
            "Embedding Benchmark (MTEB) and outperforms all models up to its size on a range\n",
            "of generative tasks. By scaling up further, GritLM 8x7B outperforms all open\n",
            "generative language models that we tried while still being among the best\n",
            "embedding models. Notably, we find that GRIT matches training on only\n",
            "generative or embedding data, thus we can unify both at no performance loss.\n",
            "Among other benefits, the unification via GRIT speeds up Retrieval-Augmented\n",
            "Generation (RAG) by > 60% for long documents, by no longer requiring separate\n",
            "retrieval and generation models. Models, code, etc. are freely available at\n",
            "https://github.com/ContextualAI/gritlm.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.10790v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-16T16:15:01Z\n",
            "Title:  In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs\n",
            "  Miss\n",
            "Last Author:  Mikhail Burtsev\n",
            "Authors:  Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev\n",
            "abs page link: http://arxiv.org/abs/2402.10790v2\n",
            "pdf link: http://arxiv.org/pdf/2402.10790v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 11M tokens, fix qa3 min facts per task in Table 1\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: This paper addresses the challenge of processing long documents using\n",
            "generative transformer models. To evaluate different approaches, we introduce\n",
            "BABILong, a new benchmark designed to assess model capabilities in extracting\n",
            "and processing distributed facts within extensive texts. Our evaluation, which\n",
            "includes benchmarks for GPT-4 and RAG, reveals that common methods are\n",
            "effective only for sequences up to $10^4$ elements. In contrast, fine-tuning\n",
            "GPT-2 with recurrent memory augmentations enables it to handle tasks involving\n",
            "up to $11\\times 10^6$ elements. This achievement marks a substantial leap, as\n",
            "it is by far the longest input processed by any neural network model to date,\n",
            "demonstrating a significant improvement in the processing capabilities for long\n",
            "sequences.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.11035v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-16T19:28:52Z\n",
            "Title:  Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?\n",
            "Last Author:  Larry Heck\n",
            "Authors:  Benjamin Reichman, Larry Heck\n",
            "abs page link: http://arxiv.org/abs/2402.11035v2\n",
            "pdf link: http://arxiv.org/pdf/2402.11035v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.IR\n",
            "Abstract: Dense passage retrieval (DPR) is the first step in the retrieval augmented\n",
            "generation (RAG) paradigm for improving the performance of large language\n",
            "models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of\n",
            "the embeddings between queries and relevant textual data. A deeper\n",
            "understanding of DPR fine-tuning will be required to fundamentally unlock the\n",
            "full potential of this approach. In this work, we explore DPR-trained models\n",
            "mechanistically by using a combination of probing, layer activation analysis,\n",
            "and model editing. Our experiments show that DPR training decentralizes how\n",
            "knowledge is stored in the network, creating multiple access pathways to the\n",
            "same information. We also uncover a limitation in this training style: the\n",
            "internal knowledge of the pre-trained model bounds what the retrieval model can\n",
            "retrieve. These findings suggest a few possible directions for dense retrieval:\n",
            "(1) expose the DPR training process to more knowledge so more can be\n",
            "decentralized, (2) inject facts as decentralized representations, (3) model and\n",
            "incorporate knowledge uncertainty in the retrieval process, and (4) directly\n",
            "map internal model knowledge to a knowledge base.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.11166v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-17T02:21:44Z\n",
            "Title:  GenDec: A robust generative Question-decomposition method for Multi-hop\n",
            "  reasoning\n",
            "Last Author:  Manabu Okumura\n",
            "Authors:  Jian Wu, Linyi Yang, Yuliang Ji, Wenhao Huang, Börje F. Karlsson, Manabu Okumura\n",
            "abs page link: http://arxiv.org/abs/2402.11166v1\n",
            "pdf link: http://arxiv.org/pdf/2402.11166v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex\n",
            "questions and find multiple relevant supporting facts. However, Existing large\n",
            "language models'(LLMs) reasoning ability in multi-hop question answering\n",
            "remains exploration, which is inadequate in answering multi-hop questions.\n",
            "Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach\n",
            "the right final answer. In this paper, we propose a \\textbf{gen}erative\n",
            "question \\textbf{dec}omposition method (GenDec) from the perspective of\n",
            "explainable QA by generating independent and complete sub-questions based on\n",
            "incorporating additional extracted evidence for enhancing LLMs' reasoning\n",
            "ability in RAG. To demonstrate the impact, generalization, and robustness of\n",
            "Gendec, we conduct two experiments, the first is combining GenDec with small QA\n",
            "systems on paragraph retrieval and QA tasks. We secondly examine the reasoning\n",
            "capabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5\n",
            "combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA,\n",
            "MuSiQue, and PokeMQA datasets.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.11782v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-19T02:15:34Z\n",
            "Title:  What Evidence Do Language Models Find Convincing?\n",
            "Last Author:  Dan Klein\n",
            "Authors:  Alexander Wan, Eric Wallace, Dan Klein\n",
            "abs page link: http://arxiv.org/abs/2402.11782v1\n",
            "pdf link: http://arxiv.org/pdf/2402.11782v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.LG\n",
            "Abstract: Retrieval-augmented language models are being increasingly tasked with\n",
            "subjective, contentious, and conflicting queries such as \"is aspartame linked\n",
            "to cancer\". To resolve these ambiguous queries, one must search through a large\n",
            "range of websites and consider \"which, if any, of this evidence do I find\n",
            "convincing?\". In this work, we study how LLMs answer this question. In\n",
            "particular, we construct ConflictingQA, a dataset that pairs controversial\n",
            "queries with a series of real-world evidence documents that contain different\n",
            "facts (e.g., quantitative results), argument styles (e.g., appeals to\n",
            "authority), and answers (Yes or No). We use this dataset to perform sensitivity\n",
            "and counterfactual analyses to explore which text features most affect LLM\n",
            "predictions. Overall, we find that current models rely heavily on the relevance\n",
            "of a website to the query, while largely ignoring stylistic features that\n",
            "humans find important such as whether a text contains scientific references or\n",
            "is written with a neutral tone. Taken together, these results highlight the\n",
            "importance of RAG corpus quality (e.g., the need to filter misinformation), and\n",
            "possibly even a shift in how LLMs are trained to better align with human\n",
            "judgements.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.12317v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-19T17:37:28Z\n",
            "Title:  ARKS: Active Retrieval in Knowledge Soup for Code Generation\n",
            "Last Author:  Tao Yu\n",
            "Authors:  Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu\n",
            "abs page link: http://arxiv.org/abs/2402.12317v1\n",
            "pdf link: http://arxiv.org/pdf/2402.12317v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Retrieval-augmented code generation\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Recently the retrieval-augmented generation (RAG) paradigm has raised much\n",
            "attention for its potential in incorporating external knowledge into large\n",
            "language models (LLMs) without further training. While widely explored in\n",
            "natural language applications, its utilization in code generation remains\n",
            "under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup\n",
            "(ARKS), an advanced strategy for generalizing large language models for code.\n",
            "In contrast to relying on a single source, we construct a knowledge soup\n",
            "integrating web search, documentation, execution feedback, and evolved code\n",
            "snippets. We employ an active retrieval strategy that iteratively refines the\n",
            "query and updates the knowledge soup. To assess the performance of ARKS, we\n",
            "compile a new benchmark comprising realistic coding problems associated with\n",
            "frequently updated libraries and long-tail programming languages. Experimental\n",
            "results on ChatGPT and CodeLlama demonstrate a substantial improvement in the\n",
            "average execution accuracy of ARKS on LLMs. The analysis confirms the\n",
            "effectiveness of our proposed knowledge soup and active retrieval strategies,\n",
            "offering rich insights into the construction of effective retrieval-augmented\n",
            "code generation (RACG) pipelines. Our model, code, and data are available at\n",
            "https://arks-codegen.github.io.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.18502v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-28T17:29:27Z\n",
            "Title:  Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware\n",
            "  Classification\n",
            "Last Author:  Abhijnan Chakraborty\n",
            "Authors:  Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, Abhijnan Chakraborty\n",
            "abs page link: http://arxiv.org/abs/2402.18502v1\n",
            "pdf link: http://arxiv.org/pdf/2402.18502v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Under review\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Employing Large Language Models (LLM) in various downstream applications such\n",
            "as classification is crucial, especially for smaller companies lacking the\n",
            "expertise and resources required for fine-tuning a model. Fairness in LLMs\n",
            "helps ensure inclusivity, equal representation based on factors such as race,\n",
            "gender and promotes responsible AI deployment. As the use of LLMs has become\n",
            "increasingly prevalent, it is essential to assess whether LLMs can generate\n",
            "fair outcomes when subjected to considerations of fairness. In this study, we\n",
            "introduce a framework outlining fairness regulations aligned with various\n",
            "fairness definitions, with each definition being modulated by varying degrees\n",
            "of abstraction. We explore the configuration for in-context learning and the\n",
            "procedure for selecting in-context demonstrations using RAG, while\n",
            "incorporating fairness rules into the process. Experiments conducted with\n",
            "different LLMs indicate that GPT-4 delivers superior results in terms of both\n",
            "accuracy and fairness compared to other models. This work is one of the early\n",
            "attempts to achieve fairness in prediction tasks by utilizing LLMs through\n",
            "in-context learning.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.18510v3\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-28T17:38:06Z\n",
            "Title:  RNNs are not Transformers (Yet): The Key Bottleneck on In-context\n",
            "  Retrieval\n",
            "Last Author:  Kaifeng Lyu\n",
            "Authors:  Kaiyue Wen, Xingyu Dang, Kaifeng Lyu\n",
            "abs page link: http://arxiv.org/abs/2402.18510v3\n",
            "pdf link: http://arxiv.org/pdf/2402.18510v3\n",
            "Journal reference: No journal ref found\n",
            "Comments: 40 pages, 5 figures, fix a bug in hybrid model training\n",
            "Primary Category: cs.LG\n",
            "All Categories: cs.LG, cs.CL, stat.ML\n",
            "Abstract: This paper investigates the gap in representation powers of Recurrent Neural\n",
            "Networks (RNNs) and Transformers in the context of solving algorithmic\n",
            "problems. We focus on understanding whether RNNs, known for their memory\n",
            "efficiency in handling long sequences, can match the performance of\n",
            "Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.\n",
            "Our theoretical analysis reveals that CoT improves RNNs but is insufficient to\n",
            "close the gap with Transformers. A key bottleneck lies in the inability of RNNs\n",
            "to perfectly retrieve information from the context, even with CoT: for several\n",
            "tasks that explicitly or implicitly require this capability, such as\n",
            "associative recall and determining if a graph is a tree, we prove that RNNs are\n",
            "not expressive enough to solve the tasks while Transformers can solve them with\n",
            "ease. Conversely, we prove that adopting techniques to enhance the in-context\n",
            "retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)\n",
            "and adding a single Transformer layer, can elevate RNNs to be capable of\n",
            "solving all polynomial-time solvable problems with CoT, hence closing the\n",
            "representation gap with Transformers.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.03792v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-06T15:40:30Z\n",
            "Title:  Neural Exec: Learning (and Learning from) Execution Triggers for Prompt\n",
            "  Injection Attacks\n",
            "Last Author:  Carmela Troncoso\n",
            "Authors:  Dario Pasquini, Martin Strohmeier, Carmela Troncoso\n",
            "abs page link: http://arxiv.org/abs/2403.03792v2\n",
            "pdf link: http://arxiv.org/pdf/2403.03792v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: v0.2\n",
            "Primary Category: cs.CR\n",
            "All Categories: cs.CR, cs.LG\n",
            "Abstract: We introduce a new family of prompt injection attacks, termed Neural Exec.\n",
            "Unlike known attacks that rely on handcrafted strings (e.g., \"Ignore previous\n",
            "instructions and...\"), we show that it is possible to conceptualize the\n",
            "creation of execution triggers as a differentiable search problem and use\n",
            "learning-based methods to autonomously generate them.\n",
            "  Our results demonstrate that a motivated adversary can forge triggers that\n",
            "are not only drastically more effective than current handcrafted ones but also\n",
            "exhibit inherent flexibility in shape, properties, and functionality. In this\n",
            "direction, we show that an attacker can design and generate Neural Execs\n",
            "capable of persisting through multi-stage preprocessing pipelines, such as in\n",
            "the case of Retrieval-Augmented Generation (RAG)-based applications. More\n",
            "critically, our findings show that attackers can produce triggers that deviate\n",
            "markedly in form and shape from any known attack, sidestepping existing\n",
            "blacklist-based detection and sanitation approaches.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.03888v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-06T17:48:06Z\n",
            "Title:  FaaF: Facts as a Function for the evaluation of generated text\n",
            "Last Author:  Gabor Barany\n",
            "Authors:  Vasileios Katranidis, Gabor Barany\n",
            "abs page link: http://arxiv.org/abs/2403.03888v2\n",
            "pdf link: http://arxiv.org/pdf/2403.03888v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 15 pages, 3 figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: The demand for accurate and efficient verification of information in texts\n",
            "generated by large language models (LMs) is at an all-time high, but remains\n",
            "unresolved. Recent efforts have focused on extracting and verifying atomic\n",
            "facts from these texts via prompting LM evaluators. However, we demonstrate\n",
            "that this method of prompting is unreliable when faced with incomplete or\n",
            "inaccurate reference information. We introduce Facts as a Function (FaaF), a\n",
            "new approach to the fact verification task that leverages the function-calling\n",
            "capabilities of LMs. FaaF significantly enhances the ability of LMs to identify\n",
            "unsupported facts in texts, while also improving efficiency and significantly\n",
            "lowering costs compared to prompt-based methods. Additionally, we propose a\n",
            "framework for evaluating factual recall in Retrieval Augmented Generation (RAG)\n",
            "systems, which we employ to compare prompt-based and FaaF methods using various\n",
            "LMs under challenging conditions.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.04307v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-07T08:25:46Z\n",
            "Title:  HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild\n",
            "Last Author:  Zhiqing Sun\n",
            "Authors:  Zhiying Zhu, Yiming Yang, Zhiqing Sun\n",
            "abs page link: http://arxiv.org/abs/2403.04307v2\n",
            "pdf link: http://arxiv.org/pdf/2403.04307v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Hallucinations pose a significant challenge to the reliability of large\n",
            "language models (LLMs) in critical domains. Recent benchmarks designed to\n",
            "assess LLM hallucinations within conventional NLP tasks, such as\n",
            "knowledge-intensive question answering (QA) and summarization, are insufficient\n",
            "for capturing the complexities of user-LLM interactions in dynamic, real-world\n",
            "settings. To address this gap, we introduce HaluEval-Wild, the first benchmark\n",
            "specifically designed to evaluate LLM hallucinations in the wild. We\n",
            "meticulously collect challenging (adversarially filtered by Alpaca) user\n",
            "queries from existing real-world user-LLM interaction datasets, including\n",
            "ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing\n",
            "the collected queries, we categorize them into five distinct types, which\n",
            "enables a fine-grained analysis of the types of hallucinations LLMs exhibit,\n",
            "and synthesize the reference answers with the powerful GPT-4 model and\n",
            "retrieval-augmented generation (RAG). Our benchmark offers a novel approach\n",
            "towards enhancing our comprehension and improvement of LLM reliability in\n",
            "scenarios reflective of real-world interactions. Our benchmark is available at\n",
            "https://github.com/Dianezzy/HaluEval-Wild.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.06840v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-11T16:01:05Z\n",
            "Title:  RA-ISF: Learning to Answer and Understand from Retrieval Augmentation\n",
            "  via Iterative Self-Feedback\n",
            "Last Author:  Tianyu Du\n",
            "Authors:  Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, Tianyu Du\n",
            "abs page link: http://arxiv.org/abs/2403.06840v2\n",
            "pdf link: http://arxiv.org/pdf/2403.06840v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 20 pages, multiple figures. Providing second version RA-ISF\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Large language models (LLMs) demonstrate exceptional performance in numerous\n",
            "tasks but still heavily rely on knowledge stored in their parameters. Moreover,\n",
            "updating this knowledge incurs high training costs. Retrieval-augmented\n",
            "generation (RAG) methods address this issue by integrating external knowledge.\n",
            "The model can answer questions it couldn't previously by retrieving knowledge\n",
            "relevant to the query. This approach improves performance in certain scenarios\n",
            "for specific tasks. However, if irrelevant texts are retrieved, it may impair\n",
            "model performance. In this paper, we propose Retrieval Augmented Iterative\n",
            "Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and\n",
            "processes them in three submodules to enhance the model's problem-solving\n",
            "capabilities. Experiments show that our method outperforms existing benchmarks,\n",
            "performing well on models like GPT3.5, Llama2, significantly enhancing factual\n",
            "reasoning capabilities and reducing hallucinations.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.09125v5\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-14T06:17:20Z\n",
            "Title:  Exploring the Capabilities and Limitations of Large Language Models in\n",
            "  the Electric Energy Sector\n",
            "Last Author:  Le Xie\n",
            "Authors:  Subir Majumder, Lin Dong, Fatemeh Doudi, Yuting Cai, Chao Tian, Dileep Kalathi, Kevin Ding, Anupam A. Thatte, Na Li, Le Xie\n",
            "abs page link: http://arxiv.org/abs/2403.09125v5\n",
            "pdf link: http://arxiv.org/pdf/2403.09125v5\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: eess.SY\n",
            "All Categories: eess.SY, cs.SY\n",
            "Abstract: Large Language Models (LLMs) as chatbots have drawn remarkable attention\n",
            "thanks to their versatile capability in natural language processing as well as\n",
            "in a wide range of tasks. While there has been great enthusiasm towards\n",
            "adopting such foundational model-based artificial intelligence tools in all\n",
            "sectors possible, the capabilities and limitations of such LLMs in improving\n",
            "the operation of the electric energy sector need to be explored, and this\n",
            "article identifies fruitful directions in this regard. Key future research\n",
            "directions include data collection systems for fine-tuning LLMs, embedding\n",
            "power system-specific tools in the LLMs, and retrieval augmented generation\n",
            "(RAG)-based knowledge pool to improve the quality of LLM responses and LLMs in\n",
            "safety-critical use cases.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.10153v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-15T09:54:04Z\n",
            "Title:  Improving Medical Multi-modal Contrastive Learning with Expert\n",
            "  Annotations\n",
            "Last Author:  Pekka Marttinen\n",
            "Authors:  Yogesh Kumar, Pekka Marttinen\n",
            "abs page link: http://arxiv.org/abs/2403.10153v1\n",
            "pdf link: http://arxiv.org/pdf/2403.10153v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Under review at a conference\n",
            "Primary Category: cs.CV\n",
            "All Categories: cs.CV, cs.LG\n",
            "Abstract: We introduce eCLIP, an enhanced version of the CLIP model that integrates\n",
            "expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key\n",
            "challenges in contrastive multi-modal medical imaging analysis, notably data\n",
            "scarcity and the \"modality gap\" -- a significant disparity between image and\n",
            "text embeddings that diminishes the quality of representations and hampers\n",
            "cross-modal interoperability. eCLIP integrates a heatmap processor and\n",
            "leverages mixup augmentation to efficiently utilize the scarce expert\n",
            "annotations, thus boosting the model's learning effectiveness. eCLIP is\n",
            "designed to be generally applicable to any variant of CLIP without requiring\n",
            "any modifications of the core architecture. Through detailed evaluations across\n",
            "several tasks, including zero-shot inference, linear probing, cross-modal\n",
            "retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using\n",
            "a frozen Large Language Model, eCLIP showcases consistent improvements in\n",
            "embedding quality. The outcomes reveal enhanced alignment and uniformity,\n",
            "affirming eCLIP's capability to harness high-quality annotations for enriched\n",
            "multi-modal analysis in the medical imaging domain.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.15736v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-23T06:03:36Z\n",
            "Title:  LLMs Instruct LLMs:An Extraction and Editing Method\n",
            "Last Author:  Qin Zhang\n",
            "Authors:  Xin Zhang, Tianjie Ju, Huijia Liang, Ying Fu, Qin Zhang\n",
            "abs page link: http://arxiv.org/abs/2403.15736v1\n",
            "pdf link: http://arxiv.org/pdf/2403.15736v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Working in progress\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: The interest in updating Large Language Models (LLMs) without retraining from\n",
            "scratch is substantial, yet it comes with some challenges.This is especially\n",
            "true for situations demanding complex reasoning with limited samples, a\n",
            "scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation\n",
            "for LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and\n",
            "Retrieval-Augmented Generation (RAG) are inadequate for this critical issue,\n",
            "particularly evident in our exploration of a specific medical context that\n",
            "epitomize the PCRA-LLM's distinct needs.To address the issue, we propose a\n",
            "Sequential Fusion method to incorporate knowledge from complex context into\n",
            "LLMs. This method employs a two-stage framework: initially, it leverages\n",
            "general LLMs to construct knowledge graphs (KGs) for extracting knowledge from\n",
            "complex texts; subsequently, it updates the domain LLMs through knowledge edit.\n",
            "According to our method, the domain LLM achieved a 71.69\\% accuracy in question\n",
            "answering tasks. Subsequently, we broadened our assessment to a novel dataset\n",
            "we developed in the economics and management field, where our method realized a\n",
            "75\\% accuracy. These outcomes underline the efficacy and adaptability of our\n",
            "approach for PCRA-LLM across various domains.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.01744v5\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-02T09:01:32Z\n",
            "Title:  Octopus v2: On-device language model for super agent\n",
            "Last Author:  Zhiyuan Li\n",
            "Authors:  Wei Chen, Zhiyuan Li\n",
            "abs page link: http://arxiv.org/abs/2404.01744v5\n",
            "pdf link: http://arxiv.org/pdf/2404.01744v5\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Language models have shown effectiveness in a variety of software\n",
            "applications, particularly in tasks related to automatic workflow. These models\n",
            "possess the crucial ability to call functions, which is essential in creating\n",
            "AI agents. Despite the high performance of large-scale language models in cloud\n",
            "environments, they are often associated with concerns over privacy and cost.\n",
            "Current on-device models for function calling face issues with latency and\n",
            "accuracy. Our research presents a new method that empowers an on-device model\n",
            "with 2 billion parameters to surpass the performance of GPT-4 in both accuracy\n",
            "and latency, and decrease the context length by 95\\%. When compared to Llama-7B\n",
            "with a RAG-based function calling mechanism, our method enhances latency by\n",
            "35-fold. This method reduces the latency to levels deemed suitable for\n",
            "deployment across a variety of edge devices in production environments,\n",
            "aligning with the performance requisites for real-world applications.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.02319v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-02T21:35:54Z\n",
            "Title:  Prompts As Programs: A Structure-Aware Approach to Efficient\n",
            "  Compile-Time Prompt Optimization\n",
            "Last Author:  Jennifer Neville\n",
            "Authors:  Tobias Schnabel, Jennifer Neville\n",
            "abs page link: http://arxiv.org/abs/2404.02319v1\n",
            "pdf link: http://arxiv.org/pdf/2404.02319v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: Large language models (LLMs) can now handle longer and more complex inputs,\n",
            "which facilitate the use of more elaborate prompts. However, prompts often\n",
            "require some tuning to improve performance for deployment. Recent work has\n",
            "proposed automatic prompt optimization methods, but as prompt complexity and\n",
            "LLM strength increase, many prompt optimization techniques are no longer\n",
            "sufficient and a new approach is needed to optimize {\\em meta prompt programs}.\n",
            "To address this, we introduce SAMMO, a framework for {\\em compile-time}\n",
            "optimizations of metaprompt programs, which represent prompts as structured\n",
            "objects that allows for a rich set of transformations that can be searched over\n",
            "during optimization. We show that SAMMO generalizes previous methods and\n",
            "improves the performance of complex prompts on (1) instruction tuning, (2) RAG\n",
            "pipeline tuning, and (3) prompt compression, across several different LLMs.\n",
            "  We make all code available open-source at https://github.com/microsoft/sammo .\n",
            "e-print metadata\n",
            "arxiv-id: 2404.02474v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-03T05:31:59Z\n",
            "Title:  uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?\n",
            "Last Author:  Yadollah Yaghoobzadeh\n",
            "Authors:  Pouya Sadeghi, Amirhossein Abaskohi, Yadollah Yaghoobzadeh\n",
            "abs page link: http://arxiv.org/abs/2404.02474v1\n",
            "pdf link: http://arxiv.org/pdf/2404.02474v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 12 pages, 5 figures, 6 tables, Proceedings of the 18th International\n",
            "  Workshop on Semantic Evaluation (SemEval-2024) @ NAACL 2024\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.IR, cs.LG\n",
            "Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for\n",
            "assessing LLMs' lateral thinking-thinking outside the box. Building upon this\n",
            "benchmark, we investigate how different prompting methods enhance LLMs'\n",
            "performance on this task to reveal their inherent power for outside-the-box\n",
            "thinking ability. Through participating in SemEval-2024, task 9, Sentence\n",
            "Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT)\n",
            "and direct prompting, enhancing with informative descriptions, and employing\n",
            "contextualizing prompts using a retrieval augmented generation (RAG) pipeline.\n",
            "Our experiments involve three LLMs including GPT-3.5, GPT-4, and\n",
            "Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and\n",
            "options using GPT-4, validated by humans for quality. Findings indicate that\n",
            "compressed informative prompts enhance performance. Dynamic in-context learning\n",
            "enhances model performance significantly. Furthermore, fine-tuning Zephyr on\n",
            "our dataset enhances performance across other commonsense datasets,\n",
            "underscoring the value of innovative thinking.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.04510v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-06T05:44:53Z\n",
            "Title:  IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe\n",
            "  Biomedical Natural Language Inference for Clinical Trials\n",
            "Last Author:  Ashutosh Modi\n",
            "Authors:  Shreyasi Mandal, Ashutosh Modi\n",
            "abs page link: http://arxiv.org/abs/2404.04510v1\n",
            "pdf link: http://arxiv.org/pdf/2404.04510v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted at SemEval 2024, NAACL 2024; 8 Pages\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: Large Language models (LLMs) have demonstrated state-of-the-art performance\n",
            "in various natural language processing (NLP) tasks across multiple domains, yet\n",
            "they are prone to shortcut learning and factual inconsistencies. This research\n",
            "investigates LLMs' robustness, consistency, and faithful reasoning when\n",
            "performing Natural Language Inference (NLI) on breast cancer Clinical Trial\n",
            "Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural\n",
            "Language Inference for Clinical Trials. We examine the reasoning capabilities\n",
            "of LLMs and their adeptness at logical problem-solving. A comparative analysis\n",
            "is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro\n",
            "under zero-shot settings using Retrieval-Augmented Generation (RAG) framework,\n",
            "integrating various reasoning chains. The evaluation yields an F1 score of\n",
            "0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test\n",
            "dataset.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.05587v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-08T15:00:36Z\n",
            "Title:  Enhancing Software-Related Information Extraction via Single-Choice\n",
            "  Question Answering with Large Language Models\n",
            "Last Author:  Stefan Dietze\n",
            "Authors:  Wolfgang Otto, Sharmila Upadhyaya, Stefan Dietze\n",
            "abs page link: http://arxiv.org/abs/2404.05587v2\n",
            "pdf link: http://arxiv.org/pdf/2404.05587v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted at: 1st Workshop on Natural Scientific Language Processing\n",
            "  and Research Knowledge Graphs (NSLP 2024) Co-located with Extended Semantic\n",
            "  Web Conference (ESWC 2024)\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, I.2.7\n",
            "Abstract: This paper describes our participation in the Shared Task on Software\n",
            "Mentions Disambiguation (SOMD), with a focus on improving relation extraction\n",
            "in scholarly texts through generative Large Language Models (LLMs) using\n",
            "single-choice question-answering. The methodology prioritises the use of\n",
            "in-context learning capabilities of GLMs to extract software-related entities\n",
            "and their descriptive attributes, such as distributive information. Our\n",
            "approach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for\n",
            "Named Entity Recognition (NER) and Attributive NER to identify relationships\n",
            "between extracted software entities, providing a structured solution for\n",
            "analysing software citations in academic literature. The paper provides a\n",
            "detailed description of our approach, demonstrating how using GLMs in a\n",
            "single-choice QA paradigm can greatly enhance IE methodologies. Our\n",
            "participation in the SOMD shared task highlights the importance of precise\n",
            "software citation practices and showcases our system's ability to overcome the\n",
            "challenges of disambiguating and extracting relationships between software\n",
            "mentions. This sets the groundwork for future research and development in this\n",
            "field.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.06004v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-09T04:20:27Z\n",
            "Title:  AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free\n",
            "  Information Retrieval\n",
            "Last Author:  Jun Deguchi\n",
            "Authors:  Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama, Kazunari Sumiyoshi, Jun Deguchi\n",
            "abs page link: http://arxiv.org/abs/2404.06004v1\n",
            "pdf link: http://arxiv.org/pdf/2404.06004v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 5 pages, 6 figures and 4 tables\n",
            "Primary Category: cs.IR\n",
            "All Categories: cs.IR, cs.CL, cs.DS\n",
            "Abstract: In approximate nearest neighbor search (ANNS) methods based on approximate\n",
            "proximity graphs, DiskANN achieves good recall-speed balance for large-scale\n",
            "datasets using both of RAM and storage. Despite it claims to save memory usage\n",
            "by loading compressed vectors by product quantization (PQ), its memory usage\n",
            "increases in proportion to the scale of datasets. In this paper, we propose\n",
            "All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the\n",
            "compressed vectors to storage. Our method achieves $\\sim$10 MB memory usage in\n",
            "query search even with billion-scale datasets with minor performance\n",
            "degradation. AiSAQ also reduces the index load time before query search, which\n",
            "enables the index switch between muitiple billion-scale datasets and\n",
            "significantly enhances the flexibility of retrieval-augmented generation (RAG).\n",
            "This method is applicable to all graph-based ANNS algorithms and can be\n",
            "combined with higher-spec ANNS methods in the future.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.06278v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-09T13:02:22Z\n",
            "Title:  Dimensionality Reduction in Sentence Transformer Vector Databases with\n",
            "  Fast Fourier Transform\n",
            "Last Author:  Alec Segal\n",
            "Authors:  Vitaly Bulgakov, Alec Segal\n",
            "abs page link: http://arxiv.org/abs/2404.06278v1\n",
            "pdf link: http://arxiv.org/pdf/2404.06278v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 13 pages, 5 figures\n",
            "Primary Category: cs.DB\n",
            "All Categories: cs.DB, cs.AI, cs.CL, cs.LG\n",
            "Abstract: Dimensionality reduction in vector databases is pivotal for streamlining AI\n",
            "data management, enabling efficient storage, faster computation, and improved\n",
            "model performance. This paper explores the benefits of reducing vector database\n",
            "dimensions, with a focus on computational efficiency and overcoming the curse\n",
            "of dimensionality. We introduce a novel application of Fast Fourier Transform\n",
            "(FFT) to dimensionality reduction, a method previously underexploited in this\n",
            "context. By demonstrating its utility across various AI domains, including\n",
            "Retrieval-Augmented Generation (RAG) models and image processing, this\n",
            "FFT-based approach promises to improve data retrieval processes and enhance the\n",
            "efficiency and scalability of AI solutions. The incorporation of FFT may not\n",
            "only optimize operations in real-time processing and recommendation systems but\n",
            "also extend to advanced image processing techniques, where dimensionality\n",
            "reduction can significantly improve performance and analysis efficiency. This\n",
            "paper advocates for the broader adoption of FFT in vector database management,\n",
            "marking a significant stride towards addressing the challenges of data volume\n",
            "and complexity in AI research and applications. Unlike many existing\n",
            "approaches, we directly handle the embedding vectors produced by the model\n",
            "after processing a test input.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.06680v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-10T02:02:34Z\n",
            "Title:  Onco-Retriever: Generative Classifier for Retrieval of EHR Records in\n",
            "  Oncology\n",
            "Last Author:  Hrituraj Singh\n",
            "Authors:  Shashi Kant Gupta, Aditya Basu, Bradley Taylor, Anai Kothari, Hrituraj Singh\n",
            "abs page link: http://arxiv.org/abs/2404.06680v1\n",
            "pdf link: http://arxiv.org/pdf/2404.06680v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 18 pages\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Retrieving information from EHR systems is essential for answering specific\n",
            "questions about patient journeys and improving the delivery of clinical care.\n",
            "Despite this fact, most EHR systems still rely on keyword-based searches. With\n",
            "the advent of generative large language models (LLMs), retrieving information\n",
            "can lead to better search and summarization capabilities. Such retrievers can\n",
            "also feed Retrieval-augmented generation (RAG) pipelines to answer any query.\n",
            "However, the task of retrieving information from EHR real-world clinical data\n",
            "contained within EHR systems in order to solve several downstream use cases is\n",
            "challenging due to the difficulty in creating query-document support pairs. We\n",
            "provide a blueprint for creating such datasets in an affordable manner using\n",
            "large language models. Our method results in a retriever that is 30-50 F-1\n",
            "points better than propriety counterparts such as Ada and Mistral for oncology\n",
            "data elements. We further compare our model, called Onco-Retriever, against\n",
            "fine-tuned PubMedBERT model as well. We conduct an extensive manual evaluation\n",
            "on real-world EHR data along with latency analysis of the different models and\n",
            "provide a path forward for healthcare organizations to build domain-specific\n",
            "retrievers.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.07376v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-10T22:26:26Z\n",
            "Title:  LLMs in Biomedicine: A study on clinical Named Entity Recognition\n",
            "Last Author:  Kai-Wei Chang\n",
            "Authors:  Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel, Melika Emami, Fazlolah Mohaghegh, Mozhdeh Rouhsedaghat, Kai-Wei Chang\n",
            "abs page link: http://arxiv.org/abs/2404.07376v1\n",
            "pdf link: http://arxiv.org/pdf/2404.07376v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Large Language Models (LLMs) demonstrate remarkable versatility in various\n",
            "NLP tasks but encounter distinct challenges in biomedicine due to medical\n",
            "language complexities and data scarcity. This paper investigates the\n",
            "application of LLMs in the medical domain by exploring strategies to enhance\n",
            "their performance for the Named-Entity Recognition (NER) task. Specifically,\n",
            "our study reveals the importance of meticulously designed prompts in\n",
            "biomedicine. Strategic selection of in-context examples yields a notable\n",
            "improvement, showcasing ~15-20\\% increase in F1 score across all benchmark\n",
            "datasets for few-shot clinical NER. Additionally, our findings suggest that\n",
            "integrating external resources through prompting strategies can bridge the gap\n",
            "between general-purpose LLM proficiency and the specialized demands of medical\n",
            "NER. Leveraging a medical knowledge base, our proposed method inspired by\n",
            "Retrieval-Augmented Generation (RAG) can boost the F1 score of LLMs for\n",
            "zero-shot clinical NER. We will release the code upon publication.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.11216v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-17T10:00:56Z\n",
            "Title:  Position Engineering: Boosting Large Language Models through Positional\n",
            "  Information Manipulation\n",
            "Last Author:  Lili Qiu\n",
            "Authors:  Zhiyuan He, Huiqiang Jiang, Zilong Wang, Yuqing Yang, Luna Qiu, Lili Qiu\n",
            "abs page link: http://arxiv.org/abs/2404.11216v1\n",
            "pdf link: http://arxiv.org/pdf/2404.11216v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: The performance of large language models (LLMs) is significantly influenced\n",
            "by the quality of the prompts provided. In response, researchers have developed\n",
            "enormous prompt engineering strategies aimed at modifying the prompt text to\n",
            "enhance task performance. In this paper, we introduce a novel technique termed\n",
            "position engineering, which offers a more efficient way to guide large language\n",
            "models. Unlike prompt engineering, which requires substantial effort to modify\n",
            "the text provided to LLMs, position engineering merely involves altering the\n",
            "positional information in the prompt without modifying the text itself. We have\n",
            "evaluated position engineering in two widely-used LLM scenarios:\n",
            "retrieval-augmented generation (RAG) and in-context learning (ICL). Our\n",
            "findings show that position engineering substantially improves upon the\n",
            "baseline in both cases. Position engineering thus represents a promising new\n",
            "strategy for exploiting the capabilities of large language models.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.11672v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-17T18:13:16Z\n",
            "Title:  MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory\n",
            "Last Author:  Hinrich Schütze\n",
            "Authors:  Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze\n",
            "abs page link: http://arxiv.org/abs/2404.11672v1\n",
            "pdf link: http://arxiv.org/pdf/2404.11672v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: While current large language models (LLMs) demonstrate some capabilities in\n",
            "knowledge-intensive tasks, they are limited by relying on their parameters as\n",
            "an implicit storage mechanism. As a result, they struggle with infrequent\n",
            "knowledge and temporal degradation. In addition, the uninterpretable nature of\n",
            "parametric memorization makes it challenging to understand and prevent\n",
            "hallucination. Parametric memory pools and model editing are only partial\n",
            "solutions. Retrieval Augmented Generation (RAG) $\\unicode{x2013}$ though\n",
            "non-parametric $\\unicode{x2013}$ has its own limitations: it lacks structure,\n",
            "complicates interpretability and makes it hard to effectively manage stored\n",
            "knowledge. In this paper, we introduce MemLLM, a novel method of enhancing LLMs\n",
            "by integrating a structured and explicit read-and-write memory module. MemLLM\n",
            "tackles the aforementioned challenges by enabling dynamic interaction with the\n",
            "memory and improving the LLM's capabilities in using stored knowledge. Our\n",
            "experiments indicate that MemLLM enhances the LLM's performance and\n",
            "interpretability, in language modeling in general and knowledge-intensive tasks\n",
            "in particular. We see MemLLM as an important step towards making LLMs more\n",
            "grounded and factual through memory augmentation.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.13892v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-22T05:46:40Z\n",
            "Title:  Retrieval-Augmented Audio Deepfake Detection\n",
            "Last Author:  Jianzong Wang\n",
            "Authors:  Zuheng Kang, Yayun He, Botao Zhao, Xiaoyang Qu, Junqing Peng, Jing Xiao, Jianzong Wang\n",
            "abs page link: http://arxiv.org/abs/2404.13892v2\n",
            "pdf link: http://arxiv.org/pdf/2404.13892v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted by the 2024 International Conference on Multimedia Retrieval\n",
            "  (ICMR 2024)\n",
            "Primary Category: cs.SD\n",
            "All Categories: cs.SD, cs.AI, eess.AS\n",
            "Abstract: With recent advances in speech synthesis including text-to-speech (TTS) and\n",
            "voice conversion (VC) systems enabling the generation of ultra-realistic audio\n",
            "deepfakes, there is growing concern about their potential misuse. However, most\n",
            "deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a\n",
            "single model, resulting in performance bottlenecks and transparency issues.\n",
            "Inspired by retrieval-augmented generation (RAG), we propose a\n",
            "retrieval-augmented detection (RAD) framework that augments test samples with\n",
            "similar retrieved samples for enhanced detection. We also extend the\n",
            "multi-fusion attentive classifier to integrate it with our proposed RAD\n",
            "framework. Extensive experiments show the superior performance of the proposed\n",
            "RAD framework over baseline methods, achieving state-of-the-art results on the\n",
            "ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.\n",
            "Further sample analysis indicates that the retriever consistently retrieves\n",
            "samples mostly from the same speaker with acoustic characteristics highly\n",
            "consistent with the query audio, thereby improving detection performance.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.01359v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-02T15:06:18Z\n",
            "Title:  GAIA: A General AI Assistant for Intelligent Accelerator Operations\n",
            "Last Author:  Frank Mayet\n",
            "Authors:  Frank Mayet\n",
            "abs page link: http://arxiv.org/abs/2405.01359v1\n",
            "pdf link: http://arxiv.org/pdf/2405.01359v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, physics.acc-ph\n",
            "Abstract: Large-scale machines like particle accelerators are usually run by a team of\n",
            "experienced operators. In case of a particle accelerator, these operators\n",
            "possess suitable background knowledge on both accelerator physics and the\n",
            "technology comprising the machine. Due to the complexity of the machine,\n",
            "particular subsystems of the machine are taken care of by experts, who the\n",
            "operators can turn to. In this work the reasoning and action (ReAct) prompting\n",
            "paradigm is used to couple an open-weights large language model (LLM) with a\n",
            "high-level machine control system framework and other tools, e.g. the\n",
            "electronic logbook or machine design documentation. By doing so, a multi-expert\n",
            "retrieval augmented generation (RAG) system is implemented, which assists\n",
            "operators in knowledge retrieval tasks, interacts with the machine directly if\n",
            "needed, or writes high level control system scripts. This consolidation of\n",
            "expert knowledge and machine interaction can simplify and speed up machine\n",
            "operation tasks for both new and experienced human operators.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.04717v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-07T23:44:09Z\n",
            "Title:  Remote Diffusion\n",
            "Last Author:  Kunal Sunil Kasodekar\n",
            "Authors:  Kunal Sunil Kasodekar\n",
            "abs page link: http://arxiv.org/abs/2405.04717v1\n",
            "pdf link: http://arxiv.org/pdf/2405.04717v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CV\n",
            "All Categories: cs.CV\n",
            "Abstract: I explored adapting Stable Diffusion v1.5 for generating domain-specific\n",
            "satellite and aerial images in remote sensing. Recognizing the limitations of\n",
            "existing models like Midjourney and Stable Diffusion, trained primarily on\n",
            "natural RGB images and lacking context for remote sensing, I used the RSICD\n",
            "dataset to train a Stable Diffusion model with a loss of 0.2. I incorporated\n",
            "descriptive captions from the dataset for text-conditioning. Additionally, I\n",
            "created a synthetic dataset for a Land Use Land Classification (LULC) task,\n",
            "employing prompting techniques with RAG and ChatGPT and fine-tuning a\n",
            "specialized remote sensing LLM. However, I faced challenges with prompt quality\n",
            "and model performance. I trained a classification model (ResNet18) on the\n",
            "synthetic dataset achieving 49.48% test accuracy in TorchGeo to create a\n",
            "baseline. Quantitative evaluation through FID scores and qualitative feedback\n",
            "from domain experts assessed the realism and quality of the generated images\n",
            "and dataset. Despite extensive fine-tuning and dataset iterations, results\n",
            "indicated subpar image quality and realism, as indicated by high FID scores and\n",
            "domain-expert evaluation. These findings call attention to the potential of\n",
            "diffusion models in remote sensing while highlighting significant challenges\n",
            "related to insufficient pretraining data and computational resources.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.06697v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-08T04:07:38Z\n",
            "Title:  Automated Conversion of Static to Dynamic Scheduler via Natural Language\n",
            "Last Author:  Hoong Chuin Lau\n",
            "Authors:  Paul Mingzheng Tang, Kenji Kah Hoe Leong, Nowshad Shaik, Hoong Chuin Lau\n",
            "abs page link: http://arxiv.org/abs/2405.06697v1\n",
            "pdf link: http://arxiv.org/pdf/2405.06697v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 7 pages (excluding appendix), 10 figures, 3 tables\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: In this paper, we explore the potential application of Large Language Models\n",
            "(LLMs) that will automatically model constraints and generate code for dynamic\n",
            "scheduling problems given an existing static model. Static scheduling problems\n",
            "are modelled and coded by optimization experts. These models may be easily\n",
            "obsoleted as the underlying constraints may need to be fine-tuned in order to\n",
            "reflect changes in the scheduling rules. Furthermore, it may be necessary to\n",
            "turn a static model into a dynamic one in order to cope with disturbances in\n",
            "the environment. In this paper, we propose a Retrieval-Augmented Generation\n",
            "(RAG) based LLM model to automate the process of implementing constraints for\n",
            "Dynamic Scheduling (RAGDyS), without seeking help from an optimization modeling\n",
            "expert. Our framework aims to minimize technical complexities related to\n",
            "mathematical modelling and computational workload for end-users, thereby\n",
            "allowing end-users to quickly obtain a new schedule close to the original\n",
            "schedule with changes reflected by natural language constraint descriptions.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.07963v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-13T17:44:05Z\n",
            "Title:  PyZoBot: A Platform for Conversational Information Extraction and\n",
            "  Synthesis from Curated Zotero Reference Libraries through Advanced\n",
            "  Retrieval-Augmented Generation\n",
            "Last Author:  Dayanjan S. Wijesinghe\n",
            "Authors:  Suad Alshammari, Lama Basalelah, Walaa Abu Rukbah, Ali Alsuhibani, Dayanjan S. Wijesinghe\n",
            "abs page link: http://arxiv.org/abs/2405.07963v1\n",
            "pdf link: http://arxiv.org/pdf/2405.07963v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 10 pages, 2 figures. The code is provided in github and the link to\n",
            "  the repository is provided at the end of the publication\n",
            "Primary Category: cs.HC\n",
            "All Categories: cs.HC\n",
            "Abstract: The exponential growth of scientific literature has resulted in information\n",
            "overload, challenging researchers to effectively synthesize relevant\n",
            "publications. This paper explores the integration of traditional reference\n",
            "management software with advanced computational techniques, including Large\n",
            "Language Models and Retrieval-Augmented Generation. We introduce PyZoBot, an\n",
            "AI-driven platform developed in Python, incorporating Zoteros reference\n",
            "management with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge\n",
            "extraction and synthesis from extensive human-curated scientific literature\n",
            "databases. It demonstrates proficiency in handling complex natural language\n",
            "queries, integrating data from multiple sources, and meticulously presenting\n",
            "references to uphold research integrity and facilitate further exploration. By\n",
            "leveraging LLMs, RAG, and human expertise through a curated library, PyZoBot\n",
            "offers an effective solution to manage information overload and keep pace with\n",
            "rapid scientific advancements. The development of such AI-enhanced tools\n",
            "promises significant improvements in research efficiency and effectiveness\n",
            "across various disciplines.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.09161v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-15T07:48:10Z\n",
            "Title:  Exploring the Potential of Large Language Models for Automation in\n",
            "  Technical Customer Service\n",
            "Last Author:  Juerg Meierhofer\n",
            "Authors:  Jochen Wulf, Juerg Meierhofer\n",
            "abs page link: http://arxiv.org/abs/2405.09161v2\n",
            "pdf link: http://arxiv.org/pdf/2405.09161v2\n",
            "Journal reference: Proceedings of the Spring Servitization Conference (SSC2024)\n",
            "Comments: No comment found\n",
            "Primary Category: econ.GN\n",
            "All Categories: econ.GN, q-fin.EC\n",
            "Abstract: Purpose: The purpose of this study is to investigate the potential of Large\n",
            "Language Models (LLMs) in transforming technical customer service (TCS) through\n",
            "the automation of cognitive tasks. Design/Methodology/Approach: Using a\n",
            "prototyping approach, the research assesses the feasibility of automating\n",
            "cognitive tasks in TCS with LLMs, employing real-world technical incident data\n",
            "from a Swiss telecommunications operator. Findings: Lower-level cognitive tasks\n",
            "such as translation, summarization, and content generation can be effectively\n",
            "automated with LLMs like GPT-4, while higher-level tasks such as reasoning\n",
            "require more advanced technological approaches such as Retrieval-Augmented\n",
            "Generation (RAG) or finetuning ; furthermore, the study underscores the\n",
            "significance of data ecosystems in enabling more complex cognitive tasks by\n",
            "fostering data sharing among various actors involved. Originality/Value: This\n",
            "study contributes to the emerging theory on LLM potential and technical\n",
            "feasibility in service management, providing concrete insights for operators of\n",
            "TCS units and highlighting the need for further research to address limitations\n",
            "and validate the applicability of LLMs across different domains.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.09980v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-16T10:53:31Z\n",
            "Title:  FinTextQA: A Dataset for Long-form Financial Question Answering\n",
            "Last Author:  Junwei Liang\n",
            "Authors:  Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, Junwei Liang\n",
            "abs page link: http://arxiv.org/abs/2405.09980v1\n",
            "pdf link: http://arxiv.org/pdf/2405.09980v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Accurate evaluation of financial question answering (QA) systems necessitates\n",
            "a comprehensive dataset encompassing diverse question types and contexts.\n",
            "However, current financial QA datasets lack scope diversity and question\n",
            "complexity. This work introduces FinTextQA, a novel dataset for long-form\n",
            "question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality,\n",
            "source-attributed QA pairs extracted and selected from finance textbooks and\n",
            "government agency websites.Moreover, we developed a Retrieval-Augmented\n",
            "Generation (RAG)-based LFQA system, comprising an embedder, retriever,\n",
            "reranker, and generator. A multi-faceted evaluation approach, including human\n",
            "ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the\n",
            "performance of different LFQA system configurations under heightened noisy\n",
            "conditions. The results indicate that: (1) Among all compared generators,\n",
            "Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The\n",
            "most effective system configuration on our dataset involved setting the\n",
            "embedder, retriever, reranker, and generator as Ada2, Automated Merged\n",
            "Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are\n",
            "less susceptible to noise after the length of contexts reaching a specific\n",
            "threshold.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.10440v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-16T20:59:28Z\n",
            "Title:  Retrieving and Refining: A Hybrid Framework with Large Language Models\n",
            "  for Rare Disease Identification\n",
            "Last Author:  Honghan Wu\n",
            "Authors:  Jinge Wu, Hang Dong, Zexi Li, Arijit Patra, Honghan Wu\n",
            "abs page link: http://arxiv.org/abs/2405.10440v1\n",
            "pdf link: http://arxiv.org/pdf/2405.10440v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: The infrequency and heterogeneity of clinical presentations in rare diseases\n",
            "often lead to underdiagnosis and their exclusion from structured datasets. This\n",
            "necessitates the utilization of unstructured text data for comprehensive\n",
            "analysis. However, the manual identification from clinical reports is an\n",
            "arduous and intrinsically subjective task. This study proposes a novel hybrid\n",
            "approach that synergistically combines a traditional dictionary-based natural\n",
            "language processing (NLP) tool with the powerful capabilities of large language\n",
            "models (LLMs) to enhance the identification of rare diseases from unstructured\n",
            "clinical notes. We comprehensively evaluate various prompting strategies on six\n",
            "large language models (LLMs) of varying sizes and domains (general and\n",
            "medical). This evaluation encompasses zero-shot, few-shot, and\n",
            "retrieval-augmented generation (RAG) techniques to enhance the LLMs' ability to\n",
            "reason about and understand contextual information in patient reports. The\n",
            "results demonstrate effectiveness in rare disease identification, highlighting\n",
            "the potential for identifying underdiagnosed patients from clinical notes.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.13873v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-22T17:56:53Z\n",
            "Title:  FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\n",
            "  Question Answering\n",
            "Last Author:  Bryan Hooi\n",
            "Authors:  Yuan Sui, Yufei He, Nian Liu, Xiaoxin He, Kun Wang, Bryan Hooi\n",
            "abs page link: http://arxiv.org/abs/2405.13873v1\n",
            "pdf link: http://arxiv.org/pdf/2405.13873v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.AI\n",
            "All Categories: cs.AI, cs.CL\n",
            "Abstract: While large language models (LLMs) have achieved significant success in\n",
            "various applications, they often struggle with hallucinations, especially in\n",
            "scenarios that require deep and responsible reasoning. These issues could be\n",
            "partially mitigate by integrating external knowledge graphs (KG) in LLM\n",
            "reasoning. However, the method of their incorporation is still largely\n",
            "unexplored. In this paper, we propose a retrieval-exploration interactive\n",
            "method, FiDelis to handle intermediate steps of reasoning grounded by KGs.\n",
            "Specifically, we propose Path-RAG module for recalling useful intermediate\n",
            "knowledge from KG for LLM reasoning. We incorporate the logic and common-sense\n",
            "reasoning of LLMs and topological connectivity of KGs into the knowledge\n",
            "retrieval process, which provides more accurate recalling performance.\n",
            "Furthermore, we propose to leverage deductive reasoning capabilities of LLMs as\n",
            "a better criterion to automatically guide the reasoning process in a stepwise\n",
            "and generalizable manner. Deductive verification serve as precise indicators\n",
            "for when to cease further reasoning, thus avoiding misleading the chains of\n",
            "reasoning and unnecessary computation. Extensive experiments show that our\n",
            "method, as a training-free method with lower computational cost and better\n",
            "generality outperforms the existing strong baselines in three benchmarks.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.16072v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-25T05:45:55Z\n",
            "Title:  SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS\n",
            "  Design Generation\n",
            "Last Author:  Andre Ivanov\n",
            "Authors:  Seyed Arash Sheikholeslam, Andre Ivanov\n",
            "abs page link: http://arxiv.org/abs/2405.16072v2\n",
            "pdf link: http://arxiv.org/pdf/2405.16072v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: This work is in progress and we will be updating it\n",
            "Primary Category: cs.AI\n",
            "All Categories: cs.AI\n",
            "Abstract: In this paper, we introduce SynthAI, a new method for the automated creation\n",
            "of High-Level Synthesis (HLS) designs. SynthAI integrates ReAct agents,\n",
            "Chain-of-Thought (CoT) prompting, web search technologies, and the\n",
            "Retrieval-Augmented Generation (RAG) framework within a structured decision\n",
            "graph. This innovative approach enables the systematic decomposition of complex\n",
            "hardware design tasks into multiple stages and smaller, manageable modules. As\n",
            "a result, SynthAI produces synthesizable designs that closely adhere to\n",
            "user-specified design objectives and functional requirements. We further\n",
            "validate the capabilities of SynthAI through several case studies, highlighting\n",
            "its proficiency in generating complex, multi-module logic designs from a single\n",
            "initial prompt. The SynthAI code is provided via the following repo:\n",
            "\\url{https://github.com/sarashs/FPGA_AGI}\n",
            "e-print metadata\n",
            "arxiv-id: 2405.17147v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-27T13:16:29Z\n",
            "Title:  Large Language Models (LLMs): Deployment, Tokenomics and Sustainability\n",
            "Last Author:  Shuang Xie\n",
            "Authors:  Haiwei Dong, Shuang Xie\n",
            "abs page link: http://arxiv.org/abs/2405.17147v1\n",
            "pdf link: http://arxiv.org/pdf/2405.17147v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted by IEEE CTSoc-NCT\n",
            "Primary Category: cs.MM\n",
            "All Categories: cs.MM\n",
            "Abstract: The rapid advancement of Large Language Models (LLMs) has significantly\n",
            "impacted human-computer interaction, epitomized by the release of GPT-4o, which\n",
            "introduced comprehensive multi-modality capabilities. In this paper, we first\n",
            "explored the deployment strategies, economic considerations, and sustainability\n",
            "challenges associated with the state-of-the-art LLMs. More specifically, we\n",
            "discussed the deployment debate between Retrieval-Augmented Generation (RAG)\n",
            "and fine-tuning, highlighting their respective advantages and limitations.\n",
            "After that, we quantitatively analyzed the requirement of xPUs in training and\n",
            "inference. Additionally, for the tokenomics of LLM services, we examined the\n",
            "balance between performance and cost from the quality of experience (QoE)'s\n",
            "perspective of end users. Lastly, we envisioned the future hybrid architecture\n",
            "of LLM processing and its corresponding sustainability concerns, particularly\n",
            "in the environmental carbon footprint impact. Through these discussions, we\n",
            "provided a comprehensive overview of the operational and strategic\n",
            "considerations essential for the responsible development and deployment of\n",
            "LLMs.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.19366v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-26T06:45:39Z\n",
            "Title:  ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with\n",
            "  LLM-Enhanced Cardiological Text\n",
            "Last Author:  Akane Sano\n",
            "Authors:  Han Yu, Peikun Guo, Akane Sano\n",
            "abs page link: http://arxiv.org/abs/2405.19366v1\n",
            "pdf link: http://arxiv.org/pdf/2405.19366v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: eess.SP\n",
            "All Categories: eess.SP, cs.AI\n",
            "Abstract: The utilization of deep learning on electrocardiogram (ECG) analysis has\n",
            "brought the advanced accuracy and efficiency of cardiac healthcare diagnostics.\n",
            "By leveraging the capabilities of deep learning in semantic understanding,\n",
            "especially in feature extraction and representation learning, this study\n",
            "introduces a new multimodal contrastive pretaining framework that aims to\n",
            "improve the quality and robustness of learned representations of 12-lead ECG\n",
            "signals. Our framework comprises two key components, including Cardio Query\n",
            "Assistant (CQA) and ECG Semantics Integrator(ESI). CQA integrates a\n",
            "retrieval-augmented generation (RAG) pipeline to leverage large language models\n",
            "(LLMs) and external medical knowledge to generate detailed textual descriptions\n",
            "of ECGs. The generated text is enriched with information about demographics and\n",
            "waveform patterns. ESI integrates both contrastive and captioning loss to\n",
            "pretrain ECG encoders for enhanced representations. We validate our approach\n",
            "through various downstream tasks, including arrhythmia detection and ECG-based\n",
            "subject identification. Our experimental results demonstrate substantial\n",
            "improvements over strong baselines in these tasks. These baselines encompass\n",
            "supervised and self-supervised learning methods, as well as prior multimodal\n",
            "pretraining approaches.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.19563v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-29T23:11:53Z\n",
            "Title:  Unlearning Climate Misinformation in Large Language Models\n",
            "Last Author:  Dimitrios Stamoulis\n",
            "Authors:  Michael Fore, Simranjit Singh, Chaehong Lee, Amritanshu Pandey, Antonios Anastasopoulos, Dimitrios Stamoulis\n",
            "abs page link: http://arxiv.org/abs/2405.19563v1\n",
            "pdf link: http://arxiv.org/pdf/2405.19563v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Misinformation regarding climate change is a key roadblock in addressing one\n",
            "of the most serious threats to humanity. This paper investigates factual\n",
            "accuracy in large language models (LLMs) regarding climate information. Using\n",
            "true/false labeled Q&A data for fine-tuning and evaluating LLMs on\n",
            "climate-related claims, we compare open-source models, assessing their ability\n",
            "to generate truthful responses to climate change questions. We investigate the\n",
            "detectability of models intentionally poisoned with false climate information,\n",
            "finding that such poisoning may not affect the accuracy of a model's responses\n",
            "in other domains. Furthermore, we compare the effectiveness of unlearning\n",
            "algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually\n",
            "grounding LLMs on climate change topics. Our evaluation reveals that unlearning\n",
            "algorithms can be effective for nuanced conceptual claims, despite previous\n",
            "findings suggesting their inefficacy in privacy contexts. These insights aim to\n",
            "guide the development of more factually reliable LLMs and highlight the need\n",
            "for additional work to secure LLMs against misinformation attacks.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.20455v3\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-30T20:05:44Z\n",
            "Title:  DepsRAG: Towards Managing Software Dependencies using Large Language\n",
            "  Models\n",
            "Last Author:  Benoit Baudry\n",
            "Authors:  Mohannad Alhanahnah, Yazan Boshmaf, Benoit Baudry\n",
            "abs page link: http://arxiv.org/abs/2405.20455v3\n",
            "pdf link: http://arxiv.org/pdf/2405.20455v3\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE\n",
            "Abstract: Managing software dependencies is a crucial maintenance task in software\n",
            "development and is becoming a rapidly growing research field, especially in\n",
            "light of the significant increase in software supply chain attacks. Specialized\n",
            "expertise and substantial developer effort are required to fully comprehend\n",
            "dependencies and reveal hidden properties about the dependencies (e.g., number\n",
            "of dependencies, dependency chains, depth of dependencies).\n",
            "  Recent advancements in Large Language Models (LLMs) allow the retrieval of\n",
            "information from various data sources for response generation, thus providing a\n",
            "new opportunity to uniquely manage software dependencies. To highlight the\n",
            "potential of this technology, we present~\\tool, a proof-of-concept Retrieval\n",
            "Augmented Generation (RAG) approach that constructs direct and transitive\n",
            "dependencies of software packages as a Knowledge Graph (KG) in four popular\n",
            "software ecosystems. DepsRAG can answer user questions about software\n",
            "dependencies by automatically generating necessary queries to retrieve\n",
            "information from the KG, and then augmenting the input of LLMs with the\n",
            "retrieved information. DepsRAG can also perform Web search to answer questions\n",
            "that the LLM cannot directly answer via the KG. We identify tangible benefits\n",
            "that DepsRAG can offer and discuss its limitations.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.00638v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-02T06:48:43Z\n",
            "Title:  COS-Mix: Cosine Similarity and Distance Fusion for Improved Information\n",
            "  Retrieval\n",
            "Last Author:  Anupam Purwar\n",
            "Authors:  Kush Juvekar, Anupam Purwar\n",
            "abs page link: http://arxiv.org/abs/2406.00638v1\n",
            "pdf link: http://arxiv.org/pdf/2406.00638v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.IR\n",
            "All Categories: cs.IR\n",
            "Abstract: This study proposes a novel hybrid retrieval strategy for Retrieval-Augmented\n",
            "Generation (RAG) that integrates cosine similarity and cosine distance measures\n",
            "to improve retrieval performance, particularly for sparse data. The traditional\n",
            "cosine similarity measure is widely used to capture the similarity between\n",
            "vectors in high-dimensional spaces. However, it has been shown that this\n",
            "measure can yield arbitrary results in certain scenarios. To address this\n",
            "limitation, we incorporate cosine distance measures to provide a complementary\n",
            "perspective by quantifying the dissimilarity between vectors. Our approach is\n",
            "experimented on proprietary data, unlike recent publications that have used\n",
            "open-source datasets. The proposed method demonstrates enhanced retrieval\n",
            "performance and provides a more comprehensive understanding of the semantic\n",
            "relationships between documents or items. This hybrid strategy offers a\n",
            "promising solution for efficiently and accurately retrieving relevant\n",
            "information in knowledge-intensive applications, leveraging techniques such as\n",
            "BM25 (sparse) retrieval , vector (Dense) retrieval, and cosine distance based\n",
            "retrieval to facilitate efficient information retrieval.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.01607v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-27T09:52:54Z\n",
            "Title:  Recent advances in text embedding: A Comprehensive Review of\n",
            "  Top-Performing Methods on the MTEB Benchmark\n",
            "Last Author:  Hongliu Cao\n",
            "Authors:  Hongliu Cao\n",
            "abs page link: http://arxiv.org/abs/2406.01607v2\n",
            "pdf link: http://arxiv.org/pdf/2406.01607v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 21 pages\n",
            "Primary Category: cs.IR\n",
            "All Categories: cs.IR, cs.AI, cs.CL\n",
            "Abstract: Text embedding methods have become increasingly popular in both industrial\n",
            "and academic fields due to their critical role in a variety of natural language\n",
            "processing tasks. The significance of universal text embeddings has been\n",
            "further highlighted with the rise of Large Language Models (LLMs) applications\n",
            "such as Retrieval-Augmented Systems (RAGs). While previous models have\n",
            "attempted to be general-purpose, they often struggle to generalize across tasks\n",
            "and domains. However, recent advancements in training data quantity, quality\n",
            "and diversity; synthetic data generation from LLMs as well as using LLMs as\n",
            "backbones encourage great improvements in pursuing universal text embeddings.\n",
            "In this paper, we provide an overview of the recent advances in universal text\n",
            "embedding models with a focus on the top performing text embeddings on Massive\n",
            "Text Embedding Benchmark (MTEB). Through detailed comparison and analysis, we\n",
            "highlight the key contributions and limitations in this area, and propose\n",
            "potentially inspiring future research directions.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.02266v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-04T12:43:23Z\n",
            "Title:  Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning\n",
            "  Compressor\n",
            "Last Author:  Hanwen Xing\n",
            "Authors:  Chuankai Xu, Dongming Zhao, Bo Wang, Hanwen Xing\n",
            "abs page link: http://arxiv.org/abs/2406.02266v1\n",
            "pdf link: http://arxiv.org/pdf/2406.02266v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Despite the prevalence of retrieval-augmented language models (RALMs), the\n",
            "seamless integration of these models with retrieval mechanisms to enhance\n",
            "performance in document-based tasks remains challenging. While some\n",
            "post-retrieval processing Retrieval-Augmented Generation (RAG) methods have\n",
            "achieved success, most still lack the ability to distinguish pertinent from\n",
            "extraneous information, leading to potential inconsistencies and reduced\n",
            "precision in the generated output, which subsequently affects the truthfulness\n",
            "of the language model's responses. To address these limitations, this work\n",
            "proposes a novel two-stage consistency learning approach for retrieved\n",
            "information compression in retrieval-augmented language models to enhance\n",
            "performance. By incorporating consistency learning, the aim is to generate\n",
            "summaries that maintain coherence and alignment with the intended semantic\n",
            "representations of a teacher model while improving faithfulness to the original\n",
            "retrieved documents. The proposed method is empirically validated across\n",
            "multiple datasets, demonstrating notable enhancements in precision and\n",
            "efficiency for question-answering tasks. It outperforms existing baselines and\n",
            "showcases the synergistic effects of combining contrastive and consistency\n",
            "learning paradigms within the retrieval-augmented generation framework.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.02472v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-04T16:42:17Z\n",
            "Title:  Analyzing Temporal Complex Events with Large Language Models? A\n",
            "  Benchmark towards Temporal, Long Context Understanding\n",
            "Last Author:  Tat-Seng Chua\n",
            "Authors:  Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, Tat-Seng Chua\n",
            "abs page link: http://arxiv.org/abs/2406.02472v1\n",
            "pdf link: http://arxiv.org/pdf/2406.02472v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted to ACL 2024\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: The digital landscape is rapidly evolving with an ever-increasing volume of\n",
            "online news, emphasizing the need for swift and precise analysis of complex\n",
            "events. We refer to the complex events composed of many news articles over an\n",
            "extended period as Temporal Complex Event (TCE). This paper proposes a novel\n",
            "approach using Large Language Models (LLMs) to systematically extract and\n",
            "analyze the event chain within TCE, characterized by their key points and\n",
            "timestamps. We establish a benchmark, named TCELongBench, to evaluate the\n",
            "proficiency of LLMs in handling temporal dynamics and understanding extensive\n",
            "text. This benchmark encompasses three distinct tasks - reading comprehension,\n",
            "temporal sequencing, and future event forecasting. In the experiment, we\n",
            "leverage retrieval-augmented generation (RAG) method and LLMs with long context\n",
            "window to deal with lengthy news articles of TCE. Our findings indicate that\n",
            "models with suitable retrievers exhibit comparable performance with those\n",
            "utilizing long context window.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.06458v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-10T16:46:22Z\n",
            "Title:  Evaluating the Retrieval Component in LLM-Based Question Answering\n",
            "  Systems\n",
            "Last Author:  Ali Vahdat\n",
            "Authors:  Ashkan Alinejad, Krtin Kumar, Ali Vahdat\n",
            "abs page link: http://arxiv.org/abs/2406.06458v1\n",
            "pdf link: http://arxiv.org/pdf/2406.06458v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.IR\n",
            "Abstract: Question answering systems (QA) utilizing Large Language Models (LLMs)\n",
            "heavily depend on the retrieval component to provide them with domain-specific\n",
            "information and reduce the risk of generating inaccurate responses or\n",
            "hallucinations. Although the evaluation of retrievers dates back to the early\n",
            "research in Information Retrieval, assessing their performance within LLM-based\n",
            "chatbots remains a challenge.\n",
            "  This study proposes a straightforward baseline for evaluating retrievers in\n",
            "Retrieval-Augmented Generation (RAG)-based chatbots. Our findings demonstrate\n",
            "that this evaluation framework provides a better image of how the retriever\n",
            "performs and is more aligned with the overall performance of the QA system.\n",
            "Although conventional metrics such as precision, recall, and F1 score may not\n",
            "fully capture LLMs' capabilities - as they can yield accurate responses despite\n",
            "imperfect retrievers - our method considers LLMs' strengths to ignore\n",
            "irrelevant contexts, as well as potential errors and hallucinations in their\n",
            "responses.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.07053v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-11T08:35:23Z\n",
            "Title:  TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation\n",
            "  and LLMs\n",
            "Last Author:  Xavier Costa-Perez\n",
            "Authors:  Girma M. Yilma, Jose A. Ayala-Romero, Andres Garcia-Saavedra, Xavier Costa-Perez\n",
            "abs page link: http://arxiv.org/abs/2406.07053v1\n",
            "pdf link: http://arxiv.org/pdf/2406.07053v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 7 pages, 2 figures, 3 tables\n",
            "Primary Category: cs.NI\n",
            "All Categories: cs.NI, cs.LG\n",
            "Abstract: Large Language Models (LLMs) have immense potential to transform the\n",
            "telecommunications industry. They could help professionals understand complex\n",
            "standards, generate code, and accelerate development. However, traditional LLMs\n",
            "struggle with the precision and source verification essential for telecom work.\n",
            "To address this, specialized LLM-based solutions tailored to telecommunication\n",
            "standards are needed. Retrieval-augmented generation (RAG) offers a way to\n",
            "create precise, fact-based answers. This paper proposes TelecomRAG, a framework\n",
            "for a Telecommunication Standards Assistant that provides accurate, detailed,\n",
            "and verifiable responses. Our implementation, using a knowledge base built from\n",
            "3GPP Release 16 and Release 18 specification documents, demonstrates how this\n",
            "assistant surpasses generic LLMs, offering superior accuracy, technical depth,\n",
            "and verifiability, and thus significant value to the telecommunications field.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.07796v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-12T01:19:36Z\n",
            "Title:  Battling Botpoop using GenAI for Higher Education: A Study of a\n",
            "  Retrieval Augmented Generation Chatbots Impact on Learning\n",
            "Last Author:  Leonard W. T. Ng\n",
            "Authors:  Maung Thway, Jose Recatala-Gomez, Fun Siong Lim, Kedar Hippalgaonkar, Leonard W. T. Ng\n",
            "abs page link: http://arxiv.org/abs/2406.07796v2\n",
            "pdf link: http://arxiv.org/pdf/2406.07796v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 13 pages, 5 figures, SI with Annexes A, B and C upon request\n",
            "Primary Category: cs.HC\n",
            "All Categories: cs.HC, cs.AI\n",
            "Abstract: Generative artificial intelligence (GenAI) and large language models (LLMs)\n",
            "have simultaneously opened new avenues for enhancing human learning and\n",
            "increased the prevalence of poor-quality information in student response -\n",
            "termed Botpoop. This study introduces Professor Leodar, a custom-built,\n",
            "Singlish-speaking Retrieval Augmented Generation (RAG) chatbot designed to\n",
            "enhance educational while reducing Botpoop. Deployed at Nanyang Technological\n",
            "University, Singapore, Professor Leodar offers a glimpse into the future of\n",
            "AI-assisted learning, offering personalized guidance, 24/7 availability, and\n",
            "contextually relevant information. Through a mixed-methods approach, we examine\n",
            "the impact of Professor Leodar on learning, engagement, and exam preparedness,\n",
            "with 97.1% of participants reporting positive experiences. These findings help\n",
            "define possible roles of AI in education and highlight the potential of custom\n",
            "GenAI chatbots. Our combination of chatbot development, in-class deployment and\n",
            "outcomes study offers a benchmark for GenAI educational tools and is a stepping\n",
            "stone for redefining the interplay between AI and human learning.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.07990v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-12T08:26:30Z\n",
            "Title:  Blowfish: Topological and statistical signatures for quantifying\n",
            "  ambiguity in semantic search\n",
            "Last Author:  Alex De Castro\n",
            "Authors:  Thomas Roland Barillot, Alex De Castro\n",
            "abs page link: http://arxiv.org/abs/2406.07990v1\n",
            "pdf link: http://arxiv.org/pdf/2406.07990v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.LG\n",
            "All Categories: cs.LG, cs.AI, cs.CL\n",
            "Abstract: This works reports evidence for the topological signatures of ambiguity in\n",
            "sentence embeddings that could be leveraged for ranking and/or explanation\n",
            "purposes in the context of vector search and Retrieval Augmented Generation\n",
            "(RAG) systems. We proposed a working definition of ambiguity and designed an\n",
            "experiment where we have broken down a proprietary dataset into collections of\n",
            "chunks of varying size - 3, 5, and 10 lines and used the different collections\n",
            "successively as queries and answers sets. It allowed us to test the signatures\n",
            "of ambiguity with removal of confounding factors. Our results show that proxy\n",
            "ambiguous queries (size 10 queries against size 3 documents) display different\n",
            "distributions of homologies 0 and 1 based features than proxy clear queries\n",
            "(size 5 queries against size 10 documents). We then discuss those results in\n",
            "terms increased manifold complexity and/or approximately discontinuous\n",
            "embedding submanifolds. Finally we propose a strategy to leverage those\n",
            "findings as a new scoring strategy of semantic similarities.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.09818v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-14T08:21:42Z\n",
            "Title:  ClimRetrieve: A Benchmarking Dataset for Information Retrieval from\n",
            "  Corporate Climate Disclosures\n",
            "Last Author:  Markus Leippold\n",
            "Authors:  Tobias Schimanski, Jingwei Ni, Roberto Spacey, Nicola Ranger, Markus Leippold\n",
            "abs page link: http://arxiv.org/abs/2406.09818v1\n",
            "pdf link: http://arxiv.org/pdf/2406.09818v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.IR\n",
            "All Categories: cs.IR\n",
            "Abstract: To handle the vast amounts of qualitative data produced in corporate climate\n",
            "communication, stakeholders increasingly rely on Retrieval Augmented Generation\n",
            "(RAG) systems. However, a significant gap remains in evaluating domain-specific\n",
            "information retrieval - the basis for answer generation. To address this\n",
            "challenge, this work simulates the typical tasks of a sustainability analyst by\n",
            "examining 30 sustainability reports with 16 detailed climate-related questions.\n",
            "As a result, we obtain a dataset with over 8.5K unique question-source-answer\n",
            "pairs labeled by different levels of relevance. Furthermore, we develop a use\n",
            "case with the dataset to investigate the integration of expert knowledge into\n",
            "information retrieval with embeddings. Although we show that incorporating\n",
            "expert knowledge works, we also outline the critical limitations of embeddings\n",
            "in knowledge-intensive downstream domains like climate change communication.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.11093v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-16T22:49:11Z\n",
            "Title:  RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation\n",
            "  Detection Using In-Context Learning based on Emotional Information\n",
            "Last Author:  Eduard Hovy\n",
            "Authors:  Zhiwei Liu, Kailai Yang, Qianqian Xie, Christine de Kock, Sophia Ananiadou, Eduard Hovy\n",
            "abs page link: http://arxiv.org/abs/2406.11093v1\n",
            "pdf link: http://arxiv.org/pdf/2406.11093v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Misinformation is prevalent in various fields such as education, politics,\n",
            "health, etc., causing significant harm to society. However, current methods for\n",
            "cross-domain misinformation detection rely on time and resources consuming\n",
            "fine-tuning and complex model structures. With the outstanding performance of\n",
            "LLMs, many studies have employed them for misinformation detection.\n",
            "Unfortunately, they focus on in-domain tasks and do not incorporate significant\n",
            "sentiment and emotion features (which we jointly call affect). In this paper,\n",
            "we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to\n",
            "address cross-domain misinformation detection using in-context learning based\n",
            "on affective information. It accomplishes this by applying an emotion-aware LLM\n",
            "to construct a retrieval database of affective embeddings. This database is\n",
            "used by our retrieval module to obtain source-domain samples, which are\n",
            "subsequently used for the inference module's in-context few-shot learning to\n",
            "detect target domain misinformation. We evaluate our framework on three\n",
            "misinformation benchmarks. Results show that RAEmoLLM achieves significant\n",
            "improvements compared to the zero-shot method on three datasets, with the\n",
            "highest increases of 20.69%, 23.94%, and 39.11% respectively. This work will be\n",
            "released on https://github.com/lzw108/RAEmoLLM.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.11177v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-17T03:29:14Z\n",
            "Title:  TIFG: Text-Informed Feature Generation with Large Language Models\n",
            "Last Author:  Kunpeng Liu\n",
            "Authors:  Xinhao Zhang, Jinghan Zhang, Fengran Mo, Yuzhong Chen, Kunpeng Liu\n",
            "abs page link: http://arxiv.org/abs/2406.11177v1\n",
            "pdf link: http://arxiv.org/pdf/2406.11177v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Textual information of data is of vital importance for data mining and\n",
            "feature engineering. However, existing methods focus on learning the data\n",
            "structures and overlook the textual information along with the data.\n",
            "Consequently, they waste this valuable resource and miss out on the deeper data\n",
            "relationships embedded within the texts. In this paper, we introduce\n",
            "Text-Informed Feature Generation (TIFG), a novel LLM-based text-informed\n",
            "feature generation framework. TIFG utilizes the textual information to generate\n",
            "features by retrieving possible relevant features within external knowledge\n",
            "with Retrieval Augmented Generation (RAG) technology. In this approach, the\n",
            "TIFG can generate new explainable features to enrich the feature space and\n",
            "further mine feature relationships. We design the TIFG to be an automated\n",
            "framework that continuously optimizes the feature generation process, adapts to\n",
            "new data inputs, and improves downstream task performance over iterations. A\n",
            "broad range of experiments in various downstream tasks showcases that our\n",
            "approach can generate high-quality and meaningful features, and is\n",
            "significantly superior to existing methods.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.12331v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-18T06:54:28Z\n",
            "Title:  Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text\n",
            "  Understanding\n",
            "Last Author:  Wei Han\n",
            "Authors:  Weizhi Fei, Xueyan Niu, Guoqing Xie, Yanhua Zhang, Bo Bai, Lei Deng, Wei Han\n",
            "abs page link: http://arxiv.org/abs/2406.12331v1\n",
            "pdf link: http://arxiv.org/pdf/2406.12331v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Current Large Language Models (LLMs) face inherent limitations due to their\n",
            "pre-defined context lengths, which impede their capacity for multi-hop\n",
            "reasoning within extensive textual contexts. While existing techniques like\n",
            "Retrieval-Augmented Generation (RAG) have attempted to bridge this gap by\n",
            "sourcing external information, they fall short when direct answers are not\n",
            "readily available. We introduce a novel approach that re-imagines information\n",
            "retrieval through dynamic in-context editing, inspired by recent breakthroughs\n",
            "in knowledge editing. By treating lengthy contexts as malleable external\n",
            "knowledge, our method interactively gathers and integrates relevant\n",
            "information, thereby enabling LLMs to perform sophisticated reasoning steps.\n",
            "Experimental results demonstrate that our method effectively empowers\n",
            "context-limited LLMs, such as Llama2, to engage in multi-hop reasoning with\n",
            "improved performance, which outperforms state-of-the-art context window\n",
            "extrapolation methods and even compares favorably to more advanced commercial\n",
            "long-context models. Our interactive method not only enhances reasoning\n",
            "capabilities but also mitigates the associated training and computational\n",
            "costs, making it a pragmatic solution for enhancing LLMs' reasoning within\n",
            "expansive contexts.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.12338v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-18T07:05:31Z\n",
            "Title:  PARAFAC2-based Coupled Matrix and Tensor Factorizations with Constraints\n",
            "Last Author:  Evrim Acar\n",
            "Authors:  Carla Schenker, Xiulin Wang, David Horner, Morten A. Rasmussen, Evrim Acar\n",
            "abs page link: http://arxiv.org/abs/2406.12338v1\n",
            "pdf link: http://arxiv.org/pdf/2406.12338v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 15 pages, 15 figures,1 table\n",
            "Primary Category: cs.LG\n",
            "All Categories: cs.LG\n",
            "Abstract: Data fusion models based on Coupled Matrix and Tensor Factorizations (CMTF)\n",
            "have been effective tools for joint analysis of data from multiple sources.\n",
            "While the vast majority of CMTF models are based on the strictly multilinear\n",
            "CANDECOMP/PARAFAC (CP) tensor model, recently also the more flexible PARAFAC2\n",
            "model has been integrated into CMTF models. PARAFAC2 tensor models can handle\n",
            "irregular/ragged tensors and have shown to be especially useful for modelling\n",
            "dynamic data with unaligned or irregular time profiles. However, existing\n",
            "PARAFAC2-based CMTF models have limitations in terms of possible\n",
            "regularizations on the factors and/or types of coupling between datasets. To\n",
            "address these limitations, in this paper we introduce a flexible algorithmic\n",
            "framework that fits PARAFAC2-based CMTF models using Alternating Optimization\n",
            "(AO) and the Alternating Direction Method of Multipliers (ADMM). The proposed\n",
            "framework allows to impose various constraints on all modes and linear\n",
            "couplings to other matrix-, CP- or PARAFAC2-models. Experiments on various\n",
            "simulated and a real dataset demonstrate the utility and versatility of the\n",
            "proposed framework as well as its benefits in terms of accuracy and efficiency\n",
            "in comparison with state-of-the-art methods.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.12806v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-18T17:22:48Z\n",
            "Title:  Identifying Performance-Sensitive Configurations in Software Systems\n",
            "  through Code Analysis with LLM Agents\n",
            "Last Author:  Tse-Hsun Chen\n",
            "Authors:  Zehao Wang, Dong Jae Kim, Tse-Hsun Chen\n",
            "abs page link: http://arxiv.org/abs/2406.12806v1\n",
            "pdf link: http://arxiv.org/pdf/2406.12806v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE, cs.AI\n",
            "Abstract: Configuration settings are essential for tailoring software behavior to meet\n",
            "specific performance requirements. However, incorrect configurations are\n",
            "widespread, and identifying those that impact system performance is challenging\n",
            "due to the vast number and complexity of possible settings. In this work, we\n",
            "present PerfSense, a lightweight framework that leverages Large Language Models\n",
            "(LLMs) to efficiently identify performance-sensitive configurations with\n",
            "minimal overhead. PerfSense employs LLM agents to simulate interactions between\n",
            "developers and performance engineers using advanced prompting techniques such\n",
            "as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of\n",
            "seven open-source Java systems demonstrates that PerfSense achieves an average\n",
            "accuracy of 64.77% in classifying performance-sensitive configurations,\n",
            "outperforming both our LLM baseline (50.36%) and the previous state-of-the-art\n",
            "method (61.75%). Notably, our prompt chaining technique improves recall by 10%\n",
            "to 30% while maintaining similar precision levels. Additionally, a manual\n",
            "analysis of 362 misclassifications reveals common issues, including LLMs'\n",
            "misunderstandings of requirements (26.8%). In summary, PerfSense significantly\n",
            "reduces manual effort in classifying performance-sensitive configurations and\n",
            "offers valuable insights for future LLM-based code analysis research.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.13331v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-19T08:29:54Z\n",
            "Title:  Improving Zero-shot LLM Re-Ranker with Risk Minimization\n",
            "Last Author:  Kang Liu\n",
            "Authors:  Xiaowei Yuan, Zhao Yang, Yequan Wang, Jun Zhao, Kang Liu\n",
            "abs page link: http://arxiv.org/abs/2406.13331v1\n",
            "pdf link: http://arxiv.org/pdf/2406.13331v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Under review\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: In the Retrieval-Augmented Generation (RAG) system, advanced Large Language\n",
            "Models (LLMs) have emerged as effective Query Likelihood Models (QLMs) in an\n",
            "unsupervised way, which re-rank documents based on the probability of\n",
            "generating the query given the content of a document. However, directly\n",
            "prompting LLMs to approximate QLMs inherently is biased, where the estimated\n",
            "distribution might diverge from the actual document-specific distribution. In\n",
            "this study, we introduce a novel framework, $\\mathrm{UR^3}$, which leverages\n",
            "Bayesian decision theory to both quantify and mitigate this estimation bias.\n",
            "Specifically, $\\mathrm{UR^3}$ reformulates the problem as maximizing the\n",
            "probability of document generation, thereby harmonizing the optimization of\n",
            "query and document generation probabilities under a unified risk minimization\n",
            "objective. Our empirical results indicate that $\\mathrm{UR^3}$ significantly\n",
            "enhances re-ranking, particularly in improving the Top-1 accuracy. It benefits\n",
            "the QA tasks by achieving higher accuracy with fewer input documents.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.13372v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-19T09:14:41Z\n",
            "Title:  Thread: A Logic-Based Data Organization Paradigm for How-To Question\n",
            "  Answering with Retrieval Augmented Generation\n",
            "Last Author:  Qi Zhang\n",
            "Authors:  Kaikai An, Fangkai Yang, Liqun Li, Junting Lu, Sitao Cheng, Lu Wang, Pu Zhao, Lele Cao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang\n",
            "abs page link: http://arxiv.org/abs/2406.13372v1\n",
            "pdf link: http://arxiv.org/pdf/2406.13372v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 21 pages, 4 figures\n",
            "Primary Category: cs.AI\n",
            "All Categories: cs.AI\n",
            "Abstract: Current question answering systems leveraging retrieval augmented generation\n",
            "perform well in answering factoid questions but face challenges with\n",
            "non-factoid questions, particularly how-to queries requiring detailed\n",
            "step-by-step instructions and explanations. In this paper, we introduce Thread,\n",
            "a novel data organization paradigm that transforms documents into logic units\n",
            "based on their inter-connectivity. Extensive experiments across open-domain and\n",
            "industrial scenarios demonstrate that Thread outperforms existing data\n",
            "organization paradigms in RAG-based QA systems, significantly improving the\n",
            "handling of how-to questions.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.14825v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-21T01:52:37Z\n",
            "Title:  TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction\n",
            "  in RAG-based Crowdsourcing Systems\n",
            "Last Author:  Fei-Yue Wang\n",
            "Authors:  Jing Yang, Yu Zhao, Yang Linyao, Xiao Wang, Long Chen, Fei-Yue Wang\n",
            "abs page link: http://arxiv.org/abs/2406.14825v2\n",
            "pdf link: http://arxiv.org/pdf/2406.14825v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 12 pages, 9 figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Temporal relation extraction (TRE) aims to grasp the evolution of events or\n",
            "actions, and thus shape the workflow of associated tasks, so it holds promise\n",
            "in helping understand task requests initiated by requesters in crowdsourcing\n",
            "systems. However, existing methods still struggle with limited and unevenly\n",
            "distributed annotated data. Therefore, inspired by the abundant global\n",
            "knowledge stored within pre-trained language models (PLMs), we propose a\n",
            "multi-task prompt learning framework for TRE (TemPrompt), incorporating prompt\n",
            "tuning and contrastive learning to tackle these issues. To elicit more\n",
            "effective prompts for PLMs, we introduce a task-oriented prompt construction\n",
            "approach that thoroughly takes the myriad factors of TRE into consideration for\n",
            "automatic prompt generation. In addition, we present temporal event reasoning\n",
            "as a supplement to bolster the model's focus on events and temporal cues. The\n",
            "experimental results demonstrate that TemPrompt outperforms all compared\n",
            "baselines across the majority of metrics under both standard and few-shot\n",
            "settings. A case study is provided to validate its effectiveness in\n",
            "crowdsourcing scenarios.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.14979v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-21T08:45:52Z\n",
            "Title:  Retrieve-Plan-Generation: An Iterative Planning and Answering Framework\n",
            "  for Knowledge-Intensive LLM Generation\n",
            "Last Author:  Enhong Chen\n",
            "Authors:  Yuanjie Lyu, Zihan Niu, Zheyong Xie, Chao Zhang, Tong Xu, Yang Wang, Enhong Chen\n",
            "abs page link: http://arxiv.org/abs/2406.14979v1\n",
            "pdf link: http://arxiv.org/pdf/2406.14979v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Despite the significant progress of large language models (LLMs) in various\n",
            "tasks, they often produce factual errors due to their limited internal\n",
            "knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with\n",
            "external knowledge sources, offers a promising solution. However, these methods\n",
            "can be misled by irrelevant paragraphs in retrieved documents. Due to the\n",
            "inherent uncertainty in LLM generation, inputting the entire document may\n",
            "introduce off-topic information, causing the model to deviate from the central\n",
            "topic and affecting the relevance of the generated content. To address these\n",
            "issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates\n",
            "plan tokens to guide subsequent generation in the plan stage. In the answer\n",
            "stage, the model selects relevant fine-grained paragraphs based on the plan and\n",
            "uses them for further answer generation. This plan-answer process is repeated\n",
            "iteratively until completion, enhancing generation relevance by focusing on\n",
            "specific topics. To implement this framework efficiently, we utilize a simple\n",
            "but effective multi-task prompt-tuning method, enabling the existing LLMs to\n",
            "handle both planning and answering. We comprehensively compare RPG with\n",
            "baselines across 5 knowledge-intensive generation tasks, demonstrating the\n",
            "effectiveness of our approach.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.15045v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-21T10:48:21Z\n",
            "Title:  Harnessing Knowledge Retrieval with Large Language Models for Clinical\n",
            "  Report Error Correction\n",
            "Last Author:  Honghan Wu\n",
            "Authors:  Jinge Wu, Zhaolong Wu, Abul Hasan, Yunsoo Kim, Jason P. Y. Cheung, Teng Zhang, Honghan Wu\n",
            "abs page link: http://arxiv.org/abs/2406.15045v1\n",
            "pdf link: http://arxiv.org/pdf/2406.15045v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: This study proposes an approach for error correction in clinical radiology\n",
            "reports, leveraging large language models (LLMs) and retrieval-augmented\n",
            "generation (RAG) techniques. The proposed framework employs internal and\n",
            "external retrieval mechanisms to extract relevant medical entities and\n",
            "relations from the report and external knowledge sources. A three-stage\n",
            "inference process is introduced, decomposing the task into error detection,\n",
            "localization, and correction subtasks, which enhances the explainability and\n",
            "performance of the system. The effectiveness of the approach is evaluated using\n",
            "a benchmark dataset created by corrupting real-world radiology reports with\n",
            "realistic errors, guided by domain experts. Experimental results demonstrate\n",
            "the benefits of the proposed methods, with the combination of internal and\n",
            "external retrieval significantly improving the accuracy of error detection,\n",
            "localization, and correction across various state-of-the-art LLMs. The findings\n",
            "contribute to the development of more robust and reliable error correction\n",
            "systems for clinical documentation.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.16167v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-23T17:18:19Z\n",
            "Title:  FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy\n",
            "  in Large Language Models\n",
            "Last Author:  Harish Tayyar Madabushi\n",
            "Authors:  Harish Tayyar Madabushi\n",
            "abs page link: http://arxiv.org/abs/2406.16167v1\n",
            "pdf link: http://arxiv.org/pdf/2406.16167v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: program code and prompts available at\n",
            "  https://github.com/H-TayyarMadabushi/A-Frame-Semantics-based-approach-for-Improved-Factual-Accuracy-in-Large-Language-Models\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: We present a novel extension to Retrieval Augmented Generation with the goal\n",
            "of mitigating factual inaccuracies in the output of large language models.\n",
            "Specifically, our method draws on the cognitive linguistic theory of frame\n",
            "semantics for the indexing and retrieval of factual information relevant to\n",
            "helping large language models answer queries. We conduct experiments to\n",
            "demonstrate the effectiveness of this method both in terms of retrieval\n",
            "effectiveness and in terms of the relevance of the frames and frame relations\n",
            "automatically generated. Our results show that this novel mechanism of Frame\n",
            "Semantic-based retrieval, designed to improve Retrieval Augmented Generation\n",
            "(FS-RAG), is effective and offers potential for providing data-driven insights\n",
            "into frame semantics theory. We provide open access to our program code and\n",
            "prompts.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.16383v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-24T07:52:05Z\n",
            "Title:  Context-augmented Retrieval: A Novel Framework for Fast Information\n",
            "  Retrieval based Response Generation using Large Language Model\n",
            "Last Author:  Gautam B\n",
            "Authors:  Sai Ganesh, Anupam Purwar, Gautam B\n",
            "abs page link: http://arxiv.org/abs/2406.16383v1\n",
            "pdf link: http://arxiv.org/pdf/2406.16383v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.IR\n",
            "All Categories: cs.IR\n",
            "Abstract: Generating high-quality answers consistently by providing contextual\n",
            "information embedded in the prompt passed to the Large Language Model (LLM) is\n",
            "dependent on the quality of information retrieval. As the corpus of contextual\n",
            "information grows, the answer/inference quality of Retrieval Augmented\n",
            "Generation (RAG) based Question Answering (QA) systems declines. This work\n",
            "solves this problem by combining classical text classification with the Large\n",
            "Language Model (LLM) to enable quick information retrieval from the vector\n",
            "store and ensure the relevancy of retrieved information. For the same, this\n",
            "work proposes a new approach Context Augmented retrieval (CAR), where\n",
            "partitioning of vector database by real-time classification of information\n",
            "flowing into the corpus is done. CAR demonstrates good quality answer\n",
            "generation along with significant reduction in information retrieval and answer\n",
            "generation time.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.17095v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-24T19:35:11Z\n",
            "Title:  Attention Instruction: Amplifying Attention in the Middle via Prompting\n",
            "Last Author:  Nigel Collier\n",
            "Authors:  Meiru Zhang, Zaiqiao Meng, Nigel Collier\n",
            "abs page link: http://arxiv.org/abs/2406.17095v1\n",
            "pdf link: http://arxiv.org/pdf/2406.17095v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: The context window of large language models has been extended to 128k tokens\n",
            "or more. However, language models still suffer from position bias and have\n",
            "difficulty in accessing and using the middle part of the context due to the\n",
            "lack of attention. We examine the relative position awareness of LLMs and the\n",
            "feasibility of mitigating disproportional attention through prompting. We\n",
            "augment the original task instruction with $\\texttt{attention instructions}$\n",
            "that direct language models to allocate more attention towards a selected\n",
            "segment of the context. We conduct a comprehensive investigation on\n",
            "multi-document question answering task with both position-based and index-based\n",
            "instructions. We find that language models do not have relative position\n",
            "awareness of the context. Nevertheless, they demonstrate the capacity to adapt\n",
            "attention to a specific segment using matching indexes. Our analysis\n",
            "contributes to a deeper understanding of position bias in LLMs and provides a\n",
            "pathway to mitigate this bias by instruction, thus benefiting LLMs in locating\n",
            "and utilizing relevant information from retrieved documents in RAG\n",
            "applications.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.17526v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-25T13:08:35Z\n",
            "Title:  LumberChunker: Long-Form Narrative Document Segmentation\n",
            "Last Author:  Arlindo L. Oliveira\n",
            "Authors:  André V. Duarte, João Marques, Miguel Graça, Miguel Freire, Lei Li, Arlindo L. Oliveira\n",
            "abs page link: http://arxiv.org/abs/2406.17526v1\n",
            "pdf link: http://arxiv.org/pdf/2406.17526v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.IR, I.2\n",
            "Abstract: Modern NLP tasks increasingly rely on dense retrieval methods to access\n",
            "up-to-date and relevant contextual information. We are motivated by the premise\n",
            "that retrieval benefits from segments that can vary in size such that a\n",
            "content's semantic independence is better captured. We propose LumberChunker, a\n",
            "method leveraging an LLM to dynamically segment documents, which iteratively\n",
            "prompts the LLM to identify the point within a group of sequential passages\n",
            "where the content begins to shift. To evaluate our method, we introduce\n",
            "GutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\n",
            "pairs derived from 100 public domain narrative books available on Project\n",
            "Gutenberg. Our experiments show that LumberChunker not only outperforms the\n",
            "most competitive baseline by 7.37% in retrieval performance (DCG@20) but also\n",
            "that, when integrated into a RAG pipeline, LumberChunker proves to be more\n",
            "effective than other chunking methods and competitive baselines, such as the\n",
            "Gemini 1.5M Pro. Our Code and Data are available at\n",
            "https://github.com/joaodsmarques/LumberChunker\n",
            "e-print metadata\n",
            "arxiv-id: 2406.18894v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-27T05:14:34Z\n",
            "Title:  Assessing the Effectiveness of LLMs in Android Application Vulnerability\n",
            "  Analysis\n",
            "Last Author:  Georgios Kambourakis\n",
            "Authors:  Vasileios Kouliaridis, Georgios Karopoulos, Georgios Kambourakis\n",
            "abs page link: http://arxiv.org/abs/2406.18894v1\n",
            "pdf link: http://arxiv.org/pdf/2406.18894v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CR\n",
            "All Categories: cs.CR\n",
            "Abstract: The increasing frequency of attacks on Android applications coupled with the\n",
            "recent popularity of large language models (LLMs) necessitates a comprehensive\n",
            "understanding of the capabilities of the latter in identifying potential\n",
            "vulnerabilities, which is key to mitigate the overall risk. To this end, the\n",
            "work at hand compares the ability of nine state-of-the-art LLMs to detect\n",
            "Android code vulnerabilities listed in the latest Open Worldwide Application\n",
            "Security Project (OWASP) Mobile Top 10. Each LLM was evaluated against an open\n",
            "dataset of over 100 vulnerable code samples, including obfuscated ones,\n",
            "assessing each model's ability to identify key vulnerabilities. Our analysis\n",
            "reveals the strengths and weaknesses of each LLM, identifying important factors\n",
            "that contribute to their performance. Additionally, we offer insights into\n",
            "context augmentation with retrieval-augmented generation (RAG) for detecting\n",
            "Android code vulnerabilities, which in turn may propel secure application\n",
            "development. Finally, while the reported findings regarding code vulnerability\n",
            "analysis show promise, they also reveal significant discrepancies among the\n",
            "different LLMs.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.19309v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-27T16:33:40Z\n",
            "Title:  Which Neurons Matter in IR? Applying Integrated Gradients-based Methods\n",
            "  to Understand Cross-Encoders\n",
            "Last Author:  Benjamin Piwowarski\n",
            "Authors:  Mathias Vast, Basile Van Cooten, Laure Soulier, Benjamin Piwowarski\n",
            "abs page link: http://arxiv.org/abs/2406.19309v1\n",
            "pdf link: http://arxiv.org/pdf/2406.19309v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted at ICTIR 2024\n",
            "Primary Category: cs.IR\n",
            "All Categories: cs.IR\n",
            "Abstract: With the recent addition of Retrieval-Augmented Generation (RAG), the scope\n",
            "and importance of Information Retrieval (IR) has expanded. As a result, the\n",
            "importance of a deeper understanding of IR models also increases. However,\n",
            "interpretability in IR remains under-explored, especially when it comes to the\n",
            "models' inner mechanisms. In this paper, we explore the possibility of adapting\n",
            "Integrated Gradient-based methods in an IR context to identify the role of\n",
            "individual neurons within the model. In particular, we provide new insights\n",
            "into the role of what we call \"relevance\" neurons, as well as how they deal\n",
            "with unseen data. Finally, we carry out an in-depth pruning study to validate\n",
            "our findings.\n",
            "e-print metadata\n",
            "arxiv-id: 2207.06300v1\n",
            "published date type <class 'str'>\n",
            "Published: 2022-07-13T15:51:40Z\n",
            "Title:  Re2G: Retrieve, Rerank, Generate\n",
            "Last Author:  Alfio Gliozzo\n",
            "Authors:  Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, Alfio Gliozzo\n",
            "abs page link: http://arxiv.org/abs/2207.06300v1\n",
            "pdf link: http://arxiv.org/pdf/2207.06300v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted at NAACL 2022\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.IR\n",
            "Abstract: As demonstrated by GPT-3 and T5, transformers grow in capability as parameter\n",
            "spaces become larger and larger. However, for tasks that require a large amount\n",
            "of knowledge, non-parametric memory allows models to grow dramatically with a\n",
            "sub-linear increase in computational cost and GPU memory requirements. Recent\n",
            "models such as RAG and REALM have introduced retrieval into conditional\n",
            "generation. These models incorporate neural initial retrieval from a corpus of\n",
            "passages. We build on this line of research, proposing Re2G, which combines\n",
            "both neural initial retrieval and reranking into a BART-based\n",
            "sequence-to-sequence generation. Our reranking approach also permits merging\n",
            "retrieval results from sources with incomparable scores, enabling an ensemble\n",
            "of BM25 and neural initial retrieval. To train our system end-to-end, we\n",
            "introduce a novel variation of knowledge distillation to train the initial\n",
            "retrieval, reranker, and generation using only ground truth on the target\n",
            "sequence output. We find large gains in four diverse tasks: zero-shot slot\n",
            "filling, question answering, fact-checking, and dialog, with relative gains of\n",
            "9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make\n",
            "our code available as open source at\n",
            "https://github.com/IBM/kgi-slot-filling/tree/re2g.\n",
            "e-print metadata\n",
            "arxiv-id: 2210.02928v2\n",
            "published date type <class 'str'>\n",
            "Published: 2022-10-06T13:58:03Z\n",
            "Title:  MuRAG: Multimodal Retrieval-Augmented Generator for Open Question\n",
            "  Answering over Images and Text\n",
            "Last Author:  William W. Cohen\n",
            "Authors:  Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\n",
            "abs page link: http://arxiv.org/abs/2210.02928v2\n",
            "pdf link: http://arxiv.org/pdf/2210.02928v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted to EMNLP 2022 main conference\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.CV\n",
            "Abstract: While language Models store a massive amount of world knowledge implicitly in\n",
            "their parameters, even very large models often fail to encode information about\n",
            "rare entities and events, while incurring huge computational costs. Recently,\n",
            "retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated\n",
            "world knowledge into language generation by leveraging an external\n",
            "non-parametric index and have demonstrated impressive performance with\n",
            "constrained model sizes. However, these methods are restricted to retrieving\n",
            "only textual knowledge, neglecting the ubiquitous amount of knowledge in other\n",
            "modalities like images -- much of which contains information not covered by any\n",
            "text. To address this limitation, we propose the first Multimodal\n",
            "Retrieval-Augmented Transformer (MuRAG), which accesses an external\n",
            "non-parametric multimodal memory to augment language generation. MuRAG is\n",
            "pre-trained with a mixture of large-scale image-text and text-only corpora\n",
            "using a joint contrastive and generative loss. We perform experiments on two\n",
            "different datasets that require retrieving and reasoning over both images and\n",
            "text to answer a given query: WebQA, and MultimodalQA. Our results show that\n",
            "MuRAG achieves state-of-the-art accuracy, outperforming existing models by\n",
            "10-20\\% absolute on both datasets and under both distractor and full-wiki\n",
            "settings.\n",
            "e-print metadata\n",
            "arxiv-id: 2307.06985v9\n",
            "published date type <class 'str'>\n",
            "Published: 2023-07-13T17:25:28Z\n",
            "Title:  Retrieval Augmented Generation using Engineering Design Knowledge\n",
            "Last Author:  Jianxi Luo\n",
            "Authors:  L. Siddharth, Jianxi Luo\n",
            "abs page link: http://arxiv.org/abs/2307.06985v9\n",
            "pdf link: http://arxiv.org/pdf/2307.06985v9\n",
            "Journal reference: No journal ref found\n",
            "Comments: Resources: Dataset -\n",
            "  https://huggingface.co/datasets/siddharthl1293/engineering_design_facts\n",
            "  Training Infrastructure - https://zenodo.org/records/12012131 Trained model -\n",
            "  https://huggingface.co/siddharthl1293/albert-albert-large-v2 Application -\n",
            "  https://github.com/siddharthl93/engineering-design-knowledge\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.DB, cs.IR\n",
            "Abstract: Aiming to support Retrieval Augmented Generation (RAG) in the design process,\n",
            "we present a method to identify explicit, engineering design facts - {head\n",
            "entity :: relationship :: tail entity} from patented artefact descriptions.\n",
            "Given a sentence with a pair of entities (based on noun phrases) marked in a\n",
            "unique manner, our method extracts the relationship that is explicitly\n",
            "communicated in the sentence. For this task, we create a dataset of 375,084\n",
            "examples and fine-tune language models for relation identification (token\n",
            "classification) and elicitation (sequence-to-sequence). The token\n",
            "classification approach achieves up to 99.7% accuracy. Upon applying the method\n",
            "to a domain of 4,870 fan system patents, we populate a knowledge base of over\n",
            "2.93 million facts. Using this knowledge base, we demonstrate how Large\n",
            "Language Models (LLMs) are guided by explicit facts to synthesise knowledge and\n",
            "generate technical and cohesive responses when sought out for knowledge\n",
            "retrieval tasks in the design process.\n",
            "e-print metadata\n",
            "arxiv-id: 2308.04662v3\n",
            "published date type <class 'str'>\n",
            "Published: 2023-08-09T02:02:46Z\n",
            "Title:  VulLibGen: Generating Names of Vulnerability-Affected Packages via a\n",
            "  Large Language Model\n",
            "Last Author:  Tao Xie\n",
            "Authors:  Tianyu Chen, Lin Li, Liuchuan Zhu, Zongyang Li, Xueqing Liu, Guangtai Liang, Qianxiang Wang, Tao Xie\n",
            "abs page link: http://arxiv.org/abs/2308.04662v3\n",
            "pdf link: http://arxiv.org/pdf/2308.04662v3\n",
            "Journal reference: No journal ref found\n",
            "Comments: ACL 2024 Main Conference\n",
            "Primary Category: cs.CR\n",
            "All Categories: cs.CR\n",
            "Abstract: Security practitioners maintain vulnerability reports (e.g., GitHub Advisory)\n",
            "to help developers mitigate security risks. An important task for these\n",
            "databases is automatically extracting structured information mentioned in the\n",
            "report, e.g., the affected software packages, to accelerate the defense of the\n",
            "vulnerability ecosystem.\n",
            "  However, it is challenging for existing work on affected package\n",
            "identification to achieve a high accuracy. One reason is that all existing work\n",
            "focuses on relatively smaller models, thus they cannot harness the knowledge\n",
            "and semantic capabilities of large language models.\n",
            "  To address this limitation, we propose VulLibGen, the first method to use LLM\n",
            "for affected package identification. In contrast to existing work, VulLibGen\n",
            "proposes the novel idea to directly generate the affected package. To improve\n",
            "the accuracy, VulLibGen employs supervised fine-tuning (SFT), retrieval\n",
            "augmented generation (RAG) and a local search algorithm. The local search\n",
            "algorithm is a novel postprocessing algorithm we introduce for reducing the\n",
            "hallucination of the generated packages. Our evaluation results show that\n",
            "VulLibGen has an average accuracy of 0.806 for identifying vulnerable packages\n",
            "in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go)\n",
            "while the best average accuracy in previous work is 0.721. Additionally,\n",
            "VulLibGen has high value to security practice: we submitted 60 <vulnerability,\n",
            "affected package> pairs to GitHub Advisory (covers four ecosystems). 34 of them\n",
            "have been accepted and merged and 20 are pending approval. Our code and dataset\n",
            "can be found in the attachments.\n",
            "e-print metadata\n",
            "arxiv-id: 2308.10401v1\n",
            "published date type <class 'str'>\n",
            "Published: 2023-08-21T00:30:43Z\n",
            "Title:  Model-Free Large-Scale Cloth Spreading With Mobile Manipulation: Initial\n",
            "  Feasibility Study\n",
            "Last Author:  K. W. Samuel Au\n",
            "Authors:  Xiangyu Chu+, Shengzhi Wang+, Minjian Feng, Jiaxi Zheng, Yuxuan Zhao, Jing Huang, K. W. Samuel Au\n",
            "abs page link: http://arxiv.org/abs/2308.10401v1\n",
            "pdf link: http://arxiv.org/pdf/2308.10401v1\n",
            "Journal reference: 2023 IEEE International Conference on Automation Science and\n",
            "  Engineering (CASE)\n",
            "Comments: 6 pages, 6 figures, submit to CASE2023\n",
            "Primary Category: cs.RO\n",
            "All Categories: cs.RO, cs.SY, eess.SY\n",
            "Abstract: Cloth manipulation is common in domestic and service tasks, and most studies\n",
            "use fixed-base manipulators to manipulate objects whose sizes are relatively\n",
            "small with respect to the manipulators' workspace, such as towels, shirts, and\n",
            "rags. In contrast, manipulation of large-scale cloth, such as bed making and\n",
            "tablecloth spreading, poses additional challenges of reachability and\n",
            "manipulation control. To address them, this paper presents a novel framework to\n",
            "spread large-scale cloth, with a single-arm mobile manipulator that can solve\n",
            "the reachability issue, for an initial feasibility study. On the manipulation\n",
            "control side, without modeling highly deformable cloth, a vision-based\n",
            "manipulation control scheme is applied and based on an online-update Jacobian\n",
            "matrix mapping from selected feature points to the end-effector motion. To\n",
            "coordinate the control of the manipulator and mobile platform, Behavior Trees\n",
            "(BTs) are used because of their modularity. Finally, experiments are conducted,\n",
            "including validation of the model-free manipulation control for cloth spreading\n",
            "in different conditions and the large-scale cloth spreading framework. The\n",
            "experimental results demonstrate the large-scale cloth spreading task\n",
            "feasibility with a single-arm mobile manipulator and the model-free deformation\n",
            "controller.\n",
            "e-print metadata\n",
            "arxiv-id: 2310.01429v1\n",
            "published date type <class 'str'>\n",
            "Published: 2023-09-28T15:32:36Z\n",
            "Title:  Chatmap : Large Language Model Interaction with Cartographic Data\n",
            "Last Author:  Eren Unlu\n",
            "Authors:  Eren Unlu\n",
            "abs page link: http://arxiv.org/abs/2310.01429v1\n",
            "pdf link: http://arxiv.org/pdf/2310.01429v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 9 pages, 4 figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: The swift advancement and widespread availability of foundational Large\n",
            "Language Models (LLMs), complemented by robust fine-tuning methodologies, have\n",
            "catalyzed their adaptation for innovative and industrious applications.\n",
            "Enabling LLMs to recognize and interpret geospatial data, while offering a\n",
            "linguistic access to vast cartographic datasets, is of significant importance.\n",
            "OpenStreetMap (OSM) is the most ambitious open-source global initiative\n",
            "offering detailed urban and rural geographic data, curated by a community of\n",
            "over 10 million contributors, which constitutes a great potential for LLM\n",
            "applications. In this study, we demonstrate the proof of concept and details of\n",
            "the process of fine-tuning a relatively small scale (1B parameters) LLM with a\n",
            "relatively small artificial dataset curated by a more capable teacher model, in\n",
            "order to provide a linguistic interface to the OSM data of an arbitrary urban\n",
            "region. Through this interface, users can inquire about a location's\n",
            "attributes, covering a wide spectrum of concepts, such as its touristic appeal\n",
            "or the potential profitability of various businesses in that vicinity. The\n",
            "study aims to provide an initial guideline for such generative artificial\n",
            "intelligence (AI) adaptations and demonstrate early signs of useful emerging\n",
            "abilities in this context even in minimal computational settings. The\n",
            "embeddings of artificially curated prompts including OSM data are also\n",
            "investigated in detail, which might be instrumental for potential geospatially\n",
            "aware urban Retrieval Augmented Generation (RAG) applications.\n",
            "e-print metadata\n",
            "arxiv-id: 2310.05628v3\n",
            "published date type <class 'str'>\n",
            "Published: 2023-10-09T11:34:41Z\n",
            "Title:  Glitter or Gold? Deriving Structured Insights from Sustainability\n",
            "  Reports via Large Language Models\n",
            "Last Author:  Jacopo Staiano\n",
            "Authors:  Marco Bronzini, Carlo Nicolini, Bruno Lepri, Andrea Passerini, Jacopo Staiano\n",
            "abs page link: http://arxiv.org/abs/2310.05628v3\n",
            "pdf link: http://arxiv.org/pdf/2310.05628v3\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.CE, cs.CY\n",
            "Abstract: Over the last decade, several regulatory bodies have started requiring the\n",
            "disclosure of non-financial information from publicly listed companies, in\n",
            "light of the investors' increasing attention to Environmental, Social, and\n",
            "Governance (ESG) issues. Publicly released information on sustainability\n",
            "practices is often disclosed in diverse, unstructured, and multi-modal\n",
            "documentation. This poses a challenge in efficiently gathering and aligning the\n",
            "data into a unified framework to derive insights related to Corporate Social\n",
            "Responsibility (CSR). Thus, using Information Extraction (IE) methods becomes\n",
            "an intuitive choice for delivering insightful and actionable data to\n",
            "stakeholders. In this study, we employ Large Language Models (LLMs), In-Context\n",
            "Learning, and the Retrieval-Augmented Generation (RAG) paradigm to extract\n",
            "structured insights related to ESG aspects from companies' sustainability\n",
            "reports. We then leverage graph-based representations to conduct statistical\n",
            "analyses concerning the extracted insights. These analyses revealed that ESG\n",
            "criteria cover a wide range of topics, exceeding 500, often beyond those\n",
            "considered in existing categorizations, and are addressed by companies through\n",
            "a variety of initiatives. Moreover, disclosure similarities emerged among\n",
            "companies from the same region or sector, validating ongoing hypotheses in the\n",
            "ESG literature. Lastly, by incorporating additional company attributes into our\n",
            "analyses, we investigated which factors impact the most on companies' ESG\n",
            "ratings, showing that ESG disclosure affects the obtained ratings more than\n",
            "other financial or company data.\n",
            "e-print metadata\n",
            "arxiv-id: 2311.07976v1\n",
            "published date type <class 'str'>\n",
            "Published: 2023-11-14T08:07:11Z\n",
            "Title:  Detailing secondary frontal bore of internal tides breaking above\n",
            "  deep-ocean topography\n",
            "Last Author:  Hans van Haren\n",
            "Authors:  Hans van Haren\n",
            "abs page link: http://arxiv.org/abs/2311.07976v1\n",
            "pdf link: http://arxiv.org/pdf/2311.07976v1\n",
            "Journal reference: Journal of Oceanography, 2023, 79: 581-592\n",
            "Comments: 26 pages, 9 figures\n",
            "Primary Category: physics.ao-ph\n",
            "All Categories: physics.ao-ph\n",
            "Abstract: Above steep deep-sea topography internal tidal waves may break vigorously.\n",
            "The associated turbulent mixing is important for resuspending matter, bringing\n",
            "it tens of meters away from the seafloor for redistribution. While intense\n",
            "turbulence-generation occurs around a primary (frontal) bore during each\n",
            "transition from warming downslope to cooling upslope phase of the internal\n",
            "(tidal) carrier wave, a secondary bore can appear about half a wave-period\n",
            "later before the turn to the warming phase. As will be demonstrated from a\n",
            "100-day mooring array consisting of 200 high-resolution temperature sensors\n",
            "between h = 6-404 m above a steep slope of a large North-Atlantic seamount and\n",
            "a low-resolution acoustic Doppler current profiler sampling between 50 and 450\n",
            "m, secondary bores show about the same turbulence intensity as around primary\n",
            "bores but they generally show larger overturns that always reach the seafloor.\n",
            "The secondary bores associate with a sudden drop in along-isobath flow speed, a\n",
            "(renewed) increase in upslope flow of up to |0.2| m s-1, and with\n",
            "first-harmonic quarter-diurnal periodicity which provides a spectral peak for\n",
            "turbulence dissipation rate. While each bore is different in appearance,\n",
            "varying from curved like a primary bore to almost straight upward with a ragged\n",
            "bore, secondary bores are in a first approximation forward breaking in contrast\n",
            "with backward breaking primary bores.\n",
            "e-print metadata\n",
            "arxiv-id: 2311.17722v1\n",
            "published date type <class 'str'>\n",
            "Published: 2023-11-29T15:21:35Z\n",
            "Title:  SenTest: Evaluating Robustness of Sentence Encoders\n",
            "Last Author:  Raviraj Joshi\n",
            "Authors:  Tanmay Chavan, Shantanu Patankar, Aditya Kane, Omkar Gokhale, Geetanjali Kale, Raviraj Joshi\n",
            "abs page link: http://arxiv.org/abs/2311.17722v1\n",
            "pdf link: http://arxiv.org/pdf/2311.17722v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.LG\n",
            "Abstract: Contrastive learning has proven to be an effective method for pre-training\n",
            "models using weakly labeled data in the vision domain. Sentence transformers\n",
            "are the NLP counterparts to this architecture, and have been growing in\n",
            "popularity due to their rich and effective sentence representations. Having\n",
            "effective sentence representations is paramount in multiple tasks, such as\n",
            "information retrieval, retrieval augmented generation (RAG), and sentence\n",
            "comparison. Keeping in mind the deployability factor of transformers,\n",
            "evaluating the robustness of sentence transformers is of utmost importance.\n",
            "This work focuses on evaluating the robustness of the sentence encoders. We\n",
            "employ several adversarial attacks to evaluate its robustness. This system uses\n",
            "character-level attacks in the form of random character substitution,\n",
            "word-level attacks in the form of synonym replacement, and sentence-level\n",
            "attacks in the form of intra-sentence word order shuffling. The results of the\n",
            "experiments strongly undermine the robustness of sentence encoders. The models\n",
            "produce significantly different predictions as well as embeddings on perturbed\n",
            "datasets. The accuracy of the models can fall up to 15 percent on perturbed\n",
            "datasets as compared to unperturbed datasets. Furthermore, the experiments\n",
            "demonstrate that these embeddings does capture the semantic and syntactic\n",
            "structure (sentence order) of sentences. However, existing supervised\n",
            "classification strategies fail to leverage this information, and merely\n",
            "function as n-gram detectors.\n",
            "e-print metadata\n",
            "arxiv-id: 2312.03141v2\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-05T21:21:01Z\n",
            "Title:  NDSEARCH: Accelerating Graph-Traversal-Based Approximate Nearest\n",
            "  Neighbor Search through Near Data Processing\n",
            "Last Author:  Yiran Chen\n",
            "Authors:  Yitu Wang, Shiyu Li, Qilin Zheng, Linghao Song, Zongwang Li, Andrew Chang, Hai \"Helen\" Li, Yiran Chen\n",
            "abs page link: http://arxiv.org/abs/2312.03141v2\n",
            "pdf link: http://arxiv.org/pdf/2312.03141v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.AR\n",
            "All Categories: cs.AR\n",
            "Abstract: Approximate nearest neighbor search (ANNS) is a key retrieval technique for\n",
            "vector database and many data center applications, such as person\n",
            "re-identification and recommendation systems. It is also fundamental to\n",
            "retrieval augmented generation (RAG) for large language models (LLM) now. Among\n",
            "all the ANNS algorithms, graph-traversal-based ANNS achieves the highest recall\n",
            "rate. However, as the size of dataset increases, the graph may require hundreds\n",
            "of gigabytes of memory, exceeding the main memory capacity of a single\n",
            "workstation node. Although we can do partitioning and use solid-state drive\n",
            "(SSD) as the backing storage, the limited SSD I/O bandwidth severely degrades\n",
            "the performance of the system. To address this challenge, we present NDSEARCH,\n",
            "a hardware-software co-designed near-data processing (NDP) solution for ANNS\n",
            "processing. NDSEARCH consists of a novel in-storage computing architecture,\n",
            "namely, SEARSSD, that supports the ANNS kernels and leverages logic unit\n",
            "(LUN)-level parallelism inside the NAND flash chips. NDSEARCH also includes a\n",
            "processing model that is customized for NDP and cooperates with SEARSSD. The\n",
            "processing model enables us to apply a two-level scheduling to improve the data\n",
            "locality and exploit the internal bandwidth in NDSEARCH, and a speculative\n",
            "searching mechanism to further accelerate the ANNS workload. Our results show\n",
            "that NDSEARCH improves the throughput by up to 31.7x, 14.6x, 7.4x 2.9x over\n",
            "CPU, GPU, a state-of-the-art SmartSSD-only design, and DeepStore, respectively.\n",
            "NDSEARCH also achieves two orders-of-magnitude higher energy efficiency than\n",
            "CPU and GPU.\n",
            "e-print metadata\n",
            "arxiv-id: 2312.04455v4\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-07T17:24:51Z\n",
            "Title:  Fortify the Shortest Stave in Attention: Enhancing Context Awareness of\n",
            "  Large Language Models for Effective Tool Use\n",
            "Last Author:  Rui Yan\n",
            "Authors:  Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, Rui Yan\n",
            "abs page link: http://arxiv.org/abs/2312.04455v4\n",
            "pdf link: http://arxiv.org/pdf/2312.04455v4\n",
            "Journal reference: No journal ref found\n",
            "Comments: ACL 2024 main\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: In this paper, we demonstrate that an inherent waveform pattern in the\n",
            "attention allocation of large language models (LLMs) significantly affects\n",
            "their performance in tasks demanding a high degree of context awareness, such\n",
            "as utilizing LLMs for tool-use. Specifically, the crucial information in the\n",
            "context will be potentially overlooked by model when it is positioned in the\n",
            "trough zone of the attention waveform, leading to decreased performance. To\n",
            "address this issue, we propose a novel inference method named Attention\n",
            "Buckets. It allows LLMs to process their input through multiple parallel\n",
            "processes. Each process utilizes a distinct base angle for the rotary position\n",
            "embedding, thereby creating a unique attention waveform. By compensating an\n",
            "attention trough of a particular process with an attention peak of another\n",
            "process, our approach enhances LLM's awareness to various contextual positions,\n",
            "thus mitigating the risk of overlooking crucial information. In the largest\n",
            "tool-use benchmark, our method elevates a 7B model to achieve state-of-the-art\n",
            "performance, comparable to that of GPT-4. On other benchmarks and some RAG\n",
            "tasks, which also demand a thorough understanding of contextual content,\n",
            "Attention Buckets also exhibited notable enhancements in performance.\n",
            "e-print metadata\n",
            "arxiv-id: 2312.14335v2\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-21T23:42:13Z\n",
            "Title:  Context-aware Decoding Reduces Hallucination in Query-focused\n",
            "  Summarization\n",
            "Last Author:  Zhichao Xu\n",
            "Authors:  Zhichao Xu\n",
            "abs page link: http://arxiv.org/abs/2312.14335v2\n",
            "pdf link: http://arxiv.org/pdf/2312.14335v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: technical report\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.IR\n",
            "Abstract: Query-focused summarization (QFS) aims to provide a summary of a single\n",
            "document/multi documents that can satisfy the information needs of a given\n",
            "query. It is useful for various real-world applications, such as abstractive\n",
            "snippet generation or more recent retrieval augmented generation (RAG). A\n",
            "prototypical QFS pipeline consists of a retriever (sparse or dense retrieval)\n",
            "and a generator (usually a large language model). However, applying large\n",
            "language models (LLM) potentially leads to hallucinations, especially when the\n",
            "evidence contradicts the prior belief of LLMs. There has been growing interest\n",
            "in developing new decoding methods to improve generation quality and reduce\n",
            "hallucination. In this work, we conduct a large-scale reproducibility study on\n",
            "one recently proposed decoding method -- Context-aware Decoding (CAD). In\n",
            "addition to replicating CAD's experiments on news summarization datasets, we\n",
            "include experiments on QFS datasets, and conduct more rigorous analysis on\n",
            "computational complexity and hyperparameter sensitivity. Experiments with eight\n",
            "different language models show that performance-wise, CAD improves QFS quality\n",
            "by (1) reducing factuality errors/hallucinations while (2) mostly retaining the\n",
            "match of lexical patterns, measured by ROUGE scores, while also at a cost of\n",
            "increased inference-time FLOPs and reduced decoding speed. The code\n",
            "implementation based on Huggingface Library is made available\n",
            "https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs\n",
            "e-print metadata\n",
            "arxiv-id: 2312.15883v2\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-26T04:49:56Z\n",
            "Title:  HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and\n",
            "  Reliable Medical LLMs Responses\n",
            "Last Author:  Yasha Wang\n",
            "Authors:  Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding, Xu Chu, Junfeng Zhao, Yasha Wang\n",
            "abs page link: http://arxiv.org/abs/2312.15883v2\n",
            "pdf link: http://arxiv.org/pdf/2312.15883v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: version 2\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: In this paper, we investigate the retrieval-augmented generation (RAG) based\n",
            "on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large\n",
            "Language Models (LLMs). Recent approaches suffer from insufficient and\n",
            "repetitive knowledge retrieval, tedious and time-consuming query parsing, and\n",
            "monotonous knowledge utilization. To this end, we develop a Hypothesis\n",
            "Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs' powerful\n",
            "reasoning capacity to compensate for the incompleteness of user queries,\n",
            "optimizes the interaction process with LLMs, and provides diverse retrieved\n",
            "knowledge. Specifically, HyKGE explores the zero-shot capability and the rich\n",
            "knowledge of LLMs with Hypothesis Outputs to extend feasible exploration\n",
            "directions in the KGs, as well as the carefully curated prompt to enhance the\n",
            "density and efficiency of LLMs' responses. Furthermore, we introduce the HO\n",
            "Fragment Granularity-aware Rerank Module to filter out noise while ensuring the\n",
            "balance between diversity and relevance in retrieved knowledge. Experiments on\n",
            "two Chinese medical multiple-choice question datasets and one Chinese\n",
            "open-domain medical Q&A dataset with two LLM turbos demonstrate the superiority\n",
            "of HyKGE in terms of accuracy and explainability.\n",
            "e-print metadata\n",
            "arxiv-id: 2312.17264v1\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-25T06:44:32Z\n",
            "Title:  ESGReveal: An LLM-based approach for extracting structured data from ESG\n",
            "  reports\n",
            "Last Author:  Wenwen Zhou\n",
            "Authors:  Yi Zou, Mengying Shi, Zhongjie Chen, Zhu Deng, ZongXiong Lei, Zihan Zeng, Shiming Yang, HongXiang Tong, Lei Xiao, Wenwen Zhou\n",
            "abs page link: http://arxiv.org/abs/2312.17264v1\n",
            "pdf link: http://arxiv.org/pdf/2312.17264v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.IR\n",
            "Abstract: ESGReveal is an innovative method proposed for efficiently extracting and\n",
            "analyzing Environmental, Social, and Governance (ESG) data from corporate\n",
            "reports, catering to the critical need for reliable ESG information retrieval.\n",
            "This approach utilizes Large Language Models (LLM) enhanced with Retrieval\n",
            "Augmented Generation (RAG) techniques. The ESGReveal system includes an ESG\n",
            "metadata module for targeted queries, a preprocessing module for assembling\n",
            "databases, and an LLM agent for data extraction. Its efficacy was appraised\n",
            "using ESG reports from 166 companies across various sectors listed on the Hong\n",
            "Kong Stock Exchange in 2022, ensuring comprehensive industry and market\n",
            "capitalization representation. Utilizing ESGReveal unearthed significant\n",
            "insights into ESG reporting with GPT-4, demonstrating an accuracy of 76.9% in\n",
            "data extraction and 83.7% in disclosure analysis, which is an improvement over\n",
            "baseline models. This highlights the framework's capacity to refine ESG data\n",
            "analysis precision. Moreover, it revealed a demand for reinforced ESG\n",
            "disclosures, with environmental and social data disclosures standing at 69.5%\n",
            "and 57.2%, respectively, suggesting a pursuit for more corporate transparency.\n",
            "While current iterations of ESGReveal do not process pictorial information, a\n",
            "functionality intended for future enhancement, the study calls for continued\n",
            "research to further develop and compare the analytical capabilities of various\n",
            "LLMs. In summary, ESGReveal is a stride forward in ESG data processing,\n",
            "offering stakeholders a sophisticated tool to better evaluate and advance\n",
            "corporate sustainability efforts. Its evolution is promising in promoting\n",
            "transparency in corporate reporting and aligning with broader sustainable\n",
            "development aims.\n",
            "e-print metadata\n",
            "arxiv-id: 2401.00544v2\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-31T17:15:25Z\n",
            "Title:  A Reliable Knowledge Processing Framework for Combustion Science using\n",
            "  Foundation Models\n",
            "Last Author:  Venkat Raman\n",
            "Authors:  Vansh Sharma, Venkat Raman\n",
            "abs page link: http://arxiv.org/abs/2401.00544v2\n",
            "pdf link: http://arxiv.org/pdf/2401.00544v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 38 pages and 10 figures; Fixed figure resolution\n",
            "Primary Category: cs.AI\n",
            "All Categories: cs.AI, cs.LG\n",
            "Abstract: This research explores the integration of large language models (LLMs) into\n",
            "scientific data assimilation, focusing on combustion science as a case study.\n",
            "Leveraging foundational models integrated with Retrieval-Augmented Generation\n",
            "(RAG) framework, the study introduces an approach to process diverse combustion\n",
            "research data, spanning experimental studies, simulations, and literature. The\n",
            "multifaceted nature of combustion research emphasizes the critical role of\n",
            "knowledge processing in navigating and extracting valuable information from a\n",
            "vast and diverse pool of sources. The developed approach minimizes\n",
            "computational and economic expenses while optimizing data privacy and accuracy.\n",
            "It incorporates prompt engineering and offline open-source LLMs, offering user\n",
            "autonomy in selecting base models. The study provides a thorough examination of\n",
            "text segmentation strategies, conducts comparative studies between LLMs, and\n",
            "explores various optimized prompts to demonstrate the effectiveness of the\n",
            "framework. By incorporating an external database, the framework outperforms a\n",
            "conventional LLM in generating accurate responses and constructing robust\n",
            "arguments. Additionally, the study delves into the investigation of optimized\n",
            "prompt templates for the purpose of efficient extraction of scientific\n",
            "literature. The research addresses concerns related to hallucinations and false\n",
            "research articles by introducing a custom workflow developed with a detection\n",
            "algorithm to filter out inaccuracies. Despite identified areas for improvement,\n",
            "the framework consistently delivers accurate domain-specific responses with\n",
            "minimal human oversight. The prompt-agnostic approach introduced holds promise\n",
            "for future deliberations. The study underscores the significance of integrating\n",
            "LLMs and knowledge processing techniques in scientific research, providing a\n",
            "foundation for advancements in data assimilation and utilization.\n",
            "e-print metadata\n",
            "arxiv-id: 2401.01701v3\n",
            "published date type <class 'str'>\n",
            "Published: 2024-01-03T12:09:43Z\n",
            "Title:  De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks\n",
            "  via Iterative Grounding\n",
            "Last Author:  Michael Pradel\n",
            "Authors:  Aryaz Eghbali, Michael Pradel\n",
            "abs page link: http://arxiv.org/abs/2401.01701v3\n",
            "pdf link: http://arxiv.org/pdf/2401.01701v3\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE\n",
            "Abstract: Large language models (LLMs) trained on datasets of publicly available source\n",
            "code have established a new state of the art in code generation tasks. However,\n",
            "these models are mostly unaware of the code that exists within a specific\n",
            "project, preventing the models from making good use of existing APIs. Instead,\n",
            "LLMs often invent, or \"hallucinate\", non-existent APIs or produce variants of\n",
            "already existing code. This paper presents De-Hallucinator, a technique that\n",
            "grounds the predictions of an LLM through a novel combination of retrieving\n",
            "suitable API references and iteratively querying the model with increasingly\n",
            "suitable context information in the prompt. The approach exploits the\n",
            "observation that predictions by LLMs often resemble the desired code, but they\n",
            "fail to correctly refer to already existing APIs. De-Hallucinator automatically\n",
            "identifies project-specific API references related to the model's initial\n",
            "predictions and adds these references into the prompt. Unlike\n",
            "retrieval-augmented generation (RAG), our approach uses the initial\n",
            "prediction(s) by the model to iteratively retrieve increasingly suitable API\n",
            "references. Our evaluation applies the approach to two tasks: predicting API\n",
            "usages in Python and generating tests in JavaScript. We show that\n",
            "De-Hallucinator consistently improves the generated code across five LLMs. In\n",
            "particular, the approach improves the edit distance by 23.3-50.6% and the\n",
            "recall of correctly predicted API usages by 23.9-61.0% for code completion, and\n",
            "improves the number of fixed tests that initially failed because of\n",
            "hallucinations by 63.2%, resulting in a 15.5% increase in statement coverage\n",
            "for test generation.\n",
            "e-print metadata\n",
            "arxiv-id: 2401.11391v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-01-21T03:46:00Z\n",
            "Title:  Interactive AI with Retrieval-Augmented Generation for Next Generation\n",
            "  Networking\n",
            "Last Author:  H. Vincent Poor\n",
            "Authors:  Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Sumei Sun, Xuemin Shen, H. Vincent Poor\n",
            "abs page link: http://arxiv.org/abs/2401.11391v1\n",
            "pdf link: http://arxiv.org/pdf/2401.11391v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 10 pages, 4 figures\n",
            "Primary Category: cs.NI\n",
            "All Categories: cs.NI, cs.IT, math.IT\n",
            "Abstract: With the advance of artificial intelligence (AI), the emergence of Google\n",
            "Gemini and OpenAI Q* marks the direction towards artificial general\n",
            "intelligence (AGI). To implement AGI, the concept of interactive AI (IAI) has\n",
            "been introduced, which can interactively understand and respond not only to\n",
            "human user input but also to dynamic system and network conditions. In this\n",
            "article, we explore an integration and enhancement of IAI in networking. We\n",
            "first comprehensively review recent developments and future perspectives of AI\n",
            "and then introduce the technology and components of IAI. We then explore the\n",
            "integration of IAI into the next-generation networks, focusing on how implicit\n",
            "and explicit interactions can enhance network functionality, improve user\n",
            "experience, and promote efficient network management. Subsequently, we propose\n",
            "an IAI-enabled network management and optimization framework, which consists of\n",
            "environment, perception, action, and brain units. We also design the pluggable\n",
            "large language model (LLM) module and retrieval augmented generation (RAG)\n",
            "module to build the knowledge base and contextual memory for decision-making in\n",
            "the brain unit. We demonstrate the effectiveness of the framework through case\n",
            "studies. Finally, we discuss potential research directions for IAI-based\n",
            "networks.\n",
            "e-print metadata\n",
            "arxiv-id: 2401.12998v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-01-20T03:41:23Z\n",
            "Title:  Evaluating and Enhancing Large Language Models Performance in\n",
            "  Domain-specific Medicine: Osteoarthritis Management with DocOA\n",
            "Last Author:  Jian Li\n",
            "Authors:  Xi Chen, MingKe You, Li Wang, WeiZhi Liu, Yu Fu, Jie Xu, Shaoting Zhang, Gang Chen, Kang Li, Jian Li\n",
            "abs page link: http://arxiv.org/abs/2401.12998v1\n",
            "pdf link: http://arxiv.org/pdf/2401.12998v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 16 Pages, 7 Figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: The efficacy of large language models (LLMs) in domain-specific medicine,\n",
            "particularly for managing complex diseases such as osteoarthritis (OA), remains\n",
            "largely unexplored. This study focused on evaluating and enhancing the clinical\n",
            "capabilities of LLMs in specific domains, using osteoarthritis (OA) management\n",
            "as a case study. A domain specific benchmark framework was developed, which\n",
            "evaluate LLMs across a spectrum from domain-specific knowledge to clinical\n",
            "applications in real-world clinical scenarios. DocOA, a specialized LLM\n",
            "tailored for OA management that integrates retrieval-augmented generation (RAG)\n",
            "and instruction prompts, was developed. The study compared the performance of\n",
            "GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human\n",
            "evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less\n",
            "effective in the specialized domain of OA management, particularly in providing\n",
            "personalized treatment recommendations. However, DocOA showed significant\n",
            "improvements. This study introduces a novel benchmark framework which assesses\n",
            "the domain-specific abilities of LLMs in multiple aspects, highlights the\n",
            "limitations of generalized LLMs in clinical contexts, and demonstrates the\n",
            "potential of tailored approaches for developing domain-specific medical LLMs.\n",
            "e-print metadata\n",
            "arxiv-id: 2401.17244v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-01-30T18:37:45Z\n",
            "Title:  LLaMP: Large Language Model Made Powerful for High-fidelity Materials\n",
            "  Knowledge Retrieval and Distillation\n",
            "Last Author:  Janosh Riebesell\n",
            "Authors:  Yuan Chiang, Elvis Hsieh, Chia-Hong Chou, Janosh Riebesell\n",
            "abs page link: http://arxiv.org/abs/2401.17244v2\n",
            "pdf link: http://arxiv.org/pdf/2401.17244v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 31 pages, 5 figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cond-mat.mtrl-sci, cs.AI\n",
            "Abstract: Reducing hallucination of Large Language Models (LLMs) is imperative for use\n",
            "in the sciences, where reliability and reproducibility are crucial. However,\n",
            "LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and\n",
            "inevitably biased task to fine-tune them on domain-specific literature and\n",
            "data. Here we introduce LLaMP, a multimodal retrieval-augmented generation\n",
            "(RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can\n",
            "dynamically and recursively interact with computational and experimental data\n",
            "on Materials Project (MP) and run atomistic simulations via high-throughput\n",
            "workflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage\n",
            "ability to comprehend and integrate various modalities of materials science\n",
            "concepts, fetch relevant data stores on the fly, process higher-order data\n",
            "(such as crystal structure and elastic tensor), and streamline complex tasks in\n",
            "computational materials and chemistry. We propose a simple metric combining\n",
            "uncertainty and confidence estimates to evaluate the self-consistency of\n",
            "responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively\n",
            "mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli,\n",
            "electronic bandgaps, and formation energies that seem to derive from mixed data\n",
            "sources. We also demonstrate LLaMP's capability to edit crystal structures and\n",
            "run annealing molecular dynamics simulations using pre-trained machine-learning\n",
            "force fields. The framework offers an intuitive and nearly hallucination-free\n",
            "approach to exploring and scaling materials informatics, and establishes a\n",
            "pathway for knowledge distillation and fine-tuning other language models. Code\n",
            "and live demo are available at https://github.com/chiang-yuan/llamp\n",
            "e-print metadata\n",
            "arxiv-id: 2402.00746v6\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-01T16:40:32Z\n",
            "Title:  Health-LLM: Personalized Retrieval-Augmented Disease Prediction System\n",
            "Last Author:  Yongfeng Zhang\n",
            "Authors:  Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, Yongfeng Zhang\n",
            "abs page link: http://arxiv.org/abs/2402.00746v6\n",
            "pdf link: http://arxiv.org/pdf/2402.00746v6\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Recent advancements in artificial intelligence (AI), especially large\n",
            "language models (LLMs), have significantly advanced healthcare applications and\n",
            "demonstrated potentials in intelligent medical treatment. However, there are\n",
            "conspicuous challenges such as vast data volumes and inconsistent symptom\n",
            "characterization standards, preventing full integration of healthcare AI\n",
            "systems with individual patients' needs. To promote professional and\n",
            "personalized healthcare, we propose an innovative framework, Heath-LLM, which\n",
            "combines large-scale feature extraction and medical knowledge trade-off\n",
            "scoring. Compared to traditional health management applications, our system has\n",
            "three main advantages: (1) It integrates health reports and medical knowledge\n",
            "into a large model to ask relevant questions to large language model for\n",
            "disease prediction; (2) It leverages a retrieval augmented generation (RAG)\n",
            "mechanism to enhance feature extraction; (3) It incorporates a semi-automated\n",
            "feature updating framework that can merge and delete features to improve\n",
            "accuracy of disease prediction. We experiment on a large number of health\n",
            "reports to assess the effectiveness of Health-LLM system. The results indicate\n",
            "that the proposed system surpasses the existing ones and has the potential to\n",
            "significantly advance disease prediction and personalized health management.\n",
            "The code is available at https://github.com/jmyissb/HealthLLM.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.01788v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-02T02:41:28Z\n",
            "Title:  LitLLM: A Toolkit for Scientific Literature Review\n",
            "Last Author:  Christopher Pal\n",
            "Authors:  Shubham Agarwal, Issam H. Laradji, Laurent Charlin, Christopher Pal\n",
            "abs page link: http://arxiv.org/abs/2402.01788v1\n",
            "pdf link: http://arxiv.org/pdf/2402.01788v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.IR\n",
            "Abstract: Conducting literature reviews for scientific papers is essential for\n",
            "understanding research, its limitations, and building on existing work. It is a\n",
            "tedious task which makes an automatic literature review generator appealing.\n",
            "Unfortunately, many existing works that generate such reviews using Large\n",
            "Language Models (LLMs) have significant limitations. They tend to\n",
            "hallucinate-generate non-actual information-and ignore the latest research they\n",
            "have not been trained on. To address these limitations, we propose a toolkit\n",
            "that operates on Retrieval Augmented Generation (RAG) principles, specialized\n",
            "prompting and instructing techniques with the help of LLMs. Our system first\n",
            "initiates a web search to retrieve relevant papers by summarizing user-provided\n",
            "abstracts into keywords using an off-the-shelf LLM. Authors can enhance the\n",
            "search by supplementing it with relevant papers or keywords, contributing to a\n",
            "tailored retrieval process. Second, the system re-ranks the retrieved papers\n",
            "based on the user-provided abstract. Finally, the related work section is\n",
            "generated based on the re-ranked results and the abstract. There is a\n",
            "substantial reduction in time and effort for literature review compared to\n",
            "traditional methods, establishing our toolkit as an efficient alternative. Our\n",
            "open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM\n",
            "and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM)\n",
            "with the video demo at https://youtu.be/E2ggOZBAFw0.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.01828v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-02T18:23:09Z\n",
            "Title:  Retrieval Augmented End-to-End Spoken Dialog Models\n",
            "Last Author:  Laurent El Shafey\n",
            "Authors:  Mingqiu Wang, Izhak Shafran, Hagen Soltau, Wei Han, Yuan Cao, Dian Yu, Laurent El Shafey\n",
            "abs page link: http://arxiv.org/abs/2402.01828v1\n",
            "pdf link: http://arxiv.org/pdf/2402.01828v1\n",
            "Journal reference: Proc. ICASSP 2024\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.SD, eess.AS\n",
            "Abstract: We recently developed SLM, a joint speech and language model, which fuses a\n",
            "pretrained foundational speech model and a large language model (LLM), while\n",
            "preserving the in-context learning capability intrinsic to the pretrained LLM.\n",
            "In this paper, we apply SLM to speech dialog applications where the dialog\n",
            "states are inferred directly from the audio signal.\n",
            "  Task-oriented dialogs often contain domain-specific entities, i.e.,\n",
            "restaurants, hotels, train stations, and city names, which are difficult to\n",
            "recognize, however, critical for the downstream applications. Inspired by the\n",
            "RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented\n",
            "SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to\n",
            "retrieve text entities mentioned in the audio. The retrieved entities are then\n",
            "added as text inputs to the underlying SLM to bias model predictions. We\n",
            "evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that\n",
            "this retrieval augmentation boosts model performance, achieving joint goal\n",
            "accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error\n",
            "rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach\n",
            "is broadly applicable to other speech tasks requiring contextual information or\n",
            "domain-specific entities, such as contextual ASR with biasing capability.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.02008v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-03T03:44:57Z\n",
            "Title:  How well do LLMs cite relevant medical references? An evaluation\n",
            "  framework and analyses\n",
            "Last Author:  James Zou\n",
            "Authors:  Kevin Wu, Eric Wu, Ally Cassasola, Angela Zhang, Kevin Wei, Teresa Nguyen, Sith Riantawan, Patricia Shi Riantawan, Daniel E. Ho, James Zou\n",
            "abs page link: http://arxiv.org/abs/2402.02008v1\n",
            "pdf link: http://arxiv.org/pdf/2402.02008v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Large language models (LLMs) are currently being used to answer medical\n",
            "questions across a variety of clinical domains. Recent top-performing\n",
            "commercial LLMs, in particular, are also capable of citing sources to support\n",
            "their responses. In this paper, we ask: do the sources that LLMs generate\n",
            "actually support the claims that they make? To answer this, we propose three\n",
            "contributions. First, as expert medical annotations are an expensive and\n",
            "time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is\n",
            "highly accurate in validating source relevance, agreeing 88% of the time with a\n",
            "panel of medical doctors. Second, we develop an end-to-end, automated pipeline\n",
            "called \\textit{SourceCheckup} and use it to evaluate five top-performing LLMs\n",
            "on a dataset of 1200 generated questions, totaling over 40K pairs of statements\n",
            "and sources. Interestingly, we find that between ~50% to 90% of LLM responses\n",
            "are not fully supported by the sources they provide. We also evaluate GPT-4\n",
            "with retrieval augmented generation (RAG) and find that, even still, around\n",
            "30\\% of individual statements are unsupported, while nearly half of its\n",
            "responses are not fully supported. Third, we open-source our curated dataset of\n",
            "medical questions and expert annotations for future evaluations. Given the\n",
            "rapid pace of LLM development and the potential harms of incorrect or outdated\n",
            "medical information, it is crucial to also understand and quantify their\n",
            "capability to produce relevant, trustworthy medical references.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.09939v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-15T13:39:55Z\n",
            "Title:  Generative AI in the Construction Industry: A State-of-the-art Analysis\n",
            "Last Author:  Tarek Zayed\n",
            "Authors:  Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed\n",
            "abs page link: http://arxiv.org/abs/2402.09939v1\n",
            "pdf link: http://arxiv.org/pdf/2402.09939v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 74 pages, 11 figures, 20 tables\n",
            "Primary Category: cs.AI\n",
            "All Categories: cs.AI, cs.CL, cs.HC, cs.IR, cs.LG\n",
            "Abstract: The construction industry is a vital sector of the global economy, but it\n",
            "faces many productivity challenges in various processes, such as design,\n",
            "planning, procurement, inspection, and maintenance. Generative artificial\n",
            "intelligence (AI), which can create novel and realistic data or content, such\n",
            "as text, image, video, or code, based on some input or prior knowledge, offers\n",
            "innovative and disruptive solutions to address these challenges. However, there\n",
            "is a gap in the literature on the current state, opportunities, and challenges\n",
            "of generative AI in the construction industry. This study aims to fill this gap\n",
            "by providing a state-of-the-art analysis of generative AI in construction, with\n",
            "three objectives: (1) to review and categorize the existing and emerging\n",
            "generative AI opportunities and challenges in the construction industry; (2) to\n",
            "propose a framework for construction firms to build customized generative AI\n",
            "solutions using their own data, comprising steps such as data collection,\n",
            "dataset curation, training custom large language model (LLM), model evaluation,\n",
            "and deployment; and (3) to demonstrate the framework via a case study of\n",
            "developing a generative model for querying contract documents. The results show\n",
            "that retrieval augmented generation (RAG) improves the baseline LLM by 5.2,\n",
            "9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study\n",
            "provides academics and construction professionals with a comprehensive analysis\n",
            "and practical framework to guide the adoption of generative AI techniques to\n",
            "enhance productivity, quality, safety, and sustainability across the\n",
            "construction industry.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.11034v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-16T19:26:09Z\n",
            "Title:  PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal\n",
            "  Question-Answering\n",
            "Last Author:  Vagelis Hristidis\n",
            "Authors:  Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis\n",
            "abs page link: http://arxiv.org/abs/2402.11034v2\n",
            "pdf link: http://arxiv.org/pdf/2402.11034v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted to Findings of ACL '24\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Existing work on Temporal Question Answering (TQA) has predominantly focused\n",
            "on questions anchored to specific timestamps or events (e.g. \"Who was the US\n",
            "president in 1970?\"). Little work has studied questions whose temporal context\n",
            "is relative to the present time (e.g. \"Who was the previous US president?\"). We\n",
            "refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses\n",
            "unique challenges: (1) large language models (LLMs) may have outdated\n",
            "knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are\n",
            "hard to reason, (3) multi-hop reasoning may be required, and (4) the gold\n",
            "answers of benchmarks must be continuously updated. To address these\n",
            "challenges, we introduce the PAT-Questions benchmark, which includes single and\n",
            "multi-hop temporal questions. The answers in PAT-Questions can be automatically\n",
            "refreshed by re-running SPARQL queries on a knowledge graph, if available. We\n",
            "evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model\n",
            "(TEMPREASON-T5) on PAT-Questions through direct prompting and\n",
            "retrieval-augmented generation (RAG). The results highlight the limitations of\n",
            "existing solutions in PATQA and motivate the need for new methods to improve\n",
            "PATQA reasoning capabilities.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.12170v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-16T06:29:16Z\n",
            "Title:  Where is the answer? Investigating Positional Bias in Language Model\n",
            "  Knowledge Extraction\n",
            "Last Author:  Yoshitaka Ushiku\n",
            "Authors:  Kuniaki Saito, Kihyuk Sohn, Chen-Yu Lee, Yoshitaka Ushiku\n",
            "abs page link: http://arxiv.org/abs/2402.12170v2\n",
            "pdf link: http://arxiv.org/pdf/2402.12170v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Large language models require updates to remain up-to-date or adapt to new\n",
            "domains by fine-tuning them with new documents. One key is memorizing the\n",
            "latest information in a way that the memorized information is extractable with\n",
            "a query prompt. However, LLMs suffer from a phenomenon called perplexity curse;\n",
            "despite minimizing document perplexity during fine-tuning, LLMs struggle to\n",
            "extract information through a prompt sentence. In this new knowledge\n",
            "acquisition and extraction, we find a very intriguing fact that LLMs can\n",
            "accurately answer questions about the first sentence, but they struggle to\n",
            "extract information described in the middle or end of the documents used for\n",
            "fine-tuning. Our study suggests that the auto-regressive training causes this\n",
            "issue; each token is prompted by reliance on all previous tokens, which hinders\n",
            "the model from recalling information from training documents by question\n",
            "prompts. To conduct the in-depth study, we publish both synthetic and real\n",
            "datasets, enabling the evaluation of the QA performance w.r.t. the position of\n",
            "the corresponding answer in a document. Our investigation shows that even a\n",
            "large model suffers from the perplexity curse, but regularization such as\n",
            "denoising auto-regressive loss can enhance the information extraction from\n",
            "diverse positions. These findings will be (i) a key to improving knowledge\n",
            "extraction from LLMs and (ii) new elements to discuss the trade-off between RAG\n",
            "and fine-tuning in adapting LLMs to a new domain.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.12869v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-20T10:00:58Z\n",
            "Title:  Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based\n",
            "  Question Answering with Domain Hybrid Data\n",
            "Last Author:  Qianren Wang\n",
            "Authors:  Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang\n",
            "abs page link: http://arxiv.org/abs/2402.12869v2\n",
            "pdf link: http://arxiv.org/pdf/2402.12869v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted to NAACL 2024 Industry Track Paper\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Augmenting Large Language Models (LLMs) for Question Answering (QA) with\n",
            "domain specific data has attracted wide attention. However, domain data often\n",
            "exists in a hybrid format, including text and semi-structured tables, posing\n",
            "challenges for the seamless integration of information. Table-to-Text\n",
            "Generation is a promising solution by facilitating the transformation of hybrid\n",
            "data into a uniformly text-formatted corpus. Although this technique has been\n",
            "widely studied by the NLP community, there is currently no comparative analysis\n",
            "on how corpora generated by different table-to-text methods affect the\n",
            "performance of QA systems. In this paper, we address this research gap in two\n",
            "steps. First, we innovatively integrate table-to-text generation into the\n",
            "framework of enhancing LLM-based QA systems with domain hybrid data. Then, we\n",
            "utilize this framework in real-world industrial data to conduct extensive\n",
            "experiments on two types of QA systems (DSFT and RAG frameworks) with four\n",
            "representative methods: Markdown format, Template serialization, TPLM-based\n",
            "method, and LLM-based method. Based on the experimental results, we draw some\n",
            "empirical findings and explore the underlying reasons behind the success of\n",
            "some methods. We hope the findings of this work will provide a valuable\n",
            "reference for the academic and industrial communities in developing robust QA\n",
            "systems.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.14480v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-22T12:13:35Z\n",
            "Title:  MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems\n",
            "  in LLM Augmented Generation\n",
            "Last Author:  Kailong Wang\n",
            "Authors:  Guanyu Wang, Yuekang Li, Yi Liu, Gelei Deng, Tianlin Li, Guosheng Xu, Yang Liu, Haoyu Wang, Kailong Wang\n",
            "abs page link: http://arxiv.org/abs/2402.14480v1\n",
            "pdf link: http://arxiv.org/pdf/2402.14480v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE\n",
            "Abstract: Augmented generation techniques such as Retrieval-Augmented Generation (RAG)\n",
            "and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing\n",
            "large language model (LLM) outputs with external knowledge and cached\n",
            "information. However, the integration of vector databases, which serve as a\n",
            "backbone for these augmentations, introduces critical challenges, particularly\n",
            "in ensuring accurate vector matching. False vector matching in these databases\n",
            "can significantly compromise the integrity and reliability of LLM outputs,\n",
            "leading to misinformation or erroneous responses. Despite the crucial impact of\n",
            "these issues, there is a notable research gap in methods to effectively detect\n",
            "and address false vector matches in LLM-augmented generation. This paper\n",
            "presents MeTMaP, a metamorphic testing framework developed to identify false\n",
            "vector matching in LLM-augmented generation systems. We derive eight\n",
            "metamorphic relations (MRs) from six NLP datasets, which form our method's\n",
            "core, based on the idea that semantically similar texts should match and\n",
            "dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets\n",
            "for testing, simulating real-world LLM scenarios. Our evaluation of MeTMaP over\n",
            "203 vector matching configurations, involving 29 embedding models and 7\n",
            "distance metrics, uncovers significant inaccuracies. The results, showing a\n",
            "maximum accuracy of only 41.51\\% on our tests compared to the original\n",
            "datasets, emphasize the widespread issue of false matches in vector matching\n",
            "methods and the critical need for effective detection and mitigation in\n",
            "LLM-augmented applications.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.00872v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-01T07:14:45Z\n",
            "Title:  DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy\n",
            "  in Large-Scale Databases\n",
            "Last Author:  Mustafa Panbiharwala\n",
            "Authors:  Shai Volvovsky, Marco Marcassa, Mustafa Panbiharwala\n",
            "abs page link: http://arxiv.org/abs/2403.00872v1\n",
            "pdf link: http://arxiv.org/pdf/2403.00872v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.DB\n",
            "All Categories: cs.DB, cs.AI\n",
            "Abstract: The task of converting natural language queries into SQL queries is\n",
            "intricate, necessitating a blend of precise techniques for an accurate\n",
            "translation. The DIN-SQL (Decomposed-In-Context SQL) methodology represents a\n",
            "significant development in this domain. This paper introduces DFIN (Decomposed\n",
            "Focused-In-Context), an innovative extension of DIN-SQL that enhances\n",
            "Text-to-SQL conversion by addressing schema linking errors, which are a major\n",
            "source of inaccuracies. DFIN uniquely alternates between prompting techniques\n",
            "and Retrieval-Augmented Generation (RAG), adapting to the size and complexity\n",
            "of the database schema. A preprocessing phase embeds database definitions and\n",
            "leverages annotated files, akin to those in the BIRD dataset, facilitating the\n",
            "runtime retrieval of pertinent schema information. This strategy significantly\n",
            "reduces the token count for schema linking prompts, enabling the use of a\n",
            "standard GPT-4 model over its larger context variant, thus handling large-scale\n",
            "databases more effectively and economically. Our evaluation on the BIRD\n",
            "dataset, a challenging real-world benchmark, demonstrates that DFIN not only\n",
            "scales efficiently but also improves accuracy, achieving a score of 51.69. This\n",
            "improvement surpasses DIN-SQL method (the current third-place), which is the\n",
            "highest-ranked model employing in-context learning rather than fine-tuning,\n",
            "previously scoring 50.72. The advancement of DFIN underscores the evolving\n",
            "capabilities of in-context learning methodologies combined with advanced\n",
            "language models, offering a promising avenue for future research in complex\n",
            "Text-to-SQL conversion tasks.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.08345v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-13T08:50:15Z\n",
            "Title:  From human experts to machines: An LLM supported approach to ontology\n",
            "  and knowledge graph construction\n",
            "Last Author:  Sheeba Samuel\n",
            "Authors:  Vamsi Krishna Kommineni, Birgitta König-Ries, Sheeba Samuel\n",
            "abs page link: http://arxiv.org/abs/2403.08345v1\n",
            "pdf link: http://arxiv.org/pdf/2403.08345v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: The conventional process of building Ontologies and Knowledge Graphs (KGs)\n",
            "heavily relies on human domain experts to define entities and relationship\n",
            "types, establish hierarchies, maintain relevance to the domain, fill the ABox\n",
            "(or populate with instances), and ensure data quality (including amongst others\n",
            "accuracy and completeness). On the other hand, Large Language Models (LLMs)\n",
            "have recently gained popularity for their ability to understand and generate\n",
            "human-like natural language, offering promising ways to automate aspects of\n",
            "this process. This work explores the (semi-)automatic construction of KGs\n",
            "facilitated by open-source LLMs. Our pipeline involves formulating competency\n",
            "questions (CQs), developing an ontology (TBox) based on these CQs, constructing\n",
            "KGs using the developed ontology, and evaluating the resultant KG with minimal\n",
            "to no involvement of human experts. We showcase the feasibility of our\n",
            "semi-automated pipeline by creating a KG on deep learning methodologies by\n",
            "exploiting scholarly publications. To evaluate the answers generated via\n",
            "Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically\n",
            "extracted using LLMs, we design a judge LLM, which rates the generated content\n",
            "based on ground truth. Our findings suggest that employing LLMs could\n",
            "potentially reduce the human effort involved in the construction of KGs,\n",
            "although a human-in-the-loop approach is recommended to evaluate automatically\n",
            "generated KGs.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.10588v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-15T17:04:27Z\n",
            "Title:  S3LLM: Large-Scale Scientific Software Understanding with LLMs using\n",
            "  Source, Metadata, and Document\n",
            "Last Author:  Yunhe Feng\n",
            "Authors:  Kareem Shaik, Dali Wang, Weijian Zheng, Qinglei Cao, Heng Fan, Peter Schwartz, Yunhe Feng\n",
            "abs page link: http://arxiv.org/abs/2403.10588v1\n",
            "pdf link: http://arxiv.org/pdf/2403.10588v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE, cs.AI\n",
            "Abstract: The understanding of large-scale scientific software poses significant\n",
            "challenges due to its diverse codebase, extensive code length, and target\n",
            "computing architectures. The emergence of generative AI, specifically large\n",
            "language models (LLMs), provides novel pathways for understanding such complex\n",
            "scientific codes. This paper presents S3LLM, an LLM-based framework designed to\n",
            "enable the examination of source code, code metadata, and summarized\n",
            "information in conjunction with textual technical reports in an interactive,\n",
            "conversational manner through a user-friendly interface. S3LLM leverages\n",
            "open-source LLaMA-2 models to enhance code analysis through the automatic\n",
            "transformation of natural language queries into domain-specific language (DSL)\n",
            "queries. Specifically, it translates these queries into Feature Query Language\n",
            "(FQL), enabling efficient scanning and parsing of entire code repositories. In\n",
            "addition, S3LLM is equipped to handle diverse metadata types, including DOT,\n",
            "SQL, and customized formats. Furthermore, S3LLM incorporates retrieval\n",
            "augmented generation (RAG) and LangChain technologies to directly query\n",
            "extensive documents. S3LLM demonstrates the potential of using locally deployed\n",
            "open-source LLMs for the rapid understanding of large-scale scientific\n",
            "computing software, eliminating the need for extensive coding expertise, and\n",
            "thereby making the process more efficient and effective. S3LLM is available at\n",
            "https://github.com/ResponsibleAILab/s3llm.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.12582v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-19T09:45:33Z\n",
            "Title:  AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented\n",
            "  Stock-Chain Framework\n",
            "Last Author:  Wei Lin\n",
            "Authors:  Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun Huang, Wei Lin\n",
            "abs page link: http://arxiv.org/abs/2403.12582v1\n",
            "pdf link: http://arxiv.org/pdf/2403.12582v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: COLING 2024. The first three authors contributed equally. Project\n",
            "  website: https://github.com/AlphaFin-proj/AlphaFin\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: The task of financial analysis primarily encompasses two key areas: stock\n",
            "trend prediction and the corresponding financial question answering. Currently,\n",
            "machine learning and deep learning algorithms (ML&DL) have been widely applied\n",
            "for stock trend predictions, leading to significant progress. However, these\n",
            "methods fail to provide reasons for predictions, lacking interpretability and\n",
            "reasoning processes. Also, they can not integrate textual information such as\n",
            "financial news or reports. Meanwhile, large language models (LLMs) have\n",
            "remarkable textual understanding and generation ability. But due to the\n",
            "scarcity of financial training datasets and limited integration with real-time\n",
            "knowledge, LLMs still suffer from hallucinations and are unable to keep up with\n",
            "the latest information. To tackle these challenges, we first release AlphaFin\n",
            "datasets, combining traditional research datasets, real-time financial data,\n",
            "and handwritten chain-of-thought (CoT) data. It has a positive impact on\n",
            "training LLMs for completing financial analysis. We then use AlphaFin datasets\n",
            "to benchmark a state-of-the-art method, called Stock-Chain, for effectively\n",
            "tackling the financial analysis task, which integrates retrieval-augmented\n",
            "generation (RAG) techniques. Extensive experiments are conducted to demonstrate\n",
            "the effectiveness of our framework on financial analysis.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.14403v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-21T13:52:30Z\n",
            "Title:  Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\n",
            "  Models through Question Complexity\n",
            "Last Author:  Jong C. Park\n",
            "Authors:  Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park\n",
            "abs page link: http://arxiv.org/abs/2403.14403v2\n",
            "pdf link: http://arxiv.org/pdf/2403.14403v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: NAACL 2024\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Retrieval-Augmented Large Language Models (LLMs), which incorporate the\n",
            "non-parametric knowledge from external knowledge bases into LLMs, have emerged\n",
            "as a promising approach to enhancing response accuracy in several tasks, such\n",
            "as Question-Answering (QA). However, even though there are various approaches\n",
            "dealing with queries of different complexities, they either handle simple\n",
            "queries with unnecessary computational overhead or fail to adequately address\n",
            "complex multi-step queries; yet, not all user requests fall into only one of\n",
            "the simple or complex categories. In this work, we propose a novel adaptive QA\n",
            "framework, that can dynamically select the most suitable strategy for\n",
            "(retrieval-augmented) LLMs from the simplest to the most sophisticated ones\n",
            "based on the query complexity. Also, this selection process is operationalized\n",
            "with a classifier, which is a smaller LM trained to predict the complexity\n",
            "level of incoming queries with automatically collected labels, obtained from\n",
            "actual predicted outcomes of models and inherent inductive biases in datasets.\n",
            "This approach offers a balanced strategy, seamlessly adapting between the\n",
            "iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval\n",
            "methods, in response to a range of query complexities. We validate our model on\n",
            "a set of open-domain QA datasets, covering multiple query complexities, and\n",
            "show that ours enhances the overall efficiency and accuracy of QA systems,\n",
            "compared to relevant baselines including the adaptive retrieval approaches.\n",
            "Code is available at: https://github.com/starsuzi/Adaptive-RAG.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.19116v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-28T03:14:18Z\n",
            "Title:  MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering\n",
            "Last Author:  Peng Zhang\n",
            "Authors:  Che Guan, Mengyu Huang, Peng Zhang\n",
            "abs page link: http://arxiv.org/abs/2403.19116v1\n",
            "pdf link: http://arxiv.org/pdf/2403.19116v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 8 pages\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: In today's fast-paced industry, professionals face the challenge of\n",
            "summarizing a large number of documents and extracting vital information from\n",
            "them on a daily basis. These metrics are frequently hidden away in tables\n",
            "and/or their nested hyperlinks. To address this challenge, the approach of\n",
            "Table Question Answering (QA) has been developed to extract the relevant\n",
            "information. However, traditional Table QA training tasks that provide a table\n",
            "and an answer(s) from a gold cell coordinate(s) for a question may not always\n",
            "ensure extracting the accurate answer(s). Recent advancements in Large Language\n",
            "Models (LLMs) have opened up new possibilities for extracting information from\n",
            "tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot\n",
            "Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The\n",
            "first step involves Few-Shot Learning (FSL), where relevant tables and\n",
            "associated contexts of hyperlinks are retrieved based on a given question. The\n",
            "retrieved content is then used to construct few-shot prompts as inputs to an\n",
            "LLM, such as ChatGPT. To tackle the challenge of answering complex questions,\n",
            "the second step leverages Chain-of-thought (CoT) prompting to decompose the\n",
            "complex question into a sequential chain of questions and reasoning thoughts in\n",
            "a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process\n",
            "by retrieving relevant tables and contexts of hyperlinks that are relevant to\n",
            "the resulting reasoning thoughts and questions. These additional contexts are\n",
            "then used to supplement the prompt used in the first step, resulting in more\n",
            "accurate answers from an LLM. Empirical results from OTT-QA demonstrate that\n",
            "our abstractive QA approach significantly improves the accuracy of extractive\n",
            "Table QA methods.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.00486v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-30T22:41:05Z\n",
            "Title:  Dialectical Alignment: Resolving the Tension of 3H and Security Threats\n",
            "  of LLMs\n",
            "Last Author:  Di Wang\n",
            "Authors:  Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang\n",
            "abs page link: http://arxiv.org/abs/2404.00486v1\n",
            "pdf link: http://arxiv.org/pdf/2404.00486v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: With the rise of large language models (LLMs), ensuring they embody the\n",
            "principles of being helpful, honest, and harmless (3H), known as Human\n",
            "Alignment, becomes crucial. While existing alignment methods like RLHF, DPO,\n",
            "etc., effectively fine-tune LLMs to match preferences in the preference\n",
            "dataset, they often lead LLMs to highly receptive human input and external\n",
            "evidence, even when this information is poisoned. This leads to a tendency for\n",
            "LLMs to be Adaptive Chameleons when external evidence conflicts with their\n",
            "parametric memory. This exacerbates the risk of LLM being attacked by external\n",
            "poisoned data, which poses a significant security risk to LLM system\n",
            "applications such as Retrieval-augmented generation (RAG). To address the\n",
            "challenge, we propose a novel framework: Dialectical Alignment (DA), which (1)\n",
            "utilizes AI feedback to identify optimal strategies for LLMs to navigate\n",
            "inter-context conflicts and context-memory conflicts with different external\n",
            "evidence in context window (i.e., different ratios of poisoned factual\n",
            "contexts); (2) constructs the SFT dataset as well as the preference dataset\n",
            "based on the AI feedback and strategies above; (3) uses the above datasets for\n",
            "LLM alignment to defense poisoned context attack while preserving the\n",
            "effectiveness of in-context knowledge editing. Our experiments show that the\n",
            "dialectical alignment model improves poisoned data attack defense by 20 and\n",
            "does not require any additional prompt engineering or prior declaration of\n",
            "``you may be attacked`` to the LLMs' context window.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.04044v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-05T11:55:52Z\n",
            "Title:  A Comparison of Methods for Evaluating Generative IR\n",
            "Last Author:  Charles L. A. Clarke\n",
            "Authors:  Negar Arabzadeh, Charles L. A. Clarke\n",
            "abs page link: http://arxiv.org/abs/2404.04044v2\n",
            "pdf link: http://arxiv.org/pdf/2404.04044v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.IR\n",
            "All Categories: cs.IR\n",
            "Abstract: Information retrieval systems increasingly incorporate generative components.\n",
            "For example, in a retrieval augmented generation (RAG) system, a retrieval\n",
            "component might provide a source of ground truth, while a generative component\n",
            "summarizes and augments its responses. In other systems, a large language model\n",
            "(LLM) might directly generate responses without consulting a retrieval\n",
            "component. While there are multiple definitions of generative information\n",
            "retrieval (Gen-IR) systems, in this paper we focus on those systems where the\n",
            "system's response is not drawn from a fixed collection of documents or\n",
            "passages. The response to a query may be entirely new text. Since traditional\n",
            "IR evaluation methods break down under this model, we explore various methods\n",
            "that extend traditional offline evaluation approaches to the Gen-IR context.\n",
            "Offline IR evaluation traditionally employs paid human assessors, but\n",
            "increasingly LLMs are replacing human assessment, demonstrating capabilities\n",
            "similar or superior to crowdsourced labels. Given that Gen-IR systems do not\n",
            "generate responses from a fixed set, we assume that methods for Gen-IR\n",
            "evaluation must largely depend on LLM-generated labels. Along with methods\n",
            "based on binary and graded relevance, we explore methods based on explicit\n",
            "subtopics, pairwise preferences, and embeddings. We first validate these\n",
            "methods against human assessments on several TREC Deep Learning Track tasks; we\n",
            "then apply these methods to evaluate the output of several purely generative\n",
            "systems. For each method we consider both its ability to act autonomously,\n",
            "without the need for human labels or other input, and its ability to support\n",
            "human auditing. To trust these methods, we must be assured that their results\n",
            "align with human assessments. In order to do so, evaluation criteria must be\n",
            "transparent, so that outcomes can be audited by human assessors.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.06347v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-09T14:34:48Z\n",
            "Title:  RAR-b: Reasoning as Retrieval Benchmark\n",
            "Last Author:  Noura Al Moubayed\n",
            "Authors:  Chenghao Xiao, G Thomas Hudson, Noura Al Moubayed\n",
            "abs page link: http://arxiv.org/abs/2404.06347v2\n",
            "pdf link: http://arxiv.org/pdf/2404.06347v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: v2, small typo fixes\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.IR\n",
            "Abstract: Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks\n",
            "have been the two major avenues to record the progress of embedding models in\n",
            "the past few years. Under the emerging Retrieval-augmented Generation (RAG)\n",
            "paradigm, we envision the need to evaluate next-level language understanding\n",
            "abilities of embedding models, and take a conscious look at the reasoning\n",
            "abilities stored in them. Addressing this, we pose the question: Can retrievers\n",
            "solve reasoning problems? By transforming reasoning tasks into retrieval tasks,\n",
            "we find that without specifically trained for reasoning-level language\n",
            "understanding, current state-of-the-art retriever models may still be far from\n",
            "being competent for playing the role of assisting LLMs, especially in\n",
            "reasoning-intensive tasks. Moreover, albeit trained to be aware of\n",
            "instructions, instruction-aware IR models are often better off without\n",
            "instructions in inference time for reasoning tasks, posing an overlooked\n",
            "retriever-LLM behavioral gap for the research community to align. However,\n",
            "recent decoder-based embedding models show great promise in narrowing the gap,\n",
            "highlighting the pathway for embedding models to achieve reasoning-level\n",
            "language understanding. We also show that, although current off-the-shelf\n",
            "re-ranker models fail on these tasks, injecting reasoning abilities into them\n",
            "through fine-tuning still appears easier than doing so to bi-encoders, and we\n",
            "are able to achieve state-of-the-art performance across all tasks by\n",
            "fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark\n",
            "(RAR-b), a holistic suite of tasks and settings to evaluate the reasoning\n",
            "abilities stored in retriever models. RAR-b is available at\n",
            "https://github.com/gowitheflow-1998/RAR-b.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.07135v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-10T16:12:50Z\n",
            "Title:  Towards Robustness of Text-to-Visualization Translation against Lexical\n",
            "  and Phrasal Variability\n",
            "Last Author:  Raymond Chi-Wing Wong\n",
            "Authors:  Jinwei Lu, Yuanfeng Song, Haodi Zhang, Chen Zhang, Raymond Chi-Wing Wong\n",
            "abs page link: http://arxiv.org/abs/2404.07135v2\n",
            "pdf link: http://arxiv.org/pdf/2404.07135v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Text-to-Vis is an emerging task in the natural language processing (NLP) area\n",
            "that aims to automatically generate data visualizations from natural language\n",
            "questions (NLQs). Despite their progress, existing text-to-vis models often\n",
            "heavily rely on lexical matching between words in the questions and tokens in\n",
            "data schemas. This overreliance on lexical matching may lead to a diminished\n",
            "level of model robustness against input variations. In this study, we\n",
            "thoroughly examine the robustness of current text-to-vis models, an area that\n",
            "has not previously been explored. In particular, we construct the first\n",
            "robustness dataset nvBench-Rob, which contains diverse lexical and phrasal\n",
            "variations based on the original text-to-vis benchmark nvBench. Then, we found\n",
            "that the performance of existing text-to-vis models on this new dataset\n",
            "dramatically drops, implying that these methods exhibit inadequate robustness\n",
            "overall. Finally, we propose a novel framework based on Retrieval-Augmented\n",
            "Generation (RAG) technique, named GRED, specifically designed to address input\n",
            "perturbations in these two variants. The framework consists of three parts:\n",
            "NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and\n",
            "Annotation-based Debugger, which are used to tackle the challenges posed by\n",
            "natural language variants, programming style differences and data schema\n",
            "variants, respectively. Extensive experimental evaluations show that, compared\n",
            "to the state-of-the-art model RGVisNet in the Text-to-Vis field, GRED performs\n",
            "better in terms of model robustness, with a 32% increase in accuracy on the\n",
            "proposed nvBench-Rob dataset.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.08878v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-13T02:39:36Z\n",
            "Title:  Generative AI Agent for Next-Generation MIMO Design: Fundamentals,\n",
            "  Challenges, and Vision\n",
            "Last Author:  Khaled B. Letaief\n",
            "Authors:  Zhe Wang, Jiayi Zhang, Hongyang Du, Ruichen Zhang, Dusit Niyato, Bo Ai, Khaled B. Letaief\n",
            "abs page link: http://arxiv.org/abs/2404.08878v1\n",
            "pdf link: http://arxiv.org/pdf/2404.08878v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 9 pages, 3 figures, 2 tables\n",
            "Primary Category: cs.NI\n",
            "All Categories: cs.NI, cs.IT, cs.LG, eess.SP, math.IT\n",
            "Abstract: Next-generation multiple input multiple output (MIMO) is expected to be\n",
            "intelligent and scalable. In this paper, we study generative artificial\n",
            "intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we\n",
            "provide an overview of the development, fundamentals, and challenges of the\n",
            "next-generation MIMO. Then, we propose the concept of the generative AI agent,\n",
            "which is capable of generating tailored and specialized contents with the aid\n",
            "of large language model (LLM) and retrieval augmented generation (RAG). Next,\n",
            "we comprehensively discuss the features and advantages of the generative AI\n",
            "agent framework. More importantly, to tackle existing challenges of\n",
            "next-generation MIMO, we discuss generative AI agent-enabled next-generation\n",
            "MIMO design, from the perspective of performance analysis, signal processing,\n",
            "and resource allocation. Furthermore, we present two compelling case studies\n",
            "that demonstrate the effectiveness of leveraging the generative AI agent for\n",
            "performance analysis in complex configuration scenarios. These examples\n",
            "highlight how the integration of generative AI agents can significantly enhance\n",
            "the analysis and design of next-generation MIMO systems. Finally, we discuss\n",
            "important potential research future directions.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.09134v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-14T03:44:54Z\n",
            "Title:  Interactive Generative AI Agents for Satellite Networks through a\n",
            "  Mixture of Experts Transmission\n",
            "Last Author:  Dong In Kim\n",
            "Authors:  Ruichen Zhang, Hongyang Du, Yinqiu Liu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Dong In Kim\n",
            "abs page link: http://arxiv.org/abs/2404.09134v1\n",
            "pdf link: http://arxiv.org/pdf/2404.09134v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 13 pages, 9 figures\n",
            "Primary Category: cs.NI\n",
            "All Categories: cs.NI, cs.LG\n",
            "Abstract: In response to the needs of 6G global communications, satellite communication\n",
            "networks have emerged as a key solution. However, the large-scale development\n",
            "of satellite communication networks is constrained by the complex system\n",
            "models, whose modeling is challenging for massive users. Moreover, transmission\n",
            "interference between satellites and users seriously affects communication\n",
            "performance. To solve these problems, this paper develops generative artificial\n",
            "intelligence (AI) agents for model formulation and then applies a mixture of\n",
            "experts (MoE) approach to design transmission strategies. Specifically, we\n",
            "leverage large language models (LLMs) to build an interactive modeling paradigm\n",
            "and utilize retrieval-augmented generation (RAG) to extract satellite expert\n",
            "knowledge that supports mathematical modeling. Afterward, by integrating the\n",
            "expertise of multiple specialized components, we propose an MoE-proximal policy\n",
            "optimization (PPO) approach to solve the formulated problem. Each expert can\n",
            "optimize the optimization variables at which it excels through specialized\n",
            "training through its own network and then aggregates them through the gating\n",
            "network to perform joint optimization. The simulation results validate the\n",
            "accuracy and effectiveness of employing a generative agent for problem\n",
            "formulation. Furthermore, the superiority of the proposed MoE-ppo approach over\n",
            "other benchmarks is confirmed in solving the formulated problem. The\n",
            "adaptability of MoE-PPO to various customized modeling problems has also been\n",
            "demonstrated.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.09296v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-14T16:34:31Z\n",
            "Title:  Cross-Data Knowledge Graph Construction for LLM-enabled Educational\n",
            "  Question-Answering System: A~Case~Study~at~HCMUT\n",
            "Last Author:  Tho Quan\n",
            "Authors:  Tuan Bui, Oanh Tran, Phuong Nguyen, Bao Ho, Long Nguyen, Thang Bui, Tho Quan\n",
            "abs page link: http://arxiv.org/abs/2404.09296v1\n",
            "pdf link: http://arxiv.org/pdf/2404.09296v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 8 pages, 7 figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: In today's rapidly evolving landscape of Artificial Intelligence, large\n",
            "language models (LLMs) have emerged as a vibrant research topic. LLMs find\n",
            "applications in various fields and contribute significantly. Despite their\n",
            "powerful language capabilities, similar to pre-trained language models (PLMs),\n",
            "LLMs still face challenges in remembering events, incorporating new\n",
            "information, and addressing domain-specific issues or hallucinations. To\n",
            "overcome these limitations, researchers have proposed Retrieval-Augmented\n",
            "Generation (RAG) techniques, some others have proposed the integration of LLMs\n",
            "with Knowledge Graphs (KGs) to provide factual context, thereby improving\n",
            "performance and delivering more accurate feedback to user queries.\n",
            "  Education plays a crucial role in human development and progress. With the\n",
            "technology transformation, traditional education is being replaced by digital\n",
            "or blended education. Therefore, educational data in the digital environment is\n",
            "increasing day by day. Data in higher education institutions are diverse,\n",
            "comprising various sources such as unstructured/structured text, relational\n",
            "databases, web/app-based API access, etc. Constructing a Knowledge Graph from\n",
            "these cross-data sources is not a simple task. This article proposes a method\n",
            "for automatically constructing a Knowledge Graph from multiple data sources and\n",
            "discusses some initial applications (experimental trials) of KG in conjunction\n",
            "with LLMs for question-answering tasks.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.10779v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-23T13:25:01Z\n",
            "Title:  Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations\n",
            "Last Author:  Yogesh Gupta\n",
            "Authors:  Mathav Raj J, Kushala VM, Harikrishna Warrier, Yogesh Gupta\n",
            "abs page link: http://arxiv.org/abs/2404.10779v1\n",
            "pdf link: http://arxiv.org/pdf/2404.10779v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 17 pages, 12 tables, 3 figures\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE, cs.LG\n",
            "Abstract: There is a compelling necessity from enterprises for fine tuning LLMs (Large\n",
            "Language Models) o get them trained on proprietary domain knowledge. The\n",
            "challenge is to imbibe the LLMs with domain specific knowledge using the most\n",
            "optimial resource and cost and in the best possible time. Many enterprises rely\n",
            "on RAG (Retrieval Augmented Generation) which does not need LLMs to be\n",
            "ine-tuned but they are limited by the quality of vector databases and their\n",
            "retrieval capabilities rather than the intrinsic capabilities of the LLMs\n",
            "themselves. In our current work we focus on fine tuning LLaMA, an open source\n",
            "LLM using proprietary documents and code from an enterprise repository and use\n",
            "the fine tuned models to evaluate the quality of responses. As part of this\n",
            "work, we aim to guide beginners on how to start with fine tuning an LLM for\n",
            "documentation and code by making educated guesses on size of GPU required and\n",
            "options that are available for formatting the data. We also propose pre\n",
            "processing recipes for both documentation and code to prepare dataset in\n",
            "different formats. The proposed methods of data preparation for document\n",
            "datasets are forming paragraph chunks, forming question and answer pairs and\n",
            "forming keyword and paragraph chunk pairs. For code dataset we propose forming\n",
            "summary and function pairs. Further, we qualitatively evaluate the results of\n",
            "the models for domain specific queries. Finally, we also propose practical\n",
            "guidelines and recommendations for fine tuning LLMs.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.12096v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-18T11:29:23Z\n",
            "Title:  LongEmbed: Extending Embedding Models for Long Context Retrieval\n",
            "Last Author:  Sujian Li\n",
            "Authors:  Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li\n",
            "abs page link: http://arxiv.org/abs/2404.12096v2\n",
            "pdf link: http://arxiv.org/pdf/2404.12096v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: Fix results for Nomic\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.LG\n",
            "Abstract: Embedding models play a pivot role in modern NLP applications such as IR and\n",
            "RAG. While the context limit of LLMs has been pushed beyond 1 million tokens,\n",
            "embedding models are still confined to a narrow context window not exceeding 8k\n",
            "tokens, refrained from application scenarios requiring long inputs such as\n",
            "legal contracts. This paper explores context window extension of existing\n",
            "embedding models, pushing the limit to 32k without requiring additional\n",
            "training. First, we examine the performance of current embedding models for\n",
            "long context retrieval on our newly constructed LongEmbed benchmark. LongEmbed\n",
            "comprises two synthetic tasks and four carefully chosen real-world tasks,\n",
            "featuring documents of varying length and dispersed target information.\n",
            "Benchmarking results underscore huge room for improvement in these models.\n",
            "Based on this, comprehensive experiments show that training-free context window\n",
            "extension strategies like position interpolation can effectively extend the\n",
            "context window of existing embedding models by several folds, regardless of\n",
            "their original context being 512 or beyond 4k. Furthermore, for models\n",
            "employing absolute position encoding (APE), we show the possibility of further\n",
            "fine-tuning to harvest notable performance gains while strictly preserving\n",
            "original behavior for short inputs. For models using rotary position embedding\n",
            "(RoPE), significant enhancements are observed when employing RoPE-specific\n",
            "methods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for\n",
            "context window extension. To facilitate future research, we release E5-Base-4k\n",
            "and E5-RoPE-Base, along with the LongEmbed benchmark.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.18077v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-28T05:46:28Z\n",
            "Title:  Generative AI for Low-Carbon Artificial Intelligence of Things\n",
            "Last Author:  Zhu Han\n",
            "Authors:  Jinbo Wen, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Hongyang Du, Yang Zhang, Zhu Han\n",
            "abs page link: http://arxiv.org/abs/2404.18077v1\n",
            "pdf link: http://arxiv.org/pdf/2404.18077v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.NI\n",
            "All Categories: cs.NI, cs.LG\n",
            "Abstract: By integrating Artificial Intelligence (AI) with the Internet of Things\n",
            "(IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields.\n",
            "However, AIoT is facing the challenges of energy consumption and carbon\n",
            "emissions due to the continuous advancement of mobile technology. Fortunately,\n",
            "Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT\n",
            "due to its excellent reasoning and generation capabilities. In this article, we\n",
            "explore the potential of GAI for carbon emissions reduction and propose a novel\n",
            "GAI-enabled solution for low-carbon AIoT. Specifically, we first study the main\n",
            "impacts that cause carbon emissions in AIoT, and then introduce GAI techniques\n",
            "and their relations to carbon emissions. We then explore the application\n",
            "prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon\n",
            "emissions of network components. Subsequently, we propose a Large Language\n",
            "Model (LLM)-enabled carbon emission optimization framework, in which we design\n",
            "pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more\n",
            "accurate and reliable optimization problems. Furthermore, we utilize Generative\n",
            "Diffusion Models (GDMs) to identify optimal strategies for carbon emission\n",
            "reduction. Simulation results demonstrate the effectiveness of the proposed\n",
            "framework. Finally, we insightfully provide open research directions for\n",
            "low-carbon AIoT.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.18470v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-29T07:11:39Z\n",
            "Title:  ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n",
            "  using Large Language Model for Stock Performance Prediction\n",
            "Last Author:  Papa Momar Ndiaye\n",
            "Authors:  Yupeng Cao, Zhi Chen, Qingyun Pei, Prashant Kumar, K. P. Subbalakshmi, Papa Momar Ndiaye\n",
            "abs page link: http://arxiv.org/abs/2404.18470v1\n",
            "pdf link: http://arxiv.org/pdf/2404.18470v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 15 pages, 3 figures, 5 tables\n",
            "Primary Category: cs.CE\n",
            "All Categories: cs.CE, cs.AI, cs.CL, q-fin.RM, q-fin.TR\n",
            "Abstract: In the realm of financial analytics, leveraging unstructured data, such as\n",
            "earnings conference calls (ECCs), to forecast stock performance is a critical\n",
            "challenge that has attracted both academics and investors. While previous\n",
            "studies have used deep learning-based models to obtain a general view of ECCs,\n",
            "they often fail to capture detailed, complex information. Our study introduces\n",
            "a novel framework: \\textbf{ECC Analyzer}, combining Large Language Models\n",
            "(LLMs) and multi-modal techniques to extract richer, more predictive insights.\n",
            "The model begins by summarizing the transcript's structure and analyzing the\n",
            "speakers' mode and confidence level by detecting variations in tone and pitch\n",
            "for audio. This analysis helps investors form an overview perception of the\n",
            "ECCs. Moreover, this model uses the Retrieval-Augmented Generation (RAG) based\n",
            "methods to meticulously extract the focuses that have a significant impact on\n",
            "stock performance from an expert's perspective, providing a more targeted\n",
            "analysis. The model goes a step further by enriching these extracted focuses\n",
            "with additional layers of analysis, such as sentiment and audio segment\n",
            "features. By integrating these insights, the ECC Analyzer performs multi-task\n",
            "predictions of stock performance, including volatility, value-at-risk (VaR),\n",
            "and return for different intervals. The results show that our model outperforms\n",
            "traditional analytic benchmarks, confirming the effectiveness of using advanced\n",
            "LLM techniques in financial analytics.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.03845v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-06T20:50:17Z\n",
            "Title:  Self-Improving Customer Review Response Generation Based on LLMs\n",
            "Last Author:  Gila Kamhi\n",
            "Authors:  Guy Azov, Tatiana Pelc, Adi Fledel Alon, Gila Kamhi\n",
            "abs page link: http://arxiv.org/abs/2405.03845v1\n",
            "pdf link: http://arxiv.org/pdf/2405.03845v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 18 pages, 4 figure, 8 figures in Appendix, accepted to LREC-COLING\n",
            "  2024 workshop\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Previous studies have demonstrated that proactive interaction with user\n",
            "reviews has a positive impact on the perception of app users and encourages\n",
            "them to submit revised ratings. Nevertheless, developers encounter challenges\n",
            "in managing a high volume of reviews, particularly in the case of popular apps\n",
            "with a substantial influx of daily reviews. Consequently, there is a demand for\n",
            "automated solutions aimed at streamlining the process of responding to user\n",
            "reviews. To address this, we have developed a new system for generating\n",
            "automatic responses by leveraging user-contributed documents with the help of\n",
            "retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).\n",
            "Our solution, named SCRABLE, represents an adaptive customer review response\n",
            "automation that enhances itself with self-optimizing prompts and a judging\n",
            "mechanism based on LLMs. Additionally, we introduce an automatic scoring\n",
            "mechanism that mimics the role of a human evaluator to assess the quality of\n",
            "responses generated in customer review domains. Extensive experiments and\n",
            "analyses conducted on real-world datasets reveal that our method is effective\n",
            "in producing high-quality responses, yielding improvement of more than 8.5%\n",
            "compared to the baseline. Further validation through manual examination of the\n",
            "generated responses underscores the efficacy our proposed system.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.07530v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-13T07:56:15Z\n",
            "Title:  Prompt-based Code Completion via Multi-Retrieval Augmented Generation\n",
            "Last Author:  Yuqun Zhang\n",
            "Authors:  Hanzhuo Tan, Qi Luo, Ling Jiang, Zizheng Zhan, Jing Li, Haotian Zhang, Yuqun Zhang\n",
            "abs page link: http://arxiv.org/abs/2405.07530v1\n",
            "pdf link: http://arxiv.org/pdf/2405.07530v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE\n",
            "Abstract: Automated code completion, aiming at generating subsequent tokens from\n",
            "unfinished code, has been significantly benefited from recent progress in\n",
            "pre-trained Large Language Models (LLMs). However, these models often suffer\n",
            "from coherence issues and hallucinations when dealing with complex code logic\n",
            "or extrapolating beyond their training data. Existing Retrieval Augmented\n",
            "Generation (RAG) techniques partially address these issues by retrieving\n",
            "relevant code with a separate encoding model where the retrieved snippet serves\n",
            "as contextual reference for code completion. However, their retrieval scope is\n",
            "subject to a singular perspective defined by the encoding model, which largely\n",
            "overlooks the complexity and diversity inherent in code semantics. To address\n",
            "this limitation, we propose ProCC, a code completion framework leveraging\n",
            "prompt engineering and the contextual multi-armed bandits algorithm to flexibly\n",
            "incorporate and adapt to multiple perspectives of code. ProCC first employs a\n",
            "prompt-based multi-retriever system which crafts prompt templates to elicit LLM\n",
            "knowledge to understand code semantics with multiple retrieval perspectives.\n",
            "Then, it adopts the adaptive retrieval selection algorithm to incorporate code\n",
            "similarity into the decision-making process to determine the most suitable\n",
            "retrieval perspective for the LLM to complete the code. Experimental results\n",
            "demonstrate that ProCC outperforms state-of-the-art code completion technique\n",
            "by 8.6% on our collected open-source benchmark suite and 10.1% on the\n",
            "private-domain benchmark suite collected from a billion-user e-commerce company\n",
            "in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in\n",
            "a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned\n",
            "model.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.12750v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-21T13:02:27Z\n",
            "Title:  Generative AI and Large Language Models for Cyber Security: All Insights\n",
            "  You Need\n",
            "Last Author:  Norbert Tihanyi\n",
            "Authors:  Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour Mechri, Norbert Tihanyi\n",
            "abs page link: http://arxiv.org/abs/2405.12750v1\n",
            "pdf link: http://arxiv.org/pdf/2405.12750v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 50 pages, 8 figures\n",
            "Primary Category: cs.CR\n",
            "All Categories: cs.CR, cs.AI\n",
            "Abstract: This paper provides a comprehensive review of the future of cybersecurity\n",
            "through Generative AI and Large Language Models (LLMs). We explore LLM\n",
            "applications across various domains, including hardware design security,\n",
            "intrusion detection, software engineering, design verification, cyber threat\n",
            "intelligence, malware detection, and phishing detection. We present an overview\n",
            "of LLM evolution and its current state, focusing on advancements in models such\n",
            "as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\n",
            "to LLM vulnerabilities, such as prompt injection, insecure output handling,\n",
            "data poisoning, DDoS attacks, and adversarial instructions. We delve into\n",
            "mitigation strategies to protect these models, providing a comprehensive look\n",
            "at potential attack scenarios and prevention techniques. Furthermore, we\n",
            "evaluate the performance of 42 LLM models in cybersecurity knowledge and\n",
            "hardware security, highlighting their strengths and weaknesses. We thoroughly\n",
            "evaluate cybersecurity datasets for LLM training and testing, covering the\n",
            "lifecycle from data creation to usage and identifying gaps for future research.\n",
            "In addition, we review new strategies for leveraging LLMs, including techniques\n",
            "like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\n",
            "Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\n",
            "Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\n",
            "to enhance real-time cybersecurity defenses and improve the sophistication of\n",
            "LLM applications in threat detection and response. Our paper provides a\n",
            "foundational understanding and strategic direction for integrating LLMs into\n",
            "future cybersecurity frameworks, emphasizing innovation and robust model\n",
            "deployment to safeguard against evolving cyber threats.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.13057v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-20T11:05:56Z\n",
            "Title:  Can Github issues be solved with Tree Of Thoughts?\n",
            "Last Author:  Bangdi Liu\n",
            "Authors:  Ricardo La Rosa, Corey Hulse, Bangdi Liu\n",
            "abs page link: http://arxiv.org/abs/2405.13057v1\n",
            "pdf link: http://arxiv.org/pdf/2405.13057v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 8 pages, 2 figures, 7 tables\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE, cs.AI\n",
            "Abstract: While there have been extensive studies in code generation by large language\n",
            "models (LLM), where benchmarks like HumanEval have been surpassed with an\n",
            "impressive 96.3% success rate, these benchmarks predominantly judge a model's\n",
            "performance on basic function-level code generation and lack the critical\n",
            "thinking and concept of scope required of real-world scenarios such as solving\n",
            "GitHub issues. This research introduces the application of the Tree of Thoughts\n",
            "(ToT) language model reasoning framework for enhancing the decision-making and\n",
            "problem-solving abilities of LLMs for this complex task. Compared to\n",
            "traditional input-output (IO) prompting and Retrieval Augmented Generation\n",
            "(RAG) techniques, ToT is designed to improve performance by facilitating a\n",
            "structured exploration of multiple reasoning trajectories and enabling\n",
            "self-assessment of potential solutions. We experimentally deploy ToT in\n",
            "tackling a Github issue contained within an instance of the SWE-bench. However,\n",
            "our results reveal that the ToT framework alone is not enough to give LLMs the\n",
            "critical reasoning capabilities to outperform existing methods. In this paper\n",
            "we analyze the potential causes of these shortcomings and identify key areas\n",
            "for improvement such as deepening the thought process and introducing agentic\n",
            "capabilities. The insights of this research are aimed at informing future\n",
            "directions for refining the application of ToT and better harnessing the\n",
            "potential of LLMs in real-world problem-solving scenarios.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.13401v3\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-22T07:21:32Z\n",
            "Title:  TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in\n",
            "  Large Language Models\n",
            "Last Author:  Gongshen Liu\n",
            "Authors:  Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, Gongshen Liu\n",
            "abs page link: http://arxiv.org/abs/2405.13401v3\n",
            "pdf link: http://arxiv.org/pdf/2405.13401v3\n",
            "Journal reference: No journal ref found\n",
            "Comments: 19 pages, 14 figures, 4 tables\n",
            "Primary Category: cs.CR\n",
            "All Categories: cs.CR, cs.CL\n",
            "Abstract: Large language models (LLMs) have raised concerns about potential security\n",
            "threats despite performing significantly in Natural Language Processing (NLP).\n",
            "Backdoor attacks initially verified that LLM is doing substantial harm at all\n",
            "stages, but the cost and robustness have been criticized. Attacking LLMs is\n",
            "inherently risky in security review, while prohibitively expensive. Besides,\n",
            "the continuous iteration of LLMs will degrade the robustness of backdoors. In\n",
            "this paper, we propose TrojanRAG, which employs a joint backdoor attack in the\n",
            "Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack\n",
            "scenarios. Specifically, the adversary constructs elaborate target contexts and\n",
            "trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized\n",
            "by contrastive learning, thus constraining the triggering conditions to a\n",
            "parameter subspace to improve the matching. To improve the recall of the RAG\n",
            "for the target contexts, we introduce a knowledge graph to construct structured\n",
            "data to achieve hard matching at a fine-grained level. Moreover, we normalize\n",
            "the backdoor scenarios in LLMs to analyze the real harm caused by backdoors\n",
            "from both attackers' and users' perspectives and further verify whether the\n",
            "context is a favorable tool for jailbreaking models. Extensive experimental\n",
            "results on truthfulness, language understanding, and harmfulness show that\n",
            "TrojanRAG exhibits versatility threats while maintaining retrieval capabilities\n",
            "on normal queries.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.14702v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-23T15:37:06Z\n",
            "Title:  G3: An Effective and Adaptive Framework for Worldwide Geolocalization\n",
            "  Using Large Multi-Modality Models\n",
            "Last Author:  Dawei Yin\n",
            "Authors:  Pengyue Jia, Yiding Liu, Xiaopeng Li, Xiangyu Zhao, Yuhao Wang, Yantong Du, Xiao Han, Xuetao Wei, Shuaiqiang Wang, Dawei Yin\n",
            "abs page link: http://arxiv.org/abs/2405.14702v1\n",
            "pdf link: http://arxiv.org/pdf/2405.14702v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CV\n",
            "All Categories: cs.CV, cs.AI\n",
            "Abstract: Worldwide geolocalization aims to locate the precise location at the\n",
            "coordinate level of photos taken anywhere on the Earth. It is very challenging\n",
            "due to 1) the difficulty of capturing subtle location-aware visual semantics,\n",
            "and 2) the heterogeneous geographical distribution of image data. As a result,\n",
            "existing studies have clear limitations when scaled to a worldwide context.\n",
            "They may easily confuse distant images with similar visual contents, or cannot\n",
            "adapt to various locations worldwide with different amounts of relevant data.\n",
            "To resolve these limitations, we propose G3, a novel framework based on\n",
            "Retrieval-Augmented Generation (RAG). In particular, G3 consists of three\n",
            "steps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to\n",
            "optimize both retrieval and generation phases of worldwide geolocalization.\n",
            "During Geo-alignment, our solution jointly learns expressive multi-modal\n",
            "representations for images, GPS and textual descriptions, which allows us to\n",
            "capture location-aware semantics for retrieving nearby images for a given\n",
            "query. During Geo-diversification, we leverage a prompt ensembling method that\n",
            "is robust to inconsistent retrieval performance for different image queries.\n",
            "Finally, we combine both retrieved and generated GPS candidates in\n",
            "Geo-verification for location prediction. Experiments on two well-established\n",
            "datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other\n",
            "state-of-the-art methods.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.17130v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-27T12:48:30Z\n",
            "Title:  Exploiting the Layered Intrinsic Dimensionality of Deep Models for\n",
            "  Practical Adversarial Training\n",
            "Last Author:  Sanjay Chawla\n",
            "Authors:  Enes Altinisik, Safa Messaoud, Husrev Taha Sencar, Hassan Sajjad, Sanjay Chawla\n",
            "abs page link: http://arxiv.org/abs/2405.17130v1\n",
            "pdf link: http://arxiv.org/pdf/2405.17130v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.LG\n",
            "All Categories: cs.LG, cs.CL\n",
            "Abstract: Despite being a heavily researched topic, Adversarial Training (AT) is\n",
            "rarely, if ever, deployed in practical AI systems for two primary reasons: (i)\n",
            "the gained robustness is frequently accompanied by a drop in generalization and\n",
            "(ii) generating adversarial examples (AEs) is computationally prohibitively\n",
            "expensive. To address these limitations, we propose SMAAT, a new AT algorithm\n",
            "that leverages the manifold conjecture, stating that off-manifold AEs lead to\n",
            "better robustness while on-manifold AEs result in better generalization.\n",
            "Specifically, SMAAT aims at generating a higher proportion of off-manifold AEs\n",
            "by perturbing the intermediate deepnet layer with the lowest intrinsic\n",
            "dimension. This systematically results in better scalability compared to\n",
            "classical AT as it reduces the PGD chains length required for generating the\n",
            "AEs. Additionally, our study provides, to the best of our knowledge, the first\n",
            "explanation for the difference in the generalization and robustness trends\n",
            "between vision and language models, ie., AT results in a drop in generalization\n",
            "in vision models whereas, in encoder-based language models, generalization\n",
            "either improves or remains unchanged. We show that vision transformers and\n",
            "decoder-based models tend to have low intrinsic dimensionality in the earlier\n",
            "layers of the network (more off-manifold AEs), while encoder-based models have\n",
            "low intrinsic dimensionality in the later layers. We demonstrate the efficacy\n",
            "of SMAAT; on several tasks, including robustifying (i) sentiment classifiers,\n",
            "(ii) safety filters in decoder-based models, and (iii) retrievers in RAG\n",
            "setups. SMAAT requires only 25-33% of the GPU time compared to standard AT,\n",
            "while significantly improving robustness across all applications and\n",
            "maintaining comparable generalization.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.18359v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-28T16:56:42Z\n",
            "Title:  Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual\n",
            "  Performance in LLMs\n",
            "Last Author:  Akshay Nambi\n",
            "Authors:  Somnath Kumar, Vaibhav Balloli, Mercy Ranjit, Kabir Ahuja, Tanuja Ganu, Sunayana Sitaram, Kalika Bali, Akshay Nambi\n",
            "abs page link: http://arxiv.org/abs/2405.18359v1\n",
            "pdf link: http://arxiv.org/pdf/2405.18359v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: Large language models (LLMs) are at the forefront of transforming numerous\n",
            "domains globally. However, their inclusivity and effectiveness remain limited\n",
            "for non-Latin scripts and low-resource languages. This paper tackles the\n",
            "imperative challenge of enhancing the multilingual performance of LLMs without\n",
            "extensive training or fine-tuning. Through systematic investigation and\n",
            "evaluation of diverse languages using popular question-answering (QA) datasets,\n",
            "we present novel techniques that unlock the true potential of LLMs in a\n",
            "polyglot landscape. Our approach encompasses three key strategies that yield\n",
            "significant improvements in multilingual proficiency. First, by meticulously\n",
            "optimizing prompts tailored for polyglot LLMs, we unlock their latent\n",
            "capabilities, resulting in substantial performance boosts across languages.\n",
            "Second, we introduce a new hybrid approach that synergizes LLM Retrieval\n",
            "Augmented Generation (RAG) with multilingual embeddings and achieves improved\n",
            "multilingual task performance. Finally, we introduce a novel learning approach\n",
            "that dynamically selects the optimal prompt strategy, LLM model, and embedding\n",
            "model per query at run-time. This dynamic adaptation maximizes the efficacy of\n",
            "LLMs across languages, outperforming best static and random strategies.\n",
            "Additionally, our approach adapts configurations in both offline and online\n",
            "settings, and can seamlessly adapt to new languages and datasets, leading to\n",
            "substantial advancements in multilingual understanding and generation across\n",
            "diverse languages.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.19893v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-30T09:50:38Z\n",
            "Title:  Similarity is Not All You Need: Endowing Retrieval Augmented Generation\n",
            "  with Multi Layered Thoughts\n",
            "Last Author:  Jun Zhou\n",
            "Authors:  Chunjing Gan, Dan Yang, Binbin Hu, Hanxiao Zhang, Siyuan Li, Ziqi Liu, Yue Shen, Lin Ju, Zhiqiang Zhang, Jinjie Gu, Lei Liang, Jun Zhou\n",
            "abs page link: http://arxiv.org/abs/2405.19893v1\n",
            "pdf link: http://arxiv.org/pdf/2405.19893v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 12 pages\n",
            "Primary Category: cs.LG\n",
            "All Categories: cs.LG, cs.AI, cs.CL\n",
            "Abstract: In recent years, large language models (LLMs) have made remarkable\n",
            "achievements in various domains. However, the untimeliness and cost of\n",
            "knowledge updates coupled with hallucination issues of LLMs have curtailed\n",
            "their applications in knowledge intensive tasks, where retrieval augmented\n",
            "generation (RAG) can be of help. Nevertheless, existing retrieval augmented\n",
            "models typically use similarity as a bridge between queries and documents and\n",
            "follow a retrieve then read procedure. In this work, we argue that similarity\n",
            "is not always the panacea and totally relying on similarity would sometimes\n",
            "degrade the performance of retrieval augmented generation. To this end, we\n",
            "propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented\n",
            "Generation framework. To begin with, beyond existing similarity oriented\n",
            "thought, we embrace a small scale utility model that draws supervision from an\n",
            "LLM for utility oriented thought and further come up with a smarter model by\n",
            "comprehensively combining the similarity and utility oriented thoughts.\n",
            "Furthermore, given the fact that the retrieved document set tends to be huge\n",
            "and using them in isolation makes it difficult to capture the commonalities and\n",
            "characteristics among them, we propose to make an LLM as a task adaptive\n",
            "summarizer to endow retrieval augmented generation with compactness-oriented\n",
            "thought. Finally, with multi layered thoughts from the precedent stages, an LLM\n",
            "is called for knowledge augmented generation. Extensive experiments on\n",
            "knowledge-intensive tasks have demonstrated the superiority of MetRag.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.20389v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-30T18:00:21Z\n",
            "Title:  Designing an Evaluation Framework for Large Language Models in Astronomy\n",
            "  Research\n",
            "Last Author:  Mikaeel Yunus\n",
            "Authors:  John F. Wu, Alina Hyk, Kiera McCormick, Christine Ye, Simone Astarita, Elina Baral, Jo Ciuca, Jesse Cranney, Anjalie Field, Kartheik Iyer, Philipp Koehn, Jenn Kotler, Sandor Kruk, Michelle Ntampaka, Charles O'Neill, Joshua E. G. Peek, Sanjib Sharma, Mikaeel Yunus\n",
            "abs page link: http://arxiv.org/abs/2405.20389v1\n",
            "pdf link: http://arxiv.org/pdf/2405.20389v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 7 pages, 3 figures. Code available at\n",
            "  https://github.com/jsalt2024-evaluating-llms-for-astronomy/astro-arxiv-bot\n",
            "Primary Category: astro-ph.IM\n",
            "All Categories: astro-ph.IM, cs.AI, cs.HC, cs.IR\n",
            "Abstract: Large Language Models (LLMs) are shifting how scientific research is done. It\n",
            "is imperative to understand how researchers interact with these models and how\n",
            "scientific sub-communities like astronomy might benefit from them. However,\n",
            "there is currently no standard for evaluating the use of LLMs in astronomy.\n",
            "Therefore, we present the experimental design for an evaluation study on how\n",
            "astronomy researchers interact with LLMs. We deploy a Slack chatbot that can\n",
            "answer queries from users via Retrieval-Augmented Generation (RAG); these\n",
            "responses are grounded in astronomy papers from arXiv. We record and anonymize\n",
            "user questions and chatbot answers, user upvotes and downvotes to LLM\n",
            "responses, user feedback to the LLM, and retrieved documents and similarity\n",
            "scores with the query. Our data collection method will enable future dynamic\n",
            "evaluations of LLM tools for astronomy.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.20774v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-27T17:59:43Z\n",
            "Title:  Exploring Backdoor Attacks against Large Language Model-based Decision\n",
            "  Making\n",
            "Last Author:  Qi Zhu\n",
            "Authors:  Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu\n",
            "abs page link: http://arxiv.org/abs/2405.20774v1\n",
            "pdf link: http://arxiv.org/pdf/2405.20774v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 27 pages, including main paper, references, and appendix\n",
            "Primary Category: cs.CR\n",
            "All Categories: cs.CR, cs.AI\n",
            "Abstract: Large Language Models (LLMs) have shown significant promise in\n",
            "decision-making tasks when fine-tuned on specific applications, leveraging\n",
            "their inherent common sense and reasoning abilities learned from vast amounts\n",
            "of data. However, these systems are exposed to substantial safety and security\n",
            "risks during the fine-tuning phase. In this work, we propose the first\n",
            "comprehensive framework for Backdoor Attacks against LLM-enabled\n",
            "Decision-making systems (BALD), systematically exploring how such attacks can\n",
            "be introduced during the fine-tuning phase across various channels.\n",
            "Specifically, we propose three attack mechanisms and corresponding backdoor\n",
            "optimization methods to attack different components in the LLM-based\n",
            "decision-making pipeline: word injection, scenario manipulation, and knowledge\n",
            "injection. Word injection embeds trigger words directly into the query prompt.\n",
            "Scenario manipulation occurs in the physical environment, where a high-level\n",
            "backdoor semantic scenario triggers the attack. Knowledge injection conducts\n",
            "backdoor attacks on retrieval augmented generation (RAG)-based LLM systems,\n",
            "strategically injecting word triggers into poisoned knowledge while ensuring\n",
            "the information remains factually accurate for stealthiness. We conduct\n",
            "extensive experiments with three popular LLMs (GPT-3.5, LLaMA2, PaLM2), using\n",
            "two datasets (HighwayEnv, nuScenes), and demonstrate the effectiveness and\n",
            "stealthiness of our backdoor triggers and mechanisms. Finally, we critically\n",
            "assess the strengths and weaknesses of our proposed approaches, highlight the\n",
            "inherent vulnerabilities of LLMs in decision-making tasks, and evaluate\n",
            "potential defenses to safeguard LLM-based decision making systems.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.01428v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-03T15:26:06Z\n",
            "Title:  Superhuman performance in urology board questions by an explainable\n",
            "  large language model enabled for context integration of the European\n",
            "  Association of Urology guidelines: the UroBot study\n",
            "Last Author:  Titus J. Brinker\n",
            "Authors:  Martin J. Hetz, Nicolas Carl, Sarah Haggenmüller, Christoph Wies, Maurice Stephan Michel, Frederik Wessels, Titus J. Brinker\n",
            "abs page link: http://arxiv.org/abs/2406.01428v2\n",
            "pdf link: http://arxiv.org/pdf/2406.01428v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.CV\n",
            "Abstract: Large Language Models (LLMs) are revolutionizing medical Question-Answering\n",
            "(medQA) through extensive use of medical literature. However, their performance\n",
            "is often hampered by outdated training data and a lack of explainability, which\n",
            "limits clinical applicability. This study aimed to create and assess UroBot, a\n",
            "urology-specialized chatbot, by comparing it with state-of-the-art models and\n",
            "the performance of urologists on urological board questions, ensuring full\n",
            "clinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4,\n",
            "and GPT-4o models, employing retrieval-augmented generation (RAG) and the\n",
            "latest 2023 guidelines from the European Association of Urology (EAU). The\n",
            "evaluation included ten runs of 200 European Board of Urology (EBU) In-Service\n",
            "Assessment (ISA) questions, with performance assessed by the mean Rate of\n",
            "Correct Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing\n",
            "GPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and\n",
            "exhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979).\n",
            "By comparison, the average performance of urologists on board questions, as\n",
            "reported in the literature, is 68.7%. UroBot's clinician-verifiable nature and\n",
            "superior accuracy compared to both existing models and urologists on board\n",
            "questions highlight its potential for clinical integration. The study also\n",
            "provides the necessary code and instructions for further development of UroBot.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.02110v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-04T08:36:39Z\n",
            "Title:  UniOQA: A Unified Framework for Knowledge Graph Question Answering with\n",
            "  Large Language Models\n",
            "Last Author:  Junzhao Du\n",
            "Authors:  Zhuoyang Li, Liran Deng, Hui Liu, Qiaoqiao Liu, Junzhao Du\n",
            "abs page link: http://arxiv.org/abs/2406.02110v1\n",
            "pdf link: http://arxiv.org/pdf/2406.02110v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 10 pages, 5 figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: OwnThink stands as the most extensive Chinese open-domain knowledge graph\n",
            "introduced in recent times. Despite prior attempts in question answering over\n",
            "OwnThink (OQA), existing studies have faced limitations in model representation\n",
            "capabilities, posing challenges in further enhancing overall accuracy in\n",
            "question answering. In this paper, we introduce UniOQA, a unified framework\n",
            "that integrates two complementary parallel workflows. Unlike conventional\n",
            "approaches, UniOQA harnesses large language models (LLMs) for precise question\n",
            "answering and incorporates a direct-answer-prediction process as a\n",
            "cost-effective complement. Initially, to bolster representation capacity, we\n",
            "fine-tune an LLM to translate questions into the Cypher query language (CQL),\n",
            "tackling issues associated with restricted semantic understanding and\n",
            "hallucinations. Subsequently, we introduce the Entity and Relation Replacement\n",
            "algorithm to ensure the executability of the generated CQL. Concurrently, to\n",
            "augment overall accuracy in question answering, we further adapt the\n",
            "Retrieval-Augmented Generation (RAG) process to the knowledge graph.\n",
            "Ultimately, we optimize answer accuracy through a dynamic decision algorithm.\n",
            "Experimental findings illustrate that UniOQA notably advances SpCQL Logical\n",
            "Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new\n",
            "state-of-the-art results on this benchmark. Through ablation experiments, we\n",
            "delve into the superior representation capacity of UniOQA and quantify its\n",
            "performance breakthrough.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.02746v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-04T20:02:52Z\n",
            "Title:  RATT: A Thought Structure for Coherent and Correct LLM Reasoning\n",
            "Last Author:  Kunpeng Liu\n",
            "Authors:  Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang, Dongjie Wang, Kunpeng Liu\n",
            "abs page link: http://arxiv.org/abs/2406.02746v2\n",
            "pdf link: http://arxiv.org/pdf/2406.02746v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Large Language Models (LLMs) gain substantial reasoning and decision-making\n",
            "capabilities from thought structures. However, existing methods such as Tree of\n",
            "Thought and Retrieval Augmented Thoughts often fall short in complex tasks due\n",
            "to the limitations of insufficient local retrieval of factual knowledge and\n",
            "inadequate global selection of strategies. These limitations make it\n",
            "challenging for these methods to balance factual accuracy and comprehensive\n",
            "logical optimization effectively. To address these limitations, we introduce\n",
            "the Retrieval Augmented Thought Tree (RATT), a novel thought structure that\n",
            "considers both overall logical soundness and factual correctness at each step\n",
            "of the thinking process. Specifically, at every point of a thought branch, RATT\n",
            "performs planning and lookahead to explore and evaluate multiple potential\n",
            "reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented\n",
            "Generation (RAG) with LLM's ability to assess overall strategy. Through this\n",
            "combination of factual knowledge and strategic feasibility, the RATT adjusts\n",
            "and integrates the thought tree structure to search for the most promising\n",
            "branches within the search space. This thought structure significantly enhances\n",
            "the model's coherence in logical inference and efficiency in decision-making,\n",
            "and thus increases the limit of the capacity of LLM to generate reliable\n",
            "inferences and decisions based on thought structures. A broad range of\n",
            "experiments on different types of tasks showcases that the RATT structure\n",
            "significantly outperforms existing methods in factual correctness and logical\n",
            "coherence.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.03777v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-06T06:41:53Z\n",
            "Title:  Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge\n",
            "  Devices\n",
            "Last Author:  Yiyu Shi\n",
            "Authors:  Ruiyang Qin, Dancheng Liu, Zheyu Yan, Zhaoxuan Tan, Zixuan Pan, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Jinjun Xiong, Yiyu Shi\n",
            "abs page link: http://arxiv.org/abs/2406.03777v2\n",
            "pdf link: http://arxiv.org/pdf/2406.03777v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: Benckmarking paper\n",
            "Primary Category: cs.LG\n",
            "All Categories: cs.LG, cs.AI\n",
            "Abstract: The scaling laws have become the de facto guidelines for designing large\n",
            "language models (LLMs), but they were studied under the assumption of unlimited\n",
            "computing resources for both training and inference. As LLMs are increasingly\n",
            "used as personalized intelligent assistants, their customization (i.e.,\n",
            "learning through fine-tuning) and deployment onto resource-constrained edge\n",
            "devices will become more and more prevalent. An urging but open question is how\n",
            "a resource-constrained computing environment would affect the design choices\n",
            "for a personalized LLM. We study this problem empirically in this work. In\n",
            "particular, we consider the tradeoffs among a number of key design factors and\n",
            "their intertwined impacts on learning efficiency and accuracy. The factors\n",
            "include the learning methods for LLM customization, the amount of personalized\n",
            "data used for learning customization, the types and sizes of LLMs, the\n",
            "compression methods of LLMs, the amount of time afforded to learn, and the\n",
            "difficulty levels of the target use cases. Through extensive experimentation\n",
            "and benchmarking, we draw a number of surprisingly insightful guidelines for\n",
            "deploying LLMs onto resource-constrained devices. For example, an optimal\n",
            "choice between parameter learning and RAG may vary depending on the difficulty\n",
            "of the downstream task, the longer fine-tuning time does not necessarily help\n",
            "the model, and a compressed LLM may be a better choice than an uncompressed LLM\n",
            "to learn from limited personalized data.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.03963v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-06T11:14:27Z\n",
            "Title:  A + B: A General Generator-Reader Framework for Optimizing LLMs to\n",
            "  Unleash Synergy Potential\n",
            "Last Author:  Pengyuan Zhou\n",
            "Authors:  Wei Tang, Yixin Cao, Jiahao Ying, Bo Wang, Yuyue Zhao, Yong Liao, Pengyuan Zhou\n",
            "abs page link: http://arxiv.org/abs/2406.03963v1\n",
            "pdf link: http://arxiv.org/pdf/2406.03963v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Accepted to ACL'24 (Findings)\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL\n",
            "Abstract: Retrieval-Augmented Generation (RAG) is an effective solution to supplement\n",
            "necessary knowledge to large language models (LLMs). Targeting its bottleneck\n",
            "of retriever performance, \"generate-then-read\" pipeline is proposed to replace\n",
            "the retrieval stage with generation from the LLM itself. Although promising,\n",
            "this research direction is underexplored and still cannot work in the scenario\n",
            "when source knowledge is given. In this paper, we formalize a general \"A + B\"\n",
            "framework with varying combinations of foundation models and types for\n",
            "systematic investigation. We explore the efficacy of the base and chat versions\n",
            "of LLMs and found their different functionalities suitable for generator A and\n",
            "reader B, respectively. Their combinations consistently outperform single\n",
            "models, especially in complex scenarios. Furthermore, we extend the application\n",
            "of the \"A + B\" framework to scenarios involving source documents through\n",
            "continuous learning, enabling the direct integration of external knowledge into\n",
            "LLMs. This approach not only facilitates effective acquisition of new knowledge\n",
            "but also addresses the challenges of safety and helpfulness post-adaptation.\n",
            "The paper underscores the versatility of the \"A + B\" framework, demonstrating\n",
            "its potential to enhance the practical application of LLMs across various\n",
            "domains.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.06519v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-10T17:58:29Z\n",
            "Title:  UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance\n",
            "  Assessor\n",
            "Last Author:  Jimmy Lin\n",
            "Authors:  Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Nick Craswell, Jimmy Lin\n",
            "abs page link: http://arxiv.org/abs/2406.06519v1\n",
            "pdf link: http://arxiv.org/pdf/2406.06519v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 5 pages, 3 figures\n",
            "Primary Category: cs.IR\n",
            "All Categories: cs.IR\n",
            "Abstract: Copious amounts of relevance judgments are necessary for the effective\n",
            "training and accurate evaluation of retrieval systems. Conventionally, these\n",
            "judgments are made by human assessors, rendering this process expensive and\n",
            "laborious. A recent study by Thomas et al. from Microsoft Bing suggested that\n",
            "large language models (LLMs) can accurately perform the relevance assessment\n",
            "task and provide human-quality judgments, but unfortunately their study did not\n",
            "yield any reusable software artifacts. Our work presents UMBRELA (a recursive\n",
            "acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source\n",
            "toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o\n",
            "model and adds more nuance to the original paper. Across Deep Learning Tracks\n",
            "from TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate\n",
            "highly with rankings generated by effective multi-stage retrieval systems. Our\n",
            "toolkit is designed to be easily extensible and can be integrated into existing\n",
            "multi-stage retrieval and evaluation pipelines, offering researchers a valuable\n",
            "resource for studying retrieval evaluation methodologies. UMBRELA will be used\n",
            "in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our\n",
            "toolkit becoming a foundation for further innovation in the field. UMBRELA is\n",
            "available at https://github.com/castorini/umbrela.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.06577v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-04T08:34:19Z\n",
            "Title:  RAG-based Crowdsourcing Task Decomposition via Masked Contrastive\n",
            "  Learning with Prompts\n",
            "Last Author:  Fei-Yue Wang\n",
            "Authors:  Jing Yang, Xiao Wang, Yu Zhao, Yuhang Liu, Fei-Yue Wang\n",
            "abs page link: http://arxiv.org/abs/2406.06577v1\n",
            "pdf link: http://arxiv.org/pdf/2406.06577v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 13 pages, 9 figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Crowdsourcing is a critical technology in social manufacturing, which\n",
            "leverages an extensive and boundless reservoir of human resources to handle a\n",
            "wide array of complex tasks. The successful execution of these complex tasks\n",
            "relies on task decomposition (TD) and allocation, with the former being a\n",
            "prerequisite for the latter. Recently, pre-trained language models (PLMs)-based\n",
            "methods have garnered significant attention. However, they are constrained to\n",
            "handling straightforward common-sense tasks due to their inherent restrictions\n",
            "involving limited and difficult-to-update knowledge as well as the presence of\n",
            "hallucinations. To address these issues, we propose a retrieval-augmented\n",
            "generation-based crowdsourcing framework that reimagines TD as event detection\n",
            "from the perspective of natural language understanding. However, the existing\n",
            "detection methods fail to distinguish differences between event types and\n",
            "always depend on heuristic rules and external semantic analyzing tools.\n",
            "Therefore, we present a Prompt-Based Contrastive learning framework for TD\n",
            "(PBCT), which incorporates a prompt-based trigger detector to overcome\n",
            "dependence. Additionally, trigger-attentive sentinel and masked contrastive\n",
            "learning are introduced to provide varying attention to trigger and contextual\n",
            "features according to different event types. Experiment results demonstrate the\n",
            "competitiveness of our method in both supervised and zero-shot detection. A\n",
            "case study on printed circuit board manufacturing is showcased to validate its\n",
            "adaptability to unknown professional domains.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.07561v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-09T18:15:12Z\n",
            "Title:  Artificial Intelligence as the New Hacker: Developing Agents for\n",
            "  Offensive Security\n",
            "Last Author:  Leroy Jacob Valencia\n",
            "Authors:  Leroy Jacob Valencia\n",
            "abs page link: http://arxiv.org/abs/2406.07561v1\n",
            "pdf link: http://arxiv.org/pdf/2406.07561v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CR\n",
            "All Categories: cs.CR, cs.AI\n",
            "Abstract: In the vast domain of cybersecurity, the transition from reactive defense to\n",
            "offensive has become critical in protecting digital infrastructures. This paper\n",
            "explores the integration of Artificial Intelligence (AI) into offensive\n",
            "cybersecurity, particularly through the development of an autonomous AI agent,\n",
            "ReaperAI, designed to simulate and execute cyberattacks. Leveraging the\n",
            "capabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\n",
            "demonstrates the potential to identify, exploit, and analyze security\n",
            "vulnerabilities autonomously.\n",
            "  This research outlines the core methodologies that can be utilized to\n",
            "increase consistency and performance, including task-driven penetration testing\n",
            "frameworks, AI-driven command generation, and advanced prompting techniques.\n",
            "The AI agent operates within a structured environment using Python, enhanced by\n",
            "Retrieval Augmented Generation (RAG) for contextual understanding and memory\n",
            "retention. ReaperAI was tested on platforms including, Hack The Box, where it\n",
            "successfully exploited known vulnerabilities, demonstrating its potential\n",
            "power.\n",
            "  However, the deployment of AI in offensive security presents significant\n",
            "ethical and operational challenges. The agent's development process revealed\n",
            "complexities in command execution, error handling, and maintaining ethical\n",
            "constraints, highlighting areas for future enhancement.\n",
            "  This study contributes to the discussion on AI's role in cybersecurity by\n",
            "showcasing how AI can augment offensive security strategies. It also proposes\n",
            "future research directions, including the refinement of AI interactions with\n",
            "cybersecurity tools, enhancement of learning mechanisms, and the discussion of\n",
            "ethical guidelines for AI in offensive roles. The findings advocate for a\n",
            "unique approach to AI implementation in cybersecurity, emphasizing innovation.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.10279v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-12T03:29:06Z\n",
            "Title:  We Have a Package for You! A Comprehensive Analysis of Package\n",
            "  Hallucinations by Code Generating LLMs\n",
            "Last Author:  Murtuza Jadliwala\n",
            "Authors:  Joseph Spracklen, Raveen Wijewickrama, A H M Nazmus Sakib, Anindya Maiti, Murtuza Jadliwala\n",
            "abs page link: http://arxiv.org/abs/2406.10279v1\n",
            "pdf link: http://arxiv.org/pdf/2406.10279v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 18 pages, 8 figures, 7 tables\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE, cs.AI, cs.CR, cs.LG\n",
            "Abstract: The reliance of popular programming languages such as Python and JavaScript\n",
            "on centralized package repositories and open-source software, combined with the\n",
            "emergence of code-generating Large Language Models (LLMs), has created a new\n",
            "type of threat to the software supply chain: package hallucinations. These\n",
            "hallucinations, which arise from fact-conflicting errors when generating code\n",
            "using LLMs, represent a novel form of package confusion attack that poses a\n",
            "critical threat to the integrity of the software supply chain. This paper\n",
            "conducts a rigorous and comprehensive evaluation of package hallucinations\n",
            "across different programming languages, settings, and parameters, exploring how\n",
            "different configurations of LLMs affect the likelihood of generating erroneous\n",
            "package recommendations and identifying the root causes of this phenomena.\n",
            "Using 16 different popular code generation models, across two programming\n",
            "languages and two unique prompt datasets, we collect 576,000 code samples which\n",
            "we analyze for package hallucinations. Our findings reveal that 19.7% of\n",
            "generated packages across all the tested LLMs are hallucinated, including a\n",
            "staggering 205,474 unique examples of hallucinated package names, further\n",
            "underscoring the severity and pervasiveness of this threat. We also implemented\n",
            "and evaluated mitigation strategies based on Retrieval Augmented Generation\n",
            "(RAG), self-detected feedback, and supervised fine-tuning. These techniques\n",
            "demonstrably reduced package hallucinations, with hallucination rates for one\n",
            "model dropping below 3%. While the mitigation efforts were effective in\n",
            "reducing hallucination rates, our study reveals that package hallucinations are\n",
            "a systemic and persistent phenomenon that pose a significant challenge for code\n",
            "generating LLMs.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.12934v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-16T22:04:10Z\n",
            "Title:  Current state of LLM Risks and AI Guardrails\n",
            "Last Author:  Limin Ge\n",
            "Authors:  Suriya Ganesh Ayyamperumal, Limin Ge\n",
            "abs page link: http://arxiv.org/abs/2406.12934v1\n",
            "pdf link: http://arxiv.org/pdf/2406.12934v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: Independent study, Exploring LLMs, Deploying LLMs and their Risks\n",
            "Primary Category: cs.CR\n",
            "All Categories: cs.CR, cs.AI, cs.HC\n",
            "Abstract: Large language models (LLMs) have become increasingly sophisticated, leading\n",
            "to widespread deployment in sensitive applications where safety and reliability\n",
            "are paramount. However, LLMs have inherent risks accompanying them, including\n",
            "bias, potential for unsafe actions, dataset poisoning, lack of explainability,\n",
            "hallucinations, and non-reproducibility. These risks necessitate the\n",
            "development of \"guardrails\" to align LLMs with desired behaviors and mitigate\n",
            "potential harm.\n",
            "  This work explores the risks associated with deploying LLMs and evaluates\n",
            "current approaches to implementing guardrails and model alignment techniques.\n",
            "We examine intrinsic and extrinsic bias evaluation methods and discuss the\n",
            "importance of fairness metrics for responsible AI development. The safety and\n",
            "reliability of agentic LLMs (those capable of real-world actions) are explored,\n",
            "emphasizing the need for testability, fail-safes, and situational awareness.\n",
            "  Technical strategies for securing LLMs are presented, including a layered\n",
            "protection model operating at external, secondary, and internal levels. System\n",
            "prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to\n",
            "minimize bias and protect privacy are highlighted.\n",
            "  Effective guardrail design requires a deep understanding of the LLM's\n",
            "intended use case, relevant regulations, and ethical considerations. Striking a\n",
            "balance between competing requirements, such as accuracy and privacy, remains\n",
            "an ongoing challenge. This work underscores the importance of continuous\n",
            "research and development to ensure the safe and responsible use of LLMs in\n",
            "real-world applications.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.16008v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-23T04:35:42Z\n",
            "Title:  Found in the Middle: Calibrating Positional Attention Bias Improves Long\n",
            "  Context Utilization\n",
            "Last Author:  Tomas Pfister\n",
            "Authors:  Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T. Le, Abhishek Kumar, James Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister\n",
            "abs page link: http://arxiv.org/abs/2406.16008v1\n",
            "pdf link: http://arxiv.org/pdf/2406.16008v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: ACL Findings 2024\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: Large language models (LLMs), even when specifically trained to process long\n",
            "input contexts, struggle to capture relevant information located in the middle\n",
            "of their input. This phenomenon has been known as the lost-in-the-middle\n",
            "problem. In this work, we make three contributions. First, we set out to\n",
            "understand the factors that cause this phenomenon. In doing so, we establish a\n",
            "connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs\n",
            "exhibit a U-shaped attention bias where the tokens at the beginning and at the\n",
            "end of its input receive higher attention, regardless of their relevance.\n",
            "Second, we mitigate this positional bias through a calibration mechanism,\n",
            "found-in-the-middle, that allows the model to attend to contexts faithfully\n",
            "according to their relevance, even though when they are in the middle. Third,\n",
            "we show found-in-the-middle not only achieves better performance in locating\n",
            "relevant information within a long context, but also eventually leads to\n",
            "improved retrieval-augmented generation (RAG) performance across various tasks,\n",
            "outperforming existing methods by up to 15 percentage points. These findings\n",
            "open up future directions in understanding LLM attention bias and its potential\n",
            "consequences.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.16252v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-24T01:22:54Z\n",
            "Title:  Graph-Augmented LLMs for Personalized Health Insights: A Case Study in\n",
            "  Sleep Analysis\n",
            "Last Author:  Amir M. Rahmani\n",
            "Authors:  Ajan Subramanian, Zhongqi Yang, Iman Azimi, Amir M. Rahmani\n",
            "abs page link: http://arxiv.org/abs/2406.16252v2\n",
            "pdf link: http://arxiv.org/pdf/2406.16252v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.LG\n",
            "All Categories: cs.LG, cs.AI\n",
            "Abstract: Health monitoring systems have revolutionized modern healthcare by enabling\n",
            "the continuous capture of physiological and behavioral data, essential for\n",
            "preventive measures and early health intervention. While integrating this data\n",
            "with Large Language Models (LLMs) has shown promise in delivering interactive\n",
            "health advice, traditional methods like Retrieval-Augmented Generation (RAG)\n",
            "and fine-tuning often fail to fully utilize the complex, multi-dimensional, and\n",
            "temporally relevant data from wearable devices. These conventional approaches\n",
            "typically provide limited actionable and personalized health insights due to\n",
            "their inadequate capacity to dynamically integrate and interpret diverse health\n",
            "data streams. In response, this paper introduces a graph-augmented LLM\n",
            "framework designed to significantly enhance the personalization and clarity of\n",
            "health insights. Utilizing a hierarchical graph structure, the framework\n",
            "captures inter and intra-patient relationships, enriching LLM prompts with\n",
            "dynamic feature importance scores derived from a Random Forest Model. The\n",
            "effectiveness of this approach is demonstrated through a sleep analysis case\n",
            "study involving 20 college students during the COVID-19 lockdown, highlighting\n",
            "the potential of our model to generate actionable and personalized health\n",
            "insights efficiently. We leverage another LLM to evaluate the insights for\n",
            "relevance, comprehensiveness, actionability, and personalization, addressing\n",
            "the critical need for models that process and interpret complex health data\n",
            "effectively. Our findings show that augmenting prompts with our framework\n",
            "yields significant improvements in all 4 criteria. Through our framework, we\n",
            "can elicit well-crafted, more thoughtful responses tailored to a specific\n",
            "patient.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.17186v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-24T23:57:57Z\n",
            "Title:  CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented\n",
            "  Analysis Generation\n",
            "Last Author:  Benjamin Van Durme\n",
            "Authors:  Abe Bohan Hou, Orion Weller, Guanghui Qin, Eugene Yang, Dawn Lawrie, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme\n",
            "abs page link: http://arxiv.org/abs/2406.17186v2\n",
            "pdf link: http://arxiv.org/pdf/2406.17186v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.CY\n",
            "Abstract: Legal professionals need to write analyses that rely on citations to relevant\n",
            "precedents, i.e., previous case decisions. Intelligent systems assisting legal\n",
            "professionals in writing such documents provide great benefits but are\n",
            "challenging to design. Such systems need to help locate, summarize, and reason\n",
            "over salient precedents in order to be useful. To enable systems for such\n",
            "tasks, we work with legal professionals to transform a large open-source legal\n",
            "corpus into a dataset supporting two important backbone tasks: information\n",
            "retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC\n",
            "(Case Law Evaluation Retrieval Corpus), is constructed for training and\n",
            "evaluating models on their ability to (1) find corresponding citations for a\n",
            "given piece of legal analysis and to (2) compile the text of these citations\n",
            "(as well as previous context) into a cogent analysis that supports a reasoning\n",
            "goal. We benchmark state-of-the-art models on CLERC, showing that current\n",
            "approaches still struggle: GPT-4o generates analyses with the highest ROUGE\n",
            "F-scores but hallucinates the most, while zero-shot IR models only achieve\n",
            "48.3% recall@1000.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.17419v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-25T09:42:56Z\n",
            "Title:  Leave No Document Behind: Benchmarking Long-Context LLMs with Extended\n",
            "  Multi-Doc QA\n",
            "Last Author:  Yongbin Li\n",
            "Authors:  Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, Yongbin Li\n",
            "abs page link: http://arxiv.org/abs/2406.17419v1\n",
            "pdf link: http://arxiv.org/pdf/2406.17419v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: We release our code and data publicly at\n",
            "  https://github.com/MozerWang/Loong\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Long-context modeling capabilities have garnered widespread attention,\n",
            "leading to the emergence of Large Language Models (LLMs) with ultra-context\n",
            "windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually\n",
            "catching up. However, existing benchmarks employ irrelevant noise texts to\n",
            "artificially extend the length of test cases, diverging from the real-world\n",
            "scenarios of long-context applications. To bridge this gap, we propose a novel\n",
            "long-context benchmark, Loong, aligning with realistic scenarios through\n",
            "extended multi-document question answering (QA). Unlike typical document QA, in\n",
            "Loong's test cases, each document is relevant to the final answer, ignoring any\n",
            "document will lead to the failure of the answer. Furthermore, Loong introduces\n",
            "four types of tasks with a range of context lengths: Spotlight Locating,\n",
            "Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic\n",
            "and comprehensive evaluation of long-context understanding. Extensive\n",
            "experiments indicate that existing long-context language models still exhibit\n",
            "considerable potential for enhancement. Retrieval augmented generation (RAG)\n",
            "achieves poor performance, demonstrating that Loong can reliably assess the\n",
            "model's long-context modeling capabilities.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.18312v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-26T12:51:37Z\n",
            "Title:  AI-native Memory: A Pathway from LLMs Towards AGI\n",
            "Last Author:  Mindverse Team\n",
            "Authors:  Jingbo Shang, Zai Zheng, Xiang Ying, Felix Tao, Mindverse Team\n",
            "abs page link: http://arxiv.org/abs/2406.18312v1\n",
            "pdf link: http://arxiv.org/pdf/2406.18312v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Large language models (LLMs) have demonstrated the world with the sparks of\n",
            "artificial general intelligence (AGI). One opinion, especially from some\n",
            "startups working on LLMs, argues that an LLM with nearly unlimited context\n",
            "length can realize AGI. However, they might be too optimistic about the\n",
            "long-context capability of (existing) LLMs -- (1) Recent literature has shown\n",
            "that their effective context length is significantly smaller than their claimed\n",
            "context length; and (2) Our reasoning-in-a-haystack experiments further\n",
            "demonstrate that simultaneously finding the relevant information from a long\n",
            "context and conducting (simple) reasoning is nearly impossible. In this paper,\n",
            "we envision a pathway from LLMs to AGI through the integration of\n",
            "\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\n",
            "processors. In addition to raw data, the memory in this system would store a\n",
            "large number of important conclusions derived from reasoning processes.\n",
            "Compared with retrieval-augmented generation (RAG) that merely processing raw\n",
            "data, this approach not only connects semantically related information closer,\n",
            "but also simplifies complex inferences at the time of querying. As an\n",
            "intermediate stage, the memory will likely be in the form of natural language\n",
            "descriptions, which can be directly consumed by users too. Ultimately, every\n",
            "agent/person should have its own large personal model, a deep neural network\n",
            "model (thus \\emph{AI-native}) that parameterizes and compresses all types of\n",
            "memory, even the ones cannot be described by natural languages. Finally, we\n",
            "discuss the significant potential of AI-native memory as the transformative\n",
            "infrastructure for (proactive) engagement, personalization, distribution, and\n",
            "social in the AGI era, as well as the incurred privacy and security challenges\n",
            "with preliminary solutions.\n",
            "e-print metadata\n",
            "arxiv-id: 2304.07197v1\n",
            "published date type <class 'str'>\n",
            "Published: 2023-04-14T15:18:44Z\n",
            "Title:  The Impacts of Neutron-Star Structure and Base Heating on Type I X-Ray\n",
            "  Bursts and Code Comparison\n",
            "Last Author:  Renxin Xu\n",
            "Authors:  Guoqing Zhen, Guoliang Lv, Helei Liu, Akira Dohi, Nobuya Nishimura, Chunhua Zhu, Liyu Song, Weiyang Wang, Renxin Xu\n",
            "abs page link: http://arxiv.org/abs/2304.07197v1\n",
            "pdf link: http://arxiv.org/pdf/2304.07197v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 14 pages, 10 figures, accepted for publication in ApJ\n",
            "Primary Category: astro-ph.HE\n",
            "All Categories: astro-ph.HE\n",
            "Abstract: Type I X-ray bursts are rapidly brightening phenomena triggered by\n",
            "thermonuclear burning on accreting layer of a neutron star (NS). The light\n",
            "curves represent the physical properties of NSs and the nuclear reactions on\n",
            "the proton-rich nuclei. The numerical treatments of the accreting NS and\n",
            "physics of the NS interior are not established, which shows uncertainty in\n",
            "modelling for observed X-ray light curves. In this study, we investigate\n",
            "theoretical X-ray-burst models, compared with burst light curves with\n",
            "GS~1826-24 observations. We focus on the impacts of the NS mass, the NS radius,\n",
            "and base-heating on the NS surface using the MESA code. We find a monotonic\n",
            "correlation between the NS mass and the parameters of the light curve. The\n",
            "higher the mass, the longer the recurrence time and the greater the peak\n",
            "luminosity. While the larger the radius, the longer the recurrence time, the\n",
            "peak luminosity remains nearly constant. In the case of increasing base\n",
            "heating, both the recurrence time and peak luminosity decrease. We also examine\n",
            "the above results using with a different numerical code, HERES, based on\n",
            "general relativity and consider the central NS. We find that the burst rate,\n",
            "burst energy and burst strength are almost same in two X-ray burst codes by\n",
            "adjusting the base-heat parameter in MESA (the relative errors $\\lesssim5\\%$),\n",
            "while the duration time and the rise time are significantly different between\n",
            "(the relative error is possibly $\\sim50\\%$). The peak luminosity and the\n",
            "e-folding time are ragged between two codes for different accretion rates.\n",
            "e-print metadata\n",
            "arxiv-id: 2307.03427v1\n",
            "published date type <class 'str'>\n",
            "Published: 2023-07-07T07:16:03Z\n",
            "Title:  Merging-Diverging Hybrid Transformer Networks for Survival Prediction in\n",
            "  Head and Neck Cancer\n",
            "Last Author:  Jinman Kim\n",
            "Authors:  Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, Jinman Kim\n",
            "abs page link: http://arxiv.org/abs/2307.03427v1\n",
            "pdf link: http://arxiv.org/pdf/2307.03427v1\n",
            "Journal reference: International Conference on Medical Image Computing and Computer\n",
            "  Assisted Intervention (MICCAI), pp. 400-410, 2023\n",
            "Comments: Early Accepted at International Conference on Medical Image Computing\n",
            "  and Computer Assisted Intervention (MICCAI 2023)\n",
            "Primary Category: eess.IV\n",
            "All Categories: eess.IV, cs.CV, cs.LG\n",
            "Abstract: Survival prediction is crucial for cancer patients as it provides early\n",
            "prognostic information for treatment planning. Recently, deep survival models\n",
            "based on deep learning and medical images have shown promising performance for\n",
            "survival prediction. However, existing deep survival models are not well\n",
            "developed in utilizing multi-modality images (e.g., PET-CT) and in extracting\n",
            "region-specific information (e.g., the prognostic information in Primary Tumor\n",
            "(PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a\n",
            "merging-diverging learning framework for survival prediction from\n",
            "multi-modality images. This framework has a merging encoder to fuse\n",
            "multi-modality information and a diverging decoder to extract region-specific\n",
            "information. In the merging encoder, we propose a Hybrid Parallel\n",
            "Cross-Attention (HPCA) block to effectively fuse multi-modality features via\n",
            "parallel convolutional layers and cross-attention transformers. In the\n",
            "diverging decoder, we propose a Region-specific Attention Gate (RAG) block to\n",
            "screen out the features related to lesion regions. Our framework is\n",
            "demonstrated on survival prediction from PET-CT images in Head and Neck (H&N)\n",
            "cancer, by designing an X-shape merging-diverging hybrid transformer network\n",
            "(named XSurv). Our XSurv combines the complementary information in PET and CT\n",
            "images and extracts the region-specific prognostic information in PT and MLN\n",
            "regions. Extensive experiments on the public dataset of HEad and neCK TumOR\n",
            "segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that\n",
            "our XSurv outperforms state-of-the-art survival prediction methods.\n",
            "e-print metadata\n",
            "arxiv-id: 2312.10904v2\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-18T03:19:31Z\n",
            "Title:  Dynamic Retrieval Augmented Generation of Ontologies using Artificial\n",
            "  Intelligence (DRAGON-AI)\n",
            "Last Author:  Christopher J Mungall\n",
            "Authors:  Sabrina Toro, Anna V Anagnostopoulos, Sue Bello, Kai Blumberg, Rhiannon Cameron, Leigh Carmody, Alexander D Diehl, Damion Dooley, William Duncan, Petra Fey, Pascale Gaudet, Nomi L Harris, Marcin Joachimiak, Leila Kiani, Tiago Lubiana, Monica C Munoz-Torres, Shawn O'Neil, David Osumi-Sutherland, Aleix Puig, Justin P Reese, Leonore Reiser, Sofia Robb, Troy Ruemping, James Seager, Eric Sid, Ray Stefancsik, Magalie Weber, Valerie Wood, Melissa A Haendel, Christopher J Mungall\n",
            "abs page link: http://arxiv.org/abs/2312.10904v2\n",
            "pdf link: http://arxiv.org/pdf/2312.10904v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.AI\n",
            "All Categories: cs.AI\n",
            "Abstract: Background: Ontologies are fundamental components of informatics\n",
            "infrastructure in domains such as biomedical, environmental, and food sciences,\n",
            "representing consensus knowledge in an accurate and computable form. However,\n",
            "their construction and maintenance demand substantial resources and necessitate\n",
            "substantial collaboration between domain experts, curators, and ontology\n",
            "experts. We present Dynamic Retrieval Augmented Generation of Ontologies using\n",
            "AI (DRAGON-AI), an ontology generation method employing Large Language Models\n",
            "(LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual\n",
            "and logical ontology components, drawing from existing knowledge in multiple\n",
            "ontologies and unstructured text sources.\n",
            "  Results: We assessed performance of DRAGON-AI on de novo term construction\n",
            "across ten diverse ontologies, making use of extensive manual evaluation of\n",
            "results. Our method has high precision for relationship generation, but has\n",
            "slightly lower precision than from logic-based reasoning. Our method is also\n",
            "able to generate definitions deemed acceptable by expert evaluators, but these\n",
            "scored worse than human-authored definitions. Notably, evaluators with the\n",
            "highest level of confidence in a domain were better able to discern flaws in\n",
            "AI-generated definitions. We also demonstrated the ability of DRAGON-AI to\n",
            "incorporate natural language instructions in the form of GitHub issues.\n",
            "  Conclusions: These findings suggest DRAGON-AI's potential to substantially\n",
            "aid the manual ontology construction process. However, our results also\n",
            "underscore the importance of having expert curators and ontology editors drive\n",
            "the ontology generation process.\n",
            "e-print metadata\n",
            "arxiv-id: 2312.15591v5\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-25T02:32:05Z\n",
            "Title:  Privacy-Preserved Neural Graph Databases\n",
            "Last Author:  Yangqiu Song\n",
            "Authors:  Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song\n",
            "abs page link: http://arxiv.org/abs/2312.15591v5\n",
            "pdf link: http://arxiv.org/pdf/2312.15591v5\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.DB\n",
            "All Categories: cs.DB, cs.CR, cs.LG\n",
            "Abstract: In the era of large language models (LLMs), efficient and accurate data\n",
            "retrieval has become increasingly crucial for the use of domain-specific or\n",
            "private data in the retrieval augmented generation (RAG). Neural graph\n",
            "databases (NGDBs) have emerged as a powerful paradigm that combines the\n",
            "strengths of graph databases (GDBs) and neural networks to enable efficient\n",
            "storage, retrieval, and analysis of graph-structured data which can be\n",
            "adaptively trained with LLMs. The usage of neural embedding storage and Complex\n",
            "neural logical Query Answering (CQA) provides NGDBs with generalization\n",
            "ability. When the graph is incomplete, by extracting latent patterns and\n",
            "representations, neural graph databases can fill gaps in the graph structure,\n",
            "revealing hidden relationships and enabling accurate query answering.\n",
            "Nevertheless, this capability comes with inherent trade-offs, as it introduces\n",
            "additional privacy risks to the domain-specific or private databases. Malicious\n",
            "attackers can infer more sensitive information in the database using\n",
            "well-designed queries such as from the answer sets of where Turing Award\n",
            "winners born before 1950 and after 1940 lived, the living places of Turing\n",
            "Award winner Hinton are probably exposed, although the living places may have\n",
            "been deleted in the training stage due to the privacy concerns. In this work,\n",
            "we propose a privacy-preserved neural graph database (P-NGDB) framework to\n",
            "alleviate the risks of privacy leakage in NGDBs. We introduce adversarial\n",
            "training techniques in the training stage to enforce the NGDBs to generate\n",
            "indistinguishable answers when queried with private information, enhancing the\n",
            "difficulty of inferring sensitive information through combinations of multiple\n",
            "innocuous queries.\n",
            "e-print metadata\n",
            "arxiv-id: 2312.17449v2\n",
            "published date type <class 'str'>\n",
            "Published: 2023-12-29T03:23:23Z\n",
            "Title:  DB-GPT: Empowering Database Interactions with Private Large Language\n",
            "  Models\n",
            "Last Author:  Faqiang Chen\n",
            "Authors:  Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, Wang Zhao, Fan Zhou, Danrui Qi, Hong Yi, Shaodong Liu, Faqiang Chen\n",
            "abs page link: http://arxiv.org/abs/2312.17449v2\n",
            "pdf link: http://arxiv.org/pdf/2312.17449v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.DB\n",
            "All Categories: cs.DB\n",
            "Abstract: The recent breakthroughs in large language models (LLMs) are positioned to\n",
            "transition many areas of software. Database technologies particularly have an\n",
            "important entanglement with LLMs as efficient and intuitive database\n",
            "interactions are paramount. In this paper, we present DB-GPT, a revolutionary\n",
            "and production-ready project that integrates LLMs with traditional database\n",
            "systems to enhance user experience and accessibility. DB-GPT is designed to\n",
            "understand natural language queries, provide context-aware responses, and\n",
            "generate complex SQL queries with high accuracy, making it an indispensable\n",
            "tool for users ranging from novice to expert. The core innovation in DB-GPT\n",
            "lies in its private LLM technology, which is fine-tuned on domain-specific\n",
            "corpora to maintain user privacy and ensure data security while offering the\n",
            "benefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which\n",
            "includes a novel retrieval augmented generation (RAG) knowledge system, an\n",
            "adaptive learning mechanism to continuously improve performance based on user\n",
            "feedback and a service-oriented multi-model framework (SMMF) with powerful\n",
            "data-driven agents. Our extensive experiments and user studies confirm that\n",
            "DB-GPT represents a paradigm shift in database interactions, offering a more\n",
            "natural, efficient, and secure way to engage with data repositories. The paper\n",
            "concludes with a discussion of the implications of DB-GPT framework on the\n",
            "future of human-database interaction and outlines potential avenues for further\n",
            "enhancements and applications in the field. The project code is available at\n",
            "https://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself by\n",
            "installing it with the instructions\n",
            "https://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minute\n",
            "video at https://www.youtube.com/watch?v=KYs4nTDzEhk.\n",
            "e-print metadata\n",
            "arxiv-id: 2401.15269v3\n",
            "published date type <class 'str'>\n",
            "Published: 2024-01-27T02:29:42Z\n",
            "Title:  Improving Medical Reasoning through Retrieval and Self-Reflection with\n",
            "  Retrieval-Augmented Large Language Models\n",
            "Last Author:  Jaewoo Kang\n",
            "Authors:  Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang\n",
            "abs page link: http://arxiv.org/abs/2401.15269v3\n",
            "pdf link: http://arxiv.org/pdf/2401.15269v3\n",
            "Journal reference: No journal ref found\n",
            "Comments: ISMB 2024\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.IR\n",
            "Abstract: Recent proprietary large language models (LLMs), such as GPT-4, have achieved\n",
            "a milestone in tackling diverse challenges in the biomedical domain, ranging\n",
            "from multiple-choice questions to long-form generations. To address challenges\n",
            "that still cannot be handled with the encoded knowledge of LLMs, various\n",
            "retrieval-augmented generation (RAG) methods have been developed by searching\n",
            "documents from the knowledge corpus and appending them unconditionally or\n",
            "selectively to the input of LLMs for generation. However, when applying\n",
            "existing methods to different domain-specific problems, poor generalization\n",
            "becomes apparent, leading to fetching incorrect documents or making inaccurate\n",
            "judgments. In this paper, we introduce Self-BioRAG, a framework reliable for\n",
            "biomedical text that specializes in generating explanations, retrieving\n",
            "domain-specific documents, and self-reflecting generated responses. We utilize\n",
            "84k filtered biomedical instruction sets to train Self-BioRAG that can assess\n",
            "its generated explanations with customized reflective tokens. Our work proves\n",
            "that domain-specific components, such as a retriever, domain-related document\n",
            "corpus, and instruction sets are necessary for adhering to domain-related\n",
            "instructions. Using three major medical question-answering benchmark datasets,\n",
            "experimental results of Self-BioRAG demonstrate significant performance gains\n",
            "by achieving a 7.2% absolute improvement on average over the state-of-the-art\n",
            "open-foundation model with a parameter size of 7B or less. Overall, we analyze\n",
            "that Self-BioRAG finds the clues in the question, retrieves relevant documents\n",
            "if needed, and understands how to answer with information from retrieved\n",
            "documents and encoded knowledge as a medical expert does. We release our data\n",
            "and code for training our framework components and model weights (7B and 13B)\n",
            "to enhance capabilities in biomedical and clinical domains.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.07688v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-12T14:53:28Z\n",
            "Title:  CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation\n",
            "  for Evaluating LLMs in Cybersecurity Knowledge\n",
            "Last Author:  Merouane Debbah\n",
            "Authors:  Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Tamas Bisztray, Merouane Debbah\n",
            "abs page link: http://arxiv.org/abs/2402.07688v2\n",
            "pdf link: http://arxiv.org/pdf/2402.07688v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.AI\n",
            "All Categories: cs.AI, cs.CR\n",
            "Abstract: Large Language Models (LLMs) are increasingly used across various domains,\n",
            "from software development to cyber threat intelligence. Understanding all the\n",
            "different fields of cybersecurity, which includes topics such as cryptography,\n",
            "reverse engineering, and risk assessment, poses a challenge even for human\n",
            "experts. To accurately test the general knowledge of LLMs in cybersecurity, the\n",
            "research community needs a diverse, accurate, and up-to-date dataset. To\n",
            "address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000,\n",
            "and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets\n",
            "comprising 80, 500, 2000, and 10,000 questions respectively. By utilizing\n",
            "GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents,\n",
            "including NIST standards, research papers, publicly accessible books, RFCs, and\n",
            "other publications in the cybersecurity domain, to generate questions, each\n",
            "with four possible answers. The results underwent several rounds of error\n",
            "checking and refinement. Human experts invested over 200 hours validating the\n",
            "questions and solutions to ensure their accuracy and relevance, and to filter\n",
            "out any questions unrelated to cybersecurity. We have evaluated and compared 25\n",
            "state-of-the-art LLM models on the CyberMetric datasets. In addition to our\n",
            "primary goal of evaluating LLMs, we involved 30 human participants to solve\n",
            "CyberMetric-80 in a closed-book scenario. The results can serve as a reference\n",
            "for comparing the general cybersecurity knowledge of humans and LLMs. The\n",
            "findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct,\n",
            "Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs.\n",
            "Additionally, the top LLMs were more accurate than humans on CyberMetric-80,\n",
            "although highly experienced human experts still outperformed small models such\n",
            "as Llama-3-8B, Phi-2 or Gemma-7b.\n",
            "e-print metadata\n",
            "arxiv-id: 2402.12659v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-02-20T02:16:16Z\n",
            "Title:  FinBen: A Holistic Financial Benchmark for Large Language Models\n",
            "Last Author:  Jimin Huang\n",
            "Authors:  Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu, Gang Hu, Jiajia Huang, Xiao-Yang Liu, Alejandro Lopez-Lira, Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, Jimin Huang\n",
            "abs page link: http://arxiv.org/abs/2402.12659v2\n",
            "pdf link: http://arxiv.org/pdf/2402.12659v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: 26 pages, 11 figures\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.CE\n",
            "Abstract: LLMs have transformed NLP and shown promise in various fields, yet their\n",
            "potential in finance is underexplored due to a lack of comprehensive evaluation\n",
            "benchmarks, the rapid development of LLMs, and the complexity of financial\n",
            "tasks. In this paper, we introduce FinBen, the first extensive open-source\n",
            "evaluation benchmark, including 36 datasets spanning 24 financial tasks,\n",
            "covering seven critical aspects: information extraction (IE), textual analysis,\n",
            "question answering (QA), text generation, risk management, forecasting, and\n",
            "decision-making. FinBen offers several key innovations: a broader range of\n",
            "tasks and datasets, the first evaluation of stock trading, novel agent and\n",
            "Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source\n",
            "evaluation datasets for text summarization, question answering, and stock\n",
            "trading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT,\n",
            "and the latest Gemini, reveals several key findings: While LLMs excel in IE and\n",
            "textual analysis, they struggle with advanced reasoning and complex tasks like\n",
            "text generation and forecasting. GPT-4 excels in IE and stock trading, while\n",
            "Gemini is better at text generation and forecasting. Instruction-tuned LLMs\n",
            "improve textual analysis but offer limited benefits for complex tasks such as\n",
            "QA. FinBen has been used to host the first financial LLMs shared task at the\n",
            "FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel\n",
            "solutions outperformed GPT-4, showcasing FinBen's potential to drive innovation\n",
            "in financial LLMs. All datasets, results, and codes are released for the\n",
            "research community: https://github.com/The-FinAI/PIXIU.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.07952v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-12T02:30:50Z\n",
            "Title:  AesopAgent: Agent-driven Evolutionary System on Story-to-Video\n",
            "  Production\n",
            "Last Author:  Zhenyu Guo\n",
            "Authors:  Jiuniu Wang, Zehua Du, Yuyuan Zhao, Bo Yuan, Kexiang Wang, Jian Liang, Yaxi Zhao, Yihen Lu, Gengliang Li, Junlong Gao, Xin Tu, Zhenyu Guo\n",
            "abs page link: http://arxiv.org/abs/2403.07952v1\n",
            "pdf link: http://arxiv.org/pdf/2403.07952v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: 22 pages, 13 figures\n",
            "Primary Category: cs.CV\n",
            "All Categories: cs.CV, cs.AI, cs.MM\n",
            "Abstract: The Agent and AIGC (Artificial Intelligence Generated Content) technologies\n",
            "have recently made significant progress. We propose AesopAgent, an Agent-driven\n",
            "Evolutionary System on Story-to-Video Production. AesopAgent is a practical\n",
            "application of agent technology for multimodal content generation. The system\n",
            "integrates multiple generative capabilities within a unified framework, so that\n",
            "individual users can leverage these modules easily. This innovative system\n",
            "would convert user story proposals into scripts, images, and audio, and then\n",
            "integrate these multimodal contents into videos. Additionally, the animating\n",
            "units (e.g., Gen-2 and Sora) could make the videos more infectious. The\n",
            "AesopAgent system could orchestrate task workflow for video generation,\n",
            "ensuring that the generated video is both rich in content and coherent. This\n",
            "system mainly contains two layers, i.e., the Horizontal Layer and the Utility\n",
            "Layer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary\n",
            "system that optimizes the whole video generation workflow and the steps within\n",
            "the workflow. It continuously evolves and iteratively optimizes workflow by\n",
            "accumulating expert experience and professional knowledge, including optimizing\n",
            "the LLM prompts and utilities usage. The Utility Layer provides multiple\n",
            "utilities, leading to consistent image generation that is visually coherent in\n",
            "terms of composition, characters, and style. Meanwhile, it provides audio and\n",
            "special effects, integrating them into expressive and logically arranged\n",
            "videos. Overall, our AesopAgent achieves state-of-the-art performance compared\n",
            "with many previous works in visual storytelling. Our AesopAgent is designed for\n",
            "convenient service for individual users, which is available on the following\n",
            "page: https://aesopai.github.io/.\n",
            "e-print metadata\n",
            "arxiv-id: 2403.17209v4\n",
            "published date type <class 'str'>\n",
            "Published: 2024-03-25T21:37:30Z\n",
            "Title:  Generation of Asset Administration Shell with Large Language Model\n",
            "  Agents: Toward Semantic Interoperability in Digital Twins in the Context of\n",
            "  Industry 4.0\n",
            "Last Author:  Michael Weyrich\n",
            "Authors:  Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich\n",
            "abs page link: http://arxiv.org/abs/2403.17209v4\n",
            "pdf link: http://arxiv.org/pdf/2403.17209v4\n",
            "Journal reference: No journal ref found\n",
            "Comments: Published in IEEE Access\n",
            "Primary Category: cs.AI\n",
            "All Categories: cs.AI, cs.IR, cs.MA, cs.SE\n",
            "Abstract: This research introduces a novel approach for achieving semantic\n",
            "interoperability in digital twins and assisting the creation of Asset\n",
            "Administration Shell (AAS) as digital twin model within the context of Industry\n",
            "4.0. The foundational idea of our research is that the communication based on\n",
            "semantics and the generation of meaningful textual data are directly linked,\n",
            "and we posit that these processes are equivalent if the exchanged information\n",
            "can be serialized in text form. Based on this, we construct a \"semantic node\"\n",
            "data structure in our research to capture the semantic essence of textual data.\n",
            "Then, a system powered by large language models is designed and implemented to\n",
            "process the \"semantic node\" and generate standardized digital twin models from\n",
            "raw textual data collected from datasheets describing technical assets. Our\n",
            "evaluation demonstrates an effective generation rate of 62-79%, indicating a\n",
            "substantial proportion of the information from the source text can be\n",
            "translated error-free to the target digital twin instance model with the\n",
            "generative capability of large language models. This result has a direct\n",
            "application in the context of Industry 4.0, and the designed system is\n",
            "implemented as a data model generation tool for reducing the manual effort in\n",
            "creating AAS model. In our evaluation, a comparative analysis of different LLMs\n",
            "and an in-depth ablation study of Retrieval-Augmented Generation (RAG)\n",
            "mechanisms provide insights into the effectiveness of LLM systems for\n",
            "interpreting technical concepts and translating data. Our findings emphasize\n",
            "LLMs' capability to automate AAS instance creation and contribute to the\n",
            "broader field of semantic interoperability for digital twins in industrial\n",
            "applications. The prototype implementation and evaluation results are presented\n",
            "on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.\n",
            "e-print metadata\n",
            "arxiv-id: 2404.10198v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-04-16T00:43:03Z\n",
            "Title:  ClashEval: Quantifying the tug-of-war between an LLM's internal prior\n",
            "  and external evidence\n",
            "Last Author:  James Zou\n",
            "Authors:  Kevin Wu, Eric Wu, James Zou\n",
            "abs page link: http://arxiv.org/abs/2404.10198v2\n",
            "pdf link: http://arxiv.org/pdf/2404.10198v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: Revised June 9 2024\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Retrieval augmented generation (RAG) is frequently used to mitigate\n",
            "hallucinations and provide up-to-date knowledge for large language models\n",
            "(LLMs). However, given that document retrieval is an imprecise task and\n",
            "sometimes results in erroneous or even harmful content being presented in\n",
            "context, this raises the question of how LLMs handle retrieved information: If\n",
            "the provided content is incorrect, does the model know to ignore it, or does it\n",
            "recapitulate the error? Conversely, when the model's initial response is\n",
            "incorrect, does it always know to use the retrieved information to correct\n",
            "itself, or does it insist on its wrong prior response? To answer this, we\n",
            "curate a dataset of over 1200 questions across six domains (e.g., drug dosages,\n",
            "Olympic records, locations) along with content relevant to answering each\n",
            "question. We further apply precise perturbations to the answers in the content\n",
            "that range from subtle to blatant errors. We benchmark six top-performing LLMs,\n",
            "including GPT-4o, on this dataset and find that LLMs are susceptible to\n",
            "adopting incorrect retrieved content, overriding their own correct prior\n",
            "knowledge over 60% of the time. However, the more unrealistic the retrieved\n",
            "content is (i.e. more deviated from truth), the less likely the model is to\n",
            "adopt it. Also, the less confident a model is in its initial response (via\n",
            "measuring token probabilities), the more likely it is to adopt the information\n",
            "in the retrieved content. We exploit this finding and demonstrate simple\n",
            "methods for improving model accuracy where there is conflicting retrieved\n",
            "content. Our results highlight a difficult task and benchmark for LLMs --\n",
            "namely, their ability to correctly discern when it is wrong in light of correct\n",
            "retrieved content and to reject cases when the provided content is incorrect.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.03267v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-06T08:38:14Z\n",
            "Title:  Characterizing the Dilemma of Performance and Index Size in\n",
            "  Billion-Scale Vector Search and Breaking It with Second-Tier Memory\n",
            "Last Author:  Haibo Chen\n",
            "Authors:  Rongxin Cheng, Yifan Peng, Xingda Wei, Hongrui Xie, Rong Chen, Sijie Shen, Haibo Chen\n",
            "abs page link: http://arxiv.org/abs/2405.03267v2\n",
            "pdf link: http://arxiv.org/pdf/2405.03267v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.DC\n",
            "All Categories: cs.DC, cs.DB, cs.IR\n",
            "Abstract: Vector searches on large-scale datasets are critical to modern online\n",
            "services like web search and RAG, which necessity storing the datasets and\n",
            "their index on the secondary storage like SSD. In this paper, we are the first\n",
            "to characterize the trade-off of performance and index size in existing\n",
            "SSD-based graph and cluster indexes: to improve throughput by 5.7$\\times$ and\n",
            "1.7$\\times$, these indexes have to pay a 5.8$\\times$ storage amplification and\n",
            "7.7$\\times$ with respect to the dataset size, respectively. The root cause is\n",
            "that the coarse-grained access of SSD mismatches the fine-grained random read\n",
            "required by vector indexes with small amplification.\n",
            "  This paper argues that second-tier memory, such as remote DRAM/NVM connected\n",
            "via RDMA or CXL, is a powerful storage for addressing the problem from a\n",
            "system's perspective, thanks to its fine-grained access granularity. However,\n",
            "putting existing indexes -- primarily designed for SSD -- directly on\n",
            "second-tier memory cannot fully utilize its power. Meanwhile, second-tier\n",
            "memory still behaves more like storage, so using it as DRAM is also\n",
            "inefficient. To this end, we build a graph and cluster index that centers\n",
            "around the performance features of second-tier memory. With careful execution\n",
            "engine and index layout designs, we show that vector indexes can achieve\n",
            "optimal performance with orders of magnitude smaller index amplification, on a\n",
            "variety of second-tier memory devices.\n",
            "  Based on our improved graph and vector indexes on second-tier memory, we\n",
            "further conduct a systematic study between them to facilitate developers\n",
            "choosing the right index for their workloads. Interestingly, the findings on\n",
            "the second-tier memory contradict the ones on SSDs.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.14383v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-23T10:00:14Z\n",
            "Title:  Perception of Knowledge Boundary for Large Language Models through\n",
            "  Semi-open-ended Question Answering\n",
            "Last Author:  Dongsheng Li\n",
            "Authors:  Zhihua Wen, Zhiliang Tian, Zexin Jian, Zhen Huang, Pei Ke, Yifu Gao, Minlie Huang, Dongsheng Li\n",
            "abs page link: http://arxiv.org/abs/2405.14383v1\n",
            "pdf link: http://arxiv.org/pdf/2405.14383v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI\n",
            "Abstract: Large Language Models (LLMs) are widely used for knowledge-seeking yet suffer\n",
            "from hallucinations. The knowledge boundary (KB) of an LLM limits its factual\n",
            "understanding, beyond which it may begin to hallucinate. Investigating the\n",
            "perception of LLMs' KB is crucial for detecting hallucinations and LLMs'\n",
            "reliable generation. Current studies perceive LLMs' KB on questions with a\n",
            "concrete answer (close-ended questions) while paying limited attention to\n",
            "semi-open-ended questions (SoeQ) that correspond to many potential answers.\n",
            "Some researchers achieve it by judging whether the question is answerable or\n",
            "not. However, this paradigm is unsuitable for SoeQ, which are usually partially\n",
            "answerable, containing both answerable and ambiguous (unanswerable) answers.\n",
            "Ambiguous answers are essential for knowledge-seeking, but they may go beyond\n",
            "the KB of LLMs. In this paper, we perceive the LLMs' KB with SoeQ by\n",
            "discovering more ambiguous answers. First, we apply an LLM-based approach to\n",
            "construct SoeQ and obtain answers from a target LLM. Unfortunately, the output\n",
            "probabilities of mainstream black-box LLMs are inaccessible to sample for\n",
            "low-probability ambiguous answers. Therefore, we apply an open-sourced\n",
            "auxiliary model to explore ambiguous answers for the target LLM. We calculate\n",
            "the nearest semantic representation for existing answers to estimate their\n",
            "probabilities, with which we reduce the generation probability of\n",
            "high-probability answers to achieve a more effective generation. Finally, we\n",
            "compare the results from the RAG-based evaluation and LLM self-evaluation to\n",
            "categorize four types of ambiguous answers that are beyond the KB of the target\n",
            "LLM. Following our method, we construct a dataset to perceive the KB for GPT-4.\n",
            "We find that GPT-4 performs poorly on SoeQ and is often unaware of its KB.\n",
            "Besides, our auxiliary model, LLaMA-2-13B, is effective in discovering more\n",
            "ambiguous answers.\n",
            "e-print metadata\n",
            "arxiv-id: 2405.16444v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-05-26T06:00:17Z\n",
            "Title:  CacheBlend: Fast Large Language Model Serving for RAG with Cached\n",
            "  Knowledge Fusion\n",
            "Last Author:  Junchen Jiang\n",
            "Authors:  Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang\n",
            "abs page link: http://arxiv.org/abs/2405.16444v2\n",
            "pdf link: http://arxiv.org/pdf/2405.16444v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.LG\n",
            "All Categories: cs.LG\n",
            "Abstract: Large language models (LLMs) often incorporate multiple text chunks in their\n",
            "inputs to provide the necessary contexts. To speed up the prefill of the long\n",
            "LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\n",
            "when the context is reused as the prefix of another LLM input. However, the\n",
            "reused text chunks are not always the input prefix, and when they are not,\n",
            "their precomputed KV caches cannot be directly used since they ignore the\n",
            "text's cross-attention with the preceding text in the LLM input. Thus, the\n",
            "benefits of reusing KV caches remain largely unrealized.\n",
            "  This paper tackles just one question: when an LLM input contains multiple\n",
            "text chunks, how to quickly combine their precomputed KV caches in order to\n",
            "achieve the same generation quality as the expensive full prefill (i.e.,\n",
            "without reusing KV cache)? We present CacheBlend, a scheme that reuses the\n",
            "pre-computed KV caches, regardless prefix or not, and selectively recomputes\n",
            "the KV values of a small subset of tokens to partially update each reused KV\n",
            "cache. In the meantime,the small extra delay for recomputing some tokens can be\n",
            "pipelined with the retrieval of KV caches within the same job,allowing\n",
            "CacheBlend to store KV caches in slower devices with more storage capacity\n",
            "while retrieving them without increasing the inference delay. By comparing\n",
            "CacheBlend with the state-of-the-art KV cache reusing schemes on three\n",
            "open-source LLMs of various sizes and four popular benchmark datasets of\n",
            "different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n",
            "2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full\n",
            "KV recompute, without compromising generation quality or incurring more storage\n",
            "cost.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.05514v2\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-08T16:24:24Z\n",
            "Title:  RAG-Enhanced Commit Message Generation\n",
            "Last Author:  Peng Liang\n",
            "Authors:  Linghao Zhang, Hongyi Zhang, Chong Wang, Peng Liang\n",
            "abs page link: http://arxiv.org/abs/2406.05514v2\n",
            "pdf link: http://arxiv.org/pdf/2406.05514v2\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.SE\n",
            "All Categories: cs.SE\n",
            "Abstract: Commit message is one of the most important textual information in software\n",
            "development and maintenance. However, it is time-consuming and labor-intensive\n",
            "to write commit messages manually. Commit Message Generation (CMG) has become a\n",
            "research hotspot in automated software engineering. Researchers have proposed\n",
            "several methods for CMG and achieved great results. In recent years, CodeBERT,\n",
            "CodeT5, and other Pre-trained Language Models (PLMs) for code have been\n",
            "proposed. These models can be easily transferred to code-related downstream\n",
            "tasks including CMG with simple fine-tuning and can achieve impressive\n",
            "performance. Moreover, Large Language Models (LLMs) with code capabilities\n",
            "(e.g., ChatGPT, Llama 3, Gemma) can be directly applied to various tasks by\n",
            "designing instruct prompts without training. This brings new possibilities to\n",
            "the CMG task. In this work, we propose REACT, a novel REtrieval-Augmented\n",
            "framework for CommiT message generation, which effectively integrates advanced\n",
            "retrieval techniques with different PLMs and LLMs and can broadly enhance the\n",
            "performance of various models on the CMG task. Specifically, we design and\n",
            "build a hybrid retriever to retrieve the most relevant code diff and commit\n",
            "message pair from the code base as an \"exemplar\". Then, the retrieved pair is\n",
            "utilized to guide and enhance the generation of commit messages by PLMs and\n",
            "LLMs through fine-tuning and in-context learning. Our approach is evaluated on\n",
            "a widely-used dataset. The experimental results show that REACT significantly\n",
            "enhances the performance of various models on the CMG task, improving the BLEU\n",
            "score of CodeT5 by up to 55%, boosting Llama 3's BLEU score by 102%, and\n",
            "substantially surpassing all baselines, achieving a new SOTA. This demonstrates\n",
            "the effectiveness and broad applicability of our framework that can enhance CMG\n",
            "by a large margin.\n",
            "e-print metadata\n",
            "arxiv-id: 2406.18039v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-06-26T03:32:35Z\n",
            "Title:  Diagnosis Assistant for Liver Cancer Utilizing a Large Language Model\n",
            "  with Three Types of Knowledge\n",
            "Last Author:  Kehong Yuan (SIGS, Tsinghua University)\n",
            "Authors:  Xuzhou Wu, Guangxin Li, Xing Wang, Zeyu Xu, Yingni Wang, Jianming Xian, Xueyu Wang, Gong Li, Kehong Yuan\n",
            "abs page link: http://arxiv.org/abs/2406.18039v1\n",
            "pdf link: http://arxiv.org/pdf/2406.18039v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: physics.med-ph\n",
            "All Categories: physics.med-ph\n",
            "Abstract: Liver cancer has a high incidence rate, but primary healthcare settings often\n",
            "lack experienced doctors. Advances in large models and AI technologies offer\n",
            "potential assistance. This work aims to address limitations in liver cancer\n",
            "diagnosis models, such as poor understanding of medical images, insufficient\n",
            "consideration of liver blood vessels, and ensuring accurate medical\n",
            "information. We propose a specialized diagnostic assistant to improve the\n",
            "diagnostic capabilities of less experienced doctors. Our framework combines\n",
            "large and small models, using optimized small models for precise patient image\n",
            "perception. Specifically, a segmentation network iteratively removes ambiguous\n",
            "pixels for liver tumor segmentation, and a multi-scale, multi-level\n",
            "differential network segments liver vessels. Features from these segmentations\n",
            "and medical records form a patient's personalized knowledge base. For\n",
            "diagnosis, Chain of Thought (COT) technology designs prompts mimicking\n",
            "experienced doctors' thought patterns, and Retrieval-Augmented Generation (RAG)\n",
            "technology provides answers based on reliable domain knowledge and trusted\n",
            "cases. Our small model methods improve liver tumor and vessel segmentation\n",
            "performance, resulting in more accurate information extraction. The large model\n",
            "component scores over 1 point higher on a 10-point scale in evaluations by\n",
            "doctors compared to control methods. Our method enhances semantic perception of\n",
            "medical images, improves classification of ambiguous pixels, and optimizes\n",
            "small object perception. It considers blood vessel positions for specific\n",
            "treatments and improves response credibility and interpretability by mimicking\n",
            "experienced doctors' thought processes using reliable resources. This approach\n",
            "has been recognized by doctors and benefits liver cancer auxiliary diagnosis.\n",
            "e-print metadata\n",
            "arxiv-id: 2401.17268v1\n",
            "published date type <class 'str'>\n",
            "Published: 2024-01-30T18:58:43Z\n",
            "Title:  Weaver: Foundation Models for Creative Writing\n",
            "Last Author:  Wangchunshu Zhou\n",
            "Authors:  Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, Wangchunshu Zhou\n",
            "abs page link: http://arxiv.org/abs/2401.17268v1\n",
            "pdf link: http://arxiv.org/pdf/2401.17268v1\n",
            "Journal reference: No journal ref found\n",
            "Comments: No comment found\n",
            "Primary Category: cs.CL\n",
            "All Categories: cs.CL, cs.AI, cs.LG\n",
            "Abstract: This work introduces Weaver, our first family of large language models (LLMs)\n",
            "dedicated to content creation. Weaver is pre-trained on a carefully selected\n",
            "corpus that focuses on improving the writing capabilities of large language\n",
            "models. We then fine-tune Weaver for creative and professional writing purposes\n",
            "and align it to the preference of professional writers using a suit of novel\n",
            "methods for instruction data synthesis and LLM alignment, making it able to\n",
            "produce more human-like texts and follow more diverse instructions for content\n",
            "creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver\n",
            "Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for\n",
            "different applications and can be dynamically dispatched by a routing agent\n",
            "according to query complexity to balance response quality and computation cost.\n",
            "Evaluation on a carefully curated benchmark for assessing the writing\n",
            "capabilities of LLMs shows Weaver models of all sizes outperform generalist\n",
            "LLMs several times larger than them. Notably, our most-capable Weaver Ultra\n",
            "model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing\n",
            "scenarios, demonstrating the advantage of training specialized LLMs for writing\n",
            "purposes. Moreover, Weaver natively supports retrieval-augmented generation\n",
            "(RAG) and function calling (tool usage). We present various use cases of these\n",
            "abilities for improving AI-assisted writing systems, including integration of\n",
            "external knowledge bases, tools, or APIs, and providing personalized writing\n",
            "assistance. Furthermore, we discuss and summarize a guideline and best\n",
            "practices for pre-training and fine-tuning domain-specific LLMs.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "python_arXiv_parsing_example.py\n",
        "\n",
        "This sample script illustrates a basic arXiv api call\n",
        "followed by parsing of the results using the\n",
        "feedparser python module.\n",
        "\n",
        "Please see the documentation at\n",
        "http://export.arxiv.org/api_help/docs/user-manual.html\n",
        "for more information, or email the arXiv api\n",
        "mailing list at arxiv-api@googlegroups.com.\n",
        "\n",
        "urllib is included in the standard python library.\n",
        "feedparser can be downloaded from http://feedparser.org/ .\n",
        "\n",
        "Author: Julius B. Lucks\n",
        "\n",
        "This is free software.  Feel free to do what you want\n",
        "with it, but please play nice with the arXiv API!\n",
        "\"\"\"\n",
        "\n",
        "import urllib.request\n",
        "import feedparser\n",
        "\n",
        "# Base api query url\n",
        "base_url = 'http://export.arxiv.org/api/query?'\n",
        "\n",
        "# Search parameters\n",
        "search_query = 'all:rag'  # search for rag in all fields\n",
        "start = 0                      # retrieve the first 5 results\n",
        "max_results = 516\n",
        "\n",
        "query = 'search_query=%s&start=%i&max_results=%i' % (search_query, start, max_results)\n",
        "# query = 'search_query=%s&start=%i' % (search_query, start)\n",
        "\n",
        "# Opensearch metadata such as totalResults, startIndex,\n",
        "# and itemsPerPage live in the opensearch namespace.\n",
        "# Some entry metadata lives in the arXiv namespace.\n",
        "# This is a hack to expose both of these namespaces in\n",
        "# feedparser v4.1\n",
        "# feedparser.FeedParserMixin.namespaces['http://a9.com/-/spec/opensearch/1.1/'] = 'opensearch'\n",
        "# feedparser.FeedParserMixin.namespaces['http://arxiv.org/schemas/atom'] = 'arxiv'\n",
        "\n",
        "pdf_links = {}\n",
        "# perform a GET request using the base_url and query\n",
        "response = urllib.request.urlopen(base_url + query).read()\n",
        "\n",
        "# parse the response using feedparser\n",
        "feed = feedparser.parse(response)\n",
        "\n",
        "# print out feed information\n",
        "print('Feed title: %s' % feed.feed.title)\n",
        "print('Feed last updated: %s' % feed.feed.updated)\n",
        "\n",
        "# print opensearch metadata\n",
        "print('totalResults for this query: %s' % feed.feed.opensearch_totalresults)\n",
        "print('itemsPerPage for this query: %s' % feed.feed.opensearch_itemsperpage)\n",
        "print('startIndex for this query: %s' % feed.feed.opensearch_startindex)\n",
        "\n",
        "# Run through each entry, and print out information\n",
        "for entry in feed.entries:\n",
        "    published_year = int(entry.published[0:4])\n",
        "    if published_year >=2022:\n",
        "      inner_dict = {}\n",
        "      print('e-print metadata')\n",
        "      print('arxiv-id: %s' % entry.id.split('/abs/')[-1])\n",
        "      print('published date type',type(entry.published))\n",
        "      print('Published: %s' % entry.published)\n",
        "      print('Title:  %s' % entry.title)\n",
        "\n",
        "      inner_dict['Published Date'] = entry.published\n",
        "      # feedparser v4.1 only grabs the first author\n",
        "      author_string = entry.author\n",
        "\n",
        "      # grab the affiliation in <arxiv:affiliation> if present\n",
        "      # - this will only grab the first affiliation encountered\n",
        "      #   (the first affiliation for the first author)\n",
        "      # Please email the list with a way to get all of this information!\n",
        "      try:\n",
        "          author_string += ' (%s)' % entry.arxiv_affiliation\n",
        "      except AttributeError:\n",
        "          pass\n",
        "\n",
        "      print('Last Author:  %s' % author_string)\n",
        "\n",
        "      # feedparser v5.0.1 correctly handles multiple authors, print them all\n",
        "      try:\n",
        "          print('Authors:  %s' % ', '.join(author.name for author in entry.authors))\n",
        "      except AttributeError:\n",
        "          pass\n",
        "\n",
        "      # get the links to the abs page and pdf for this e-print\n",
        "\n",
        "      for link in entry.links:\n",
        "          count = 1\n",
        "          if link.rel == 'alternate':\n",
        "              print('abs page link: %s' % link.href)\n",
        "              inner_dict['alternate link'+str(count)] = link.href\n",
        "          elif link.title == 'pdf':\n",
        "              print('pdf link: %s' % link.href)\n",
        "              inner_dict['pdf link'+str(count)] =  link.href\n",
        "          count+=1\n",
        "\n",
        "      # The journal reference, comments and primary_category sections live under\n",
        "      # the arxiv namespace\n",
        "      try:\n",
        "          journal_ref = entry.arxiv_journal_ref\n",
        "      except AttributeError:\n",
        "          journal_ref = 'No journal ref found'\n",
        "      print('Journal reference: %s' % journal_ref)\n",
        "\n",
        "      try:\n",
        "          comment = entry.arxiv_comment\n",
        "      except AttributeError:\n",
        "          comment = 'No comment found'\n",
        "      print('Comments: %s' % comment)\n",
        "\n",
        "      # Since the <arxiv:primary_category> element has no data, only\n",
        "      # attributes, feedparser does not store anything inside\n",
        "      # entry.arxiv_primary_category\n",
        "      # This is a dirty hack to get the primary_category, just take the\n",
        "      # first element in entry.tags.  If anyone knows a better way to do\n",
        "      # this, please email the list!\n",
        "      print('Primary Category: %s' % entry.tags[0]['term'])\n",
        "\n",
        "      # Lets get all the categories\n",
        "      all_categories = [t['term'] for t in entry.tags]\n",
        "      print('All Categories: %s' % ', '.join(all_categories))\n",
        "\n",
        "      # The abstract is in the <summary> element\n",
        "      print('Abstract: %s' % entry.summary)\n",
        "      pdf_links[entry.title] = inner_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feed.feed.opensearch_totalresults"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pnrtjSUxZWmw",
        "outputId": "c88f628a-c85d-449f-87f2-f51b81fee97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'516'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pdf_links)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9w_XxScLih9",
        "outputId": "f2fc6db5-3df2-48b7-dcae-2a11d1c7cd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "467"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_pubDates = [pdf_links]"
      ],
      "metadata": {
        "id": "JSEGNV2ZYjDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_pubDates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwmI_rWXYp0L",
        "outputId": "0eb9034f-d2e0-4a38-e449-9cf31cd43c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\\n  Queries': {'Published Date': '2024-01-27T11:41:48Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.15391v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.15391v1'},\n",
              "  'Seven Failure Points When Engineering a Retrieval Augmented Generation\\n  System': {'Published Date': '2024-01-11T12:04:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.05856v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.05856v1'},\n",
              "  'RAGGED: Towards Informed Design of Retrieval Augmented Generation\\n  Systems': {'Published Date': '2024-03-14T02:26:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.09040v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.09040v1'},\n",
              "  'Observations on Building RAG Systems for Technical Documents': {'Published Date': '2024-03-31T12:01:34Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.00657v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.00657v1'},\n",
              "  'Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented\\n  Generation in Niche Domains, Exemplified by Korean Medicine': {'Published Date': '2024-01-20T14:59:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.11246v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.11246v1'},\n",
              "  'The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented\\n  Generation (RAG)': {'Published Date': '2024-02-23T18:35:15Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.16893v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.16893v1'},\n",
              "  'CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions\\n  for RAG systems': {'Published Date': '2024-04-02T17:00:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.02103v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.02103v1'},\n",
              "  'Evaluation of Retrieval-Augmented Generation: A Survey': {'Published Date': '2024-05-13T02:33:25Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.07437v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.07437v1'},\n",
              "  'FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation\\n  Research': {'Published Date': '2024-05-22T12:12:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13576v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13576v1'},\n",
              "  'Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database\\n  Filtering with LLM-Extracted Metadata': {'Published Date': '2024-06-19T04:53:48Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13213v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13213v1'},\n",
              "  \"Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by\\n  Simulating Documents in the Wild via Low-level Perturbations\": {'Published Date': '2024-04-22T07:49:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.13948v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.13948v1'},\n",
              "  'From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical\\n  Regulatory Compliance Process': {'Published Date': '2024-01-26T08:23:29Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01717v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01717v1'},\n",
              "  'Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy\\n  with Semantic Search and Hybrid Query-Based Retrievers': {'Published Date': '2024-03-22T17:13:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.07220v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.07220v1'},\n",
              "  'Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language\\n  Models for Telecommunications': {'Published Date': '2024-04-24T15:58:59Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.15939v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.15939v2'},\n",
              "  'Improving the Domain Adaptation of Retrieval Augmented Generation (RAG)\\n  Models for Open Domain Question Answering': {'Published Date': '2022-10-06T01:21:25Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2210.02627v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2210.02627v1'},\n",
              "  'Understand What LLM Needs: Dual Preference Alignment for\\n  Retrieval-Augmented Generation': {'Published Date': '2024-06-26T18:26:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.18676v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.18676v1'},\n",
              "  'Retrieval-Augmented Generation for AI-Generated Content: A Survey': {'Published Date': '2024-02-29T18:59:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.19473v6',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.19473v6'},\n",
              "  'FIT-RAG: Black-Box RAG with Factual Information and Token Reduction': {'Published Date': '2024-03-21T13:05:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.14374v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.14374v1'},\n",
              "  'Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge\\n  Gaps': {'Published Date': '2023-12-12T23:22:57Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.07796v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.07796v1'},\n",
              "  'Retrieval-Augmented Generation for Large Language Models: A Survey': {'Published Date': '2023-12-18T07:47:33Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.10997v5',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.10997v5'},\n",
              "  'Unsupervised Information Refinement Training of Large Language Models\\n  for Retrieval-Augmented Generation': {'Published Date': '2024-02-28T08:24:38Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.18150v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.18150v2'},\n",
              "  'Investigating the performance of Retrieval-Augmented Generation and\\n  fine-tuning for the development of AI-driven knowledge-based systems': {'Published Date': '2024-03-12T21:06:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.09727v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.09727v1'},\n",
              "  'From Local to Global: A Graph RAG Approach to Query-Focused\\n  Summarization': {'Published Date': '2024-04-24T18:38:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.16130v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.16130v1'},\n",
              "  'Robust Implementation of Retrieval-Augmented Generation on Edge-based\\n  Computing-in-Memory Architectures': {'Published Date': '2024-05-07T22:31:50Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.04700v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.04700v1'},\n",
              "  'The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented\\n  Generation (FutureDial-RAG)': {'Published Date': '2024-05-21T07:35:21Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13084v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13084v1'},\n",
              "  'Unveil the Duality of Retrieval-Augmented Generation: Theoretical\\n  Analysis and Practical Solution': {'Published Date': '2024-06-03T02:56:14Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00944v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00944v1'},\n",
              "  'DomainRAG: A Chinese Benchmark for Evaluating Domain-specific\\n  Retrieval-Augmented Generation': {'Published Date': '2024-06-09T05:33:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.05654v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.05654v2'},\n",
              "  'Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level\\n  RAG': {'Published Date': '2024-06-17T02:25:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11147v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11147v2'},\n",
              "  'ARES: An Automated Evaluation Framework for Retrieval-Augmented\\n  Generation Systems': {'Published Date': '2023-11-16T00:39:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.09476v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.09476v2'},\n",
              "  'Revolutionizing Retrieval-Augmented Generation with Enhanced PDF\\n  Structure Recognition': {'Published Date': '2024-01-23T09:54:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.12599v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.12599v1'},\n",
              "  'FeB4RAG: Evaluating Federated Search in the Context of Retrieval\\n  Augmented Generation': {'Published Date': '2024-02-19T07:06:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.11891v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.11891v1'},\n",
              "  'ARAGOG: Advanced RAG Output Grading': {'Published Date': '2024-04-01T10:43:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.01037v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.01037v1'},\n",
              "  'Improving Retrieval for RAG based Question Answering Models on Financial\\n  Documents': {'Published Date': '2024-03-23T00:49:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.07221v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.07221v1'},\n",
              "  'A Survey on Retrieval-Augmented Text Generation for Large Language\\n  Models': {'Published Date': '2024-04-17T01:27:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.10981v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.10981v1'},\n",
              "  'Stochastic RAG: End-to-End Retrieval-Augmented Generation through\\n  Expected Utility Maximization': {'Published Date': '2024-05-05T05:42:33Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.02816v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.02816v1'},\n",
              "  \"Don't Forget to Connect! Improving RAG with Graph-based Reranking\": {'Published Date': '2024-05-28T17:56:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.18414v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.18414v1'},\n",
              "  'RAG Does Not Work for Enterprises': {'Published Date': '2024-05-31T23:30:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.04369v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.04369v1'},\n",
              "  'A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems': {'Published Date': '2024-06-21T08:31:02Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14972v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14972v1'},\n",
              "  'CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented\\n  Generation of Large Language Models': {'Published Date': '2024-01-30T14:25:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.17043v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.17043v2'},\n",
              "  'C-RAG: Certified Generation Risks for Retrieval-Augmented Language\\n  Models': {'Published Date': '2024-02-05T16:46:16Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.03181v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.03181v4'},\n",
              "  'GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning': {'Published Date': '2024-05-30T15:14:24Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20139v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20139v1'},\n",
              "  'Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework': {'Published Date': '2024-06-20T23:20:34Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14783v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14783v1'},\n",
              "  'Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024\\n  Retrieval-Augmented Generation Track': {'Published Date': '2024-06-24T17:37:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.16828v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.16828v1'},\n",
              "  'Robust Action Governor for Uncertain Piecewise Affine Systems with\\n  Non-convex Constraints and Safe Reinforcement Learning': {'Published Date': '2022-07-17T17:31:07Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2207.08240v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2207.08240v1'},\n",
              "  'Enhancing Multilingual Information Retrieval in Mixed Human Resources\\n  Environments: A RAG Model Implementation for Multicultural Enterprise': {'Published Date': '2024-01-03T02:32:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.01511v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.01511v1'},\n",
              "  'RAG-Fusion: a New Take on Retrieval-Augmented Generation': {'Published Date': '2024-01-31T22:06:07Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.03367v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.03367v2'},\n",
              "  'Financial Report Chunking for Effective Retrieval Augmented Generation': {'Published Date': '2024-02-05T22:35:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.05131v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.05131v3'},\n",
              "  'Retrieval Augmented Generation Systems: Automatic Dataset Creation,\\n  Evaluation and Boolean Agent Setup': {'Published Date': '2024-02-26T12:56:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.00820v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.00820v1'},\n",
              "  'RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots': {'Published Date': '2024-03-02T12:19:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.01193v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.01193v3'},\n",
              "  'Boosting Conversational Question Answering with Fine-Grained\\n  Retrieval-Augmentation and Self-Check': {'Published Date': '2024-03-27T04:20:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.18243v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.18243v1'},\n",
              "  'Introducing Super RAGs in Mistral 8x7B-v1': {'Published Date': '2024-04-13T09:33:00Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.08940v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.08940v1'},\n",
              "  'InspectorRAGet: An Introspection Platform for RAG Evaluation': {'Published Date': '2024-04-26T11:51:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.17347v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.17347v1'},\n",
              "  'Towards a Search Engine for Machines: Unified Ranking for Multiple\\n  Retrieval-Augmented Large Language Models': {'Published Date': '2024-04-30T19:51:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.00175v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.00175v1'},\n",
              "  'Machine Against the RAG: Jamming Retrieval-Augmented Generation with\\n  Blocker Documents': {'Published Date': '2024-06-09T17:55:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.05870v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.05870v1'},\n",
              "  'Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG)\\n  via Pure Synthetic Data': {'Published Date': '2024-06-20T22:53:09Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14773v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14773v1'},\n",
              "  'Seeing Is Believing: Black-Box Membership Inference Attacks Against\\n  Retrieval Augmented Generation': {'Published Date': '2024-06-27T14:58:38Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.19234v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.19234v1'},\n",
              "  'DuetRAG: Collaborative Retrieval-Augmented Generation': {'Published Date': '2024-05-12T09:48:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13002v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13002v1'},\n",
              "  'Benchmarking Large Language Models in Retrieval-Augmented Generation': {'Published Date': '2023-09-04T08:28:44Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2309.01431v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2309.01431v2'},\n",
              "  'The Chronicles of RAG: The Retriever, the Chunk and the Generator': {'Published Date': '2024-01-15T18:25:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.07883v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.07883v1'},\n",
              "  'The Power of Noise: Redefining Retrieval for RAG Systems': {'Published Date': '2024-01-26T14:14:59Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.14887v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.14887v4'},\n",
              "  'Benchmarking Retrieval-Augmented Generation for Medicine': {'Published Date': '2024-02-20T17:44:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.13178v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.13178v2'},\n",
              "  'Compressing Long Context for Enhancing RAG with AMR-based Concept\\n  Distillation': {'Published Date': '2024-05-06T00:18:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.03085v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.03085v1'},\n",
              "  'Accelerating Inference of Retrieval-Augmented Generation via Sparse\\n  Context Selection': {'Published Date': '2024-05-25T11:10:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.16178v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.16178v1'},\n",
              "  'Empowering Large Language Models to Set up a Knowledge Retrieval Indexer\\n  via Self-Learning': {'Published Date': '2024-05-27T08:26:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.16933v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.16933v1'},\n",
              "  'Is My Data in Your Retrieval Database? Membership Inference Attacks\\n  Against Retrieval Augmented Generation': {'Published Date': '2024-05-30T19:46:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20446v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20446v2'},\n",
              "  'Phantom: General Trigger Attacks on Retrieval Augmented Language\\n  Generation': {'Published Date': '2024-05-30T21:19:24Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20485v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20485v1'},\n",
              "  'Multi-Head RAG: Solving Multi-Aspect Problems with LLMs': {'Published Date': '2024-06-07T16:59:38Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.05085v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.05085v1'},\n",
              "  'Development and Testing of Retrieval Augmented Generation in Large\\n  Language Models -- A Case Study Report': {'Published Date': '2024-01-29T06:49:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01733v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01733v1'},\n",
              "  'A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs': {'Published Date': '2024-04-09T07:40:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.06082v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.06082v1'},\n",
              "  'RAGAS: Automated Evaluation of Retrieval Augmented Generation': {'Published Date': '2023-09-26T19:23:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2309.15217v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2309.15217v1'},\n",
              "  'DFA-RAG: Conversational Semantic Router for Large Language Model with\\n  Definite Finite Automaton': {'Published Date': '2024-02-06T21:14:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.04411v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.04411v2'},\n",
              "  'ActiveRAG: Revealing the Treasures of Knowledge via Active Learning': {'Published Date': '2024-02-21T06:04:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.13547v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.13547v1'},\n",
              "  'Follow My Instruction and Spill the Beans: Scalable Data Extraction from\\n  Retrieval-Augmented Generation Systems': {'Published Date': '2024-02-27T19:08:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.17840v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.17840v2'},\n",
              "  'Fine Tuning vs. Retrieval Augmented Generation for Less Popular\\n  Knowledge': {'Published Date': '2024-03-03T08:07:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.01432v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.01432v2'},\n",
              "  'Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A\\n  Case Study on Domain-Specific Queries in Private Knowledge-Bases': {'Published Date': '2024-03-15T16:30:14Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.10446v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.10446v1'},\n",
              "  'RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation': {'Published Date': '2024-03-31T08:58:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.00610v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.00610v1'},\n",
              "  'CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs\\n  for Legal Question Answering': {'Published Date': '2024-04-04T21:47:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.04302v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.04302v1'},\n",
              "  'RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political\\n  Fact-Checking using Multimodal Large Language Models': {'Published Date': '2024-04-18T10:25:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.12065v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.12065v1'},\n",
              "  'Evaluating Retrieval Quality in Retrieval-Augmented Generation': {'Published Date': '2024-04-21T21:22:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.13781v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.13781v1'},\n",
              "  'Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular\\n  RAG Applications': {'Published Date': '2024-04-28T14:58:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.01585v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.01585v1'},\n",
              "  'Question-Based Retrieval using Atomic Units for Enterprise RAG': {'Published Date': '2024-05-20T20:27:00Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.12363v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.12363v1'},\n",
              "  'RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented\\n  Generation and Readability Control for Layman Summarization of Biomedical\\n  Texts': {'Published Date': '2024-05-21T20:03:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13179v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13179v4'},\n",
              "  'Automated Evaluation of Retrieval-Augmented Language Models with\\n  Task-Specific Exam Generation': {'Published Date': '2024-05-22T13:14:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13622v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13622v1'},\n",
              "  'M-RAG: Reinforcing Large Language Model Performance through\\n  Retrieval-Augmented Generation with Multiple Partitions': {'Published Date': '2024-05-26T04:03:13Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.16420v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.16420v1'},\n",
              "  'A Multi-Source Retrieval Question Answering Framework Based on RAG': {'Published Date': '2024-05-29T15:47:57Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.19207v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.19207v1'},\n",
              "  'RAG Enabled Conversations about Household Electricity Monitoring': {'Published Date': '2024-06-03T07:44:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.06566v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.06566v1'},\n",
              "  'Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and\\n  Abbreviation De-hallucination': {'Published Date': '2024-06-03T19:40:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.06575v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.06575v1'},\n",
              "  'HIRO: Hierarchical Information Retrieval Optimization': {'Published Date': '2024-06-14T12:41:07Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.09979v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.09979v1'},\n",
              "  'The Impact of Quantization on Retrieval-Augmented Generation: An\\n  Analysis of Small LLMs': {'Published Date': '2024-06-10T08:23:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.10251v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.10251v1'},\n",
              "  'Iterative Utility Judgment Framework via LLMs Inspired by Relevance in\\n  Philosophy': {'Published Date': '2024-06-17T07:52:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11290v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11290v1'},\n",
              "  'Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG\\n  Systems: A Comparative Study of Performance and Scalability': {'Published Date': '2024-06-17T11:22:25Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11424v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11424v1'},\n",
              "  'CrAM: Credibility-Aware Attention Modification in LLMs for Combating\\n  Misinformation in RAG': {'Published Date': '2024-06-17T13:01:12Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11497v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11497v2'},\n",
              "  'Model Internals-based Answer Attribution for Trustworthy\\n  Retrieval-Augmented Generation': {'Published Date': '2024-06-19T16:10:26Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13663v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13663v1'},\n",
              "  'On the Role of Long-tail Knowledge in Retrieval Augmented Large Language\\n  Models': {'Published Date': '2024-06-24T07:17:59Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.16367v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.16367v1'},\n",
              "  'Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode\\n  and Effects Analysis': {'Published Date': '2024-06-26T07:02:49Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.18114v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.18114v1'},\n",
              "  'RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on\\n  Agriculture': {'Published Date': '2024-01-16T14:44:47Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.08406v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.08406v3'},\n",
              "  'CRAG -- Comprehensive RAG Benchmark': {'Published Date': '2024-06-07T08:43:07Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.04744v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.04744v1'},\n",
              "  'RAGTruth: A Hallucination Corpus for Developing Trustworthy\\n  Retrieval-Augmented Language Models': {'Published Date': '2023-12-31T04:43:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.00396v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.00396v2'},\n",
              "  'Prompt Perturbation in Retrieval-Augmented Generation based Large\\n  Language Models': {'Published Date': '2024-02-11T12:25:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.07179v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.07179v2'},\n",
              "  'T-RAG: Lessons from the LLM Trenches': {'Published Date': '2024-02-12T08:45:08Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.07483v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.07483v2'},\n",
              "  'REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\\n  Question Answering': {'Published Date': '2024-02-27T13:22:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.17497v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.17497v1'},\n",
              "  'CONFLARE: CONFormal LArge language model REtrieval': {'Published Date': '2024-04-04T02:58:21Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.04287v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.04287v1'},\n",
              "  'RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation': {'Published Date': '2024-04-18T18:32:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.12457v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.12457v2'},\n",
              "  'ERATTA: Extreme RAG for Table To Answers with Large Language Models': {'Published Date': '2024-05-07T02:49:59Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.03963v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.03963v2'},\n",
              "  'Leveraging Lecture Content for Improved Feedback: Explorations with\\n  GPT-4 and Retrieval Augmented Generation': {'Published Date': '2024-05-05T18:32:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.06681v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.06681v1'},\n",
              "  'CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control': {'Published Date': '2024-05-29T03:17:16Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.18727v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.18727v1'},\n",
              "  'Clustered Retrieved Augmented Generation (CRAG)': {'Published Date': '2024-05-24T16:36:47Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00029v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00029v1'},\n",
              "  'Toward Conversational Agents with Context and Time Sensitive Long-term\\n  Memory': {'Published Date': '2024-05-29T18:19:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00057v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00057v2'},\n",
              "  'DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented\\n  Generation for Question-Answering': {'Published Date': '2024-06-11T15:15:33Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.07348v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.07348v3'},\n",
              "  'Reinforcement Learning for Optimizing RAG for Domain Chatbots': {'Published Date': '2024-01-10T02:57:20Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.06800v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.06800v1'},\n",
              "  'Development and Testing of a Novel Large Language Model-Based Clinical\\n  Decision Support Systems for Medication Safety in 12 Clinical Specialties': {'Published Date': '2024-01-29T16:03:29Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01741v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01741v2'},\n",
              "  'iRAG: An Incremental Retrieval Augmented Generation System for Videos': {'Published Date': '2024-04-18T16:38:02Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.12309v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.12309v1'},\n",
              "  'A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language\\n  Models': {'Published Date': '2024-05-10T02:48:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.06211v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.06211v3'},\n",
              "  'BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of\\n  Large Language Models': {'Published Date': '2024-06-03T02:25:33Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00083v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00083v2'},\n",
              "  'Retrieval Augmented Generation and Representative Vector Summarization\\n  for large unstructured textual data in Medical Education': {'Published Date': '2023-08-01T12:04:50Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2308.00479v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2308.00479v1'},\n",
              "  'Dynamic Contexts for Generating Suggestion Questions in RAG Based\\n  Conversational Systems': {'Published Date': '2024-03-18T02:01:58Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.11413v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.11413v1'},\n",
              "  'RAM: Towards an Ever-Improving Memory System by Learning from\\n  Communications': {'Published Date': '2024-04-18T09:58:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.12045v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.12045v1'},\n",
              "  'Control Token with Dense Passage Retrieval': {'Published Date': '2024-05-13T09:17:19Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13008v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13008v1'},\n",
              "  'Retrieval-Augmented Generation for Generative Artificial Intelligence in\\n  Medicine': {'Published Date': '2024-06-18T09:53:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12449v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12449v1'},\n",
              "  'Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval\\n  Augmented Generation Models for Open Book Question-Answering': {'Published Date': '2023-07-12T04:44:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2307.05915v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2307.05915v2'},\n",
              "  'A Study on the Implementation of Generative AI Services Using an\\n  Enterprise Data-Based LLM Application Architecture': {'Published Date': '2023-09-03T07:03:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2309.01105v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2309.01105v2'},\n",
              "  'Retrieval-augmented Generation to Improve Math Question-Answering:\\n  Trade-offs Between Groundedness and Human Preference': {'Published Date': '2023-10-04T22:09:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.03184v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.03184v2'},\n",
              "  'Self-RAG: Learning to Retrieve, Generate, and Critique through\\n  Self-Reflection': {'Published Date': '2023-10-17T18:18:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.11511v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.11511v1'},\n",
              "  'GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval': {'Published Date': '2023-10-31T03:52:08Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.20158v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.20158v1'},\n",
              "  'ChatQA: Surpassing GPT-4 on Conversational QA and RAG': {'Published Date': '2024-01-18T18:59:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.10225v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.10225v4'},\n",
              "  'RAFT: Adapting Language Model to Domain Specific RAG': {'Published Date': '2024-03-15T09:26:02Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.10131v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.10131v2'},\n",
              "  'Towards a RAG-based Summarization Agent for the Electron-Ion Collider': {'Published Date': '2024-03-23T05:32:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.15729v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.15729v3'},\n",
              "  'Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning:\\n  A Comparative Study': {'Published Date': '2024-04-17T23:00:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.11792v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.11792v2'},\n",
              "  'GRAG: Graph Retrieval-Augmented Generation': {'Published Date': '2024-05-26T10:11:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.16506v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.16506v1'},\n",
              "  'Retrieval Meets Reasoning: Even High-school Textbook Knowledge Benefits\\n  Multimodal Reasoning': {'Published Date': '2024-05-31T14:23:49Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20834v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20834v1'},\n",
              "  'Leveraging Large Language Models for Web Scraping': {'Published Date': '2024-06-12T14:15:15Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.08246v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.08246v1'},\n",
              "  'Refiner: Restructure Retrieval Content Efficiently to Advance\\n  Question-Answering Capabilities': {'Published Date': '2024-06-17T09:25:10Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11357v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11357v2'},\n",
              "  'InstructRAG: Instructing Retrieval-Augmented Generation with Explicit\\n  Denoising': {'Published Date': '2024-06-19T15:25:29Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13629v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13629v1'},\n",
              "  'Robust affine point matching via quadratic assignment on Grassmannians': {'Published Date': '2023-03-05T15:27:24Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2303.02698v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2303.02698v4'},\n",
              "  'A Resource-efficient FIR Filter Design Based on an RAG Improved\\n  Algorithm': {'Published Date': '2023-10-02T05:58:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.00912v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.00912v2'},\n",
              "  'Context Tuning for Retrieval Augmented Generation': {'Published Date': '2023-12-09T23:33:16Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.05708v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.05708v1'},\n",
              "  'PaperQA: Retrieval-Augmented Generative Agent for Scientific Research': {'Published Date': '2023-12-08T18:50:20Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.07559v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.07559v2'},\n",
              "  'Beyond Extraction: Contextualising Tabular Data for Efficient\\n  Summarisation by Language Models': {'Published Date': '2024-01-04T16:16:14Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.02333v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.02333v3'},\n",
              "  'Bridging the Preference Gap between Retrievers and LLMs': {'Published Date': '2024-01-13T02:20:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.06954v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.06954v2'},\n",
              "  'Corrective Retrieval Augmented Generation': {'Published Date': '2024-01-29T04:36:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.15884v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.15884v2'},\n",
              "  'Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for\\n  Semantic Representations': {'Published Date': '2024-02-05T14:36:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.03053v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.03053v1'},\n",
              "  'Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning': {'Published Date': '2024-02-13T12:40:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.08416v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.08416v1'},\n",
              "  'From RAGs to riches: Using large language models to write documents for\\n  clinical trials': {'Published Date': '2024-02-26T08:59:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.16406v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.16406v1'},\n",
              "  'Enhancing Retrieval Processes for Language Generation with Augmented\\n  Queries': {'Published Date': '2024-02-06T13:19:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.16874v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.16874v1'},\n",
              "  'Superposition Prompting: Improving and Accelerating Retrieval-Augmented\\n  Generation': {'Published Date': '2024-04-10T11:03:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.06910v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.06910v1'},\n",
              "  'Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented\\n  Generation': {'Published Date': '2024-04-19T13:27:38Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.12879v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.12879v1'},\n",
              "  'IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection &\\n  Correction Task On the Shoulders of Medical Agents': {'Published Date': '2024-04-23T20:00:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.15488v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.15488v1'},\n",
              "  'Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered\\n  Applications': {'Published Date': '2024-04-26T07:11:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.17196v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.17196v1'},\n",
              "  'RaFe: Ranking Feedback Improves Query Rewriting for RAG': {'Published Date': '2024-05-23T11:00:19Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.14431v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.14431v1'},\n",
              "  'ATM: Adversarial Tuning Multi-agent System Makes a Robust\\n  Retrieval-Augmented Generator': {'Published Date': '2024-05-28T12:18:50Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.18111v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.18111v2'},\n",
              "  'One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for\\n  Retrieval-Augmented Large Language Models': {'Published Date': '2024-05-30T03:44:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.19670v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.19670v3'},\n",
              "  'SoccerRAG: Multimodal Soccer Information Retrieval via Natural Queries': {'Published Date': '2024-06-03T12:39:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.01273v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.01273v1'},\n",
              "  'Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis\\n  with Context-Aware Contrastive Language-Audio Pretraining': {'Published Date': '2024-06-06T03:17:44Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.03714v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.03714v1'},\n",
              "  'RE-RAG: Improving Open-Domain QA Performance and Interpretability with\\n  Relevance Estimator in Retrieval-Augmented Generation': {'Published Date': '2024-06-09T14:11:19Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.05794v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.05794v2'},\n",
              "  'TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for\\n  Retrieval-Augmented Generation': {'Published Date': '2024-06-17T12:23:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11460v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11460v1'},\n",
              "  'From RAGs to rich parameters: Probing how language models utilize\\n  external knowledge over parametric information for factual queries': {'Published Date': '2024-06-18T17:46:08Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12824v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12824v1'},\n",
              "  'DIRAS: Efficient LLM-Assisted Annotation of Document Relevance in\\n  Retrieval Augmented Generation': {'Published Date': '2024-06-20T10:04:09Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14162v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14162v1'},\n",
              "  'Biomedical knowledge graph-optimized prompt generation for large\\n  language models': {'Published Date': '2023-11-29T03:07:00Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.17330v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.17330v2'},\n",
              "  'PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented\\n  Generation of Large Language Models': {'Published Date': '2024-02-12T18:28:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.07867v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.07867v1'},\n",
              "  'First bromine doped cryogenic implosion at the National Ignition\\n  Facility': {'Published Date': '2023-07-07T17:35:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2307.03730v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2307.03730v1'},\n",
              "  'LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation': {'Published Date': '2023-10-08T01:43:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.04963v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.04963v3'},\n",
              "  'Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented\\n  Generation and Soft-Prompting for Non-Specialist LLM Users': {'Published Date': '2023-11-10T07:13:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.05903v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.05903v2'},\n",
              "  'Advancing TTP Analysis: Harnessing the Power of Encoder-Only and\\n  Decoder-Only Language Models with Retrieval Augmented Generation': {'Published Date': '2023-12-30T16:56:24Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.00280v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.00280v2'},\n",
              "  'CorpusLM: Towards a Unified Language Model on Corpus for\\n  Knowledge-Intensive Tasks': {'Published Date': '2024-02-02T06:44:22Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01176v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01176v2'},\n",
              "  'Grounding Language Model with Chunking-Free In-Context Retrieval': {'Published Date': '2024-02-15T07:22:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.09760v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.09760v1'},\n",
              "  'Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge': {'Published Date': '2024-02-19T18:31:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.12352v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.12352v1'},\n",
              "  'JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\\n  and Professional Question Answering Capability': {'Published Date': '2024-02-27T21:01:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.17887v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.17887v3'},\n",
              "  'Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based\\n  Search Engines': {'Published Date': '2024-02-29T18:20:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.19421v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.19421v1'},\n",
              "  'Repoformer: Selective Retrieval for Repository-Level Code Completion': {'Published Date': '2024-03-15T06:59:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.10059v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.10059v2'},\n",
              "  'CPR: Retrieval Augmented Generation for Copyright Protection': {'Published Date': '2024-03-27T18:09:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.18920v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.18920v1'},\n",
              "  'Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation': {'Published Date': '2024-04-10T07:56:26Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.06809v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.06809v2'},\n",
              "  'LLMs Know What They Need: Leveraging a Missing Information Guided\\n  Framework to Empower Retrieval-Augmented Generation': {'Published Date': '2024-04-22T09:56:59Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.14043v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.14043v1'},\n",
              "  'Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented\\n  Large Language Models': {'Published Date': '2024-04-27T13:11:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.17897v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.17897v1'},\n",
              "  'Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf\\n  Disease Remediation': {'Published Date': '2024-05-02T14:19:25Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.01310v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.01310v1'},\n",
              "  'ERAGent: Enhancing Retrieval-Augmented Language Models with Improved\\n  Accuracy, Efficiency, and Personalization': {'Published Date': '2024-05-06T04:42:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.06683v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.06683v1'},\n",
              "  'IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning\\n  Inner Monologues': {'Published Date': '2024-05-15T12:41:20Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13021v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13021v1'},\n",
              "  'Augmenting Textual Generation via Topology Aware Retrieval': {'Published Date': '2024-05-27T19:02:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.17602v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.17602v1'},\n",
              "  'Enhancing Noise Robustness of Retrieval-Augmented Language Models with\\n  Adaptive Adversarial Training': {'Published Date': '2024-05-31T16:24:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20978v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20978v1'},\n",
              "  'AMGPT: a Large Language Model for Contextual Querying in Additive\\n  Manufacturing': {'Published Date': '2024-05-24T20:03:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00031v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00031v1'},\n",
              "  'Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large\\n  Language Models': {'Published Date': '2024-06-17T04:35:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11201v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11201v1'},\n",
              "  'Enhancing Biomedical Knowledge Retrieval-Augmented Generation with\\n  Self-Rewarding Tree Search and Proximal Policy Optimization': {'Published Date': '2024-06-17T06:48:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11258v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11258v1'},\n",
              "  'R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval\\n  Augmented Large Language Models': {'Published Date': '2024-06-17T15:59:49Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11681v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11681v1'},\n",
              "  'Unified Active Retrieval for Retrieval Augmented Generation': {'Published Date': '2024-06-18T12:09:02Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12534v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12534v3'},\n",
              "  'WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge\\n  Conflicts from Wikipedia': {'Published Date': '2024-06-19T20:13:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13805v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13805v1'},\n",
              "  'Development of a Reliable and Accessible Caregiving Language Model\\n  (CaLM)': {'Published Date': '2024-03-11T16:12:34Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.06857v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.06857v1'},\n",
              "  'Structure Learning for Hybrid Bayesian Networks': {'Published Date': '2022-06-03T01:18:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2206.01356v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2206.01356v2'},\n",
              "  'Formation of asymmetric arms in barred galaxies': {'Published Date': '2023-01-26T19:58:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2301.11385v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2301.11385v1'},\n",
              "  'TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal\\n  Prediction': {'Published Date': '2023-07-07T02:42:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2307.04642v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2307.04642v2'},\n",
              "  'Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for\\n  Retrieval Augmented Generation': {'Published Date': '2023-11-07T18:03:23Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.04177v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.04177v1'},\n",
              "  'Minimizing Factual Inconsistency and Hallucination in Large Language\\n  Models': {'Published Date': '2023-11-23T09:58:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.13878v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.13878v1'},\n",
              "  'Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs': {'Published Date': '2023-12-10T16:52:00Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.05934v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.05934v3'},\n",
              "  'Enhancing Large Language Model Performance To Answer Questions and\\n  Extract Information More Accurately': {'Published Date': '2024-01-27T00:18:07Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01722v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01722v1'},\n",
              "  'HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents\\n  QA': {'Published Date': '2024-02-01T02:24:15Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01767v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01767v1'},\n",
              "  'Enhancing Textbook Question Answering Task with Large Language Models\\n  and Retrieval Augmented Generation': {'Published Date': '2024-02-05T11:58:56Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.05128v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.05128v2'},\n",
              "  'Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning': {'Published Date': '2024-02-19T14:33:24Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.12177v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.12177v4'},\n",
              "  'Assessing generalization capability of text ranking models in Polish': {'Published Date': '2024-02-22T06:21:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.14318v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.14318v1'},\n",
              "  'ESE: Espresso Sentence Embeddings': {'Published Date': '2024-02-22T18:35:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.14776v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.14776v2'},\n",
              "  'Federated Recommendation via Hybrid Retrieval Augmented Generation': {'Published Date': '2024-03-07T06:38:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.04256v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.04256v1'},\n",
              "  'PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System\\n  Co-design': {'Published Date': '2024-03-08T21:09:20Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.05676v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.05676v1'},\n",
              "  'Retrieval augmented text-to-SQL generation for epidemiological question\\n  answering using electronic health records': {'Published Date': '2024-03-14T09:45:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.09226v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.09226v2'},\n",
              "  'JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented\\n  Fine-Tuning': {'Published Date': '2024-03-17T23:02:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.11366v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.11366v2'},\n",
              "  'LexDrafter: Terminology Drafting for Legislative Documents using\\n  Retrieval Augmented Generation': {'Published Date': '2024-03-24T21:02:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.16295v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.16295v1'},\n",
              "  'Evaluation of Semantic Search and its Role in\\n  Retrieved-Augmented-Generation (RAG) for Arabic Language': {'Published Date': '2024-03-27T08:42:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.18350v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.18350v2'},\n",
              "  'Towards a Robust Retrieval-Based Summarization System': {'Published Date': '2024-03-29T00:14:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.19889v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.19889v1'},\n",
              "  'Reducing hallucination in structured outputs via Retrieval-Augmented\\n  Generation': {'Published Date': '2024-04-12T01:42:09Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.08189v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.08189v1'},\n",
              "  'Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for\\n  Text-to-SQL': {'Published Date': '2024-04-19T00:48:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.12560v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.12560v1'},\n",
              "  'Studying Large Language Model Behaviors Under Realistic Knowledge\\n  Conflicts': {'Published Date': '2024-04-24T17:59:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.16032v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.16032v1'},\n",
              "  'GRAMMAR: Grounded and Modular Methodology for Assessment of\\n  Domain-Specific Retrieval-Augmented Language Model': {'Published Date': '2024-04-30T03:29:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.19232v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.19232v4'},\n",
              "  'Comparative Analysis of Retrieval Systems in the Real World': {'Published Date': '2024-05-03T12:30:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.02048v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.02048v1'},\n",
              "  'From Questions to Insightful Answers: Building an Informed Chatbot for\\n  University Resources': {'Published Date': '2024-05-13T19:05:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.08120v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.08120v1'},\n",
              "  'KG-RAG: Bridging the Gap Between Knowledge and Creativity': {'Published Date': '2024-05-20T14:03:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.12035v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.12035v1'},\n",
              "  'Certifiably Robust RAG against Retrieval Corruption': {'Published Date': '2024-05-24T13:44:25Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.15556v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.15556v1'},\n",
              "  'RAGSys: Item-Cold-Start Recommender as RAG System': {'Published Date': '2024-05-27T18:40:49Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.17587v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.17587v1'},\n",
              "  'Video Enriched Retrieval Augmented Generation Using Aligned Video\\n  Captions': {'Published Date': '2024-05-27T23:39:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.17706v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.17706v1'},\n",
              "  'Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical\\n  Machine Reading Comprehension': {'Published Date': '2024-05-29T01:12:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.18682v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.18682v1'},\n",
              "  'Two-layer retrieval augmented generation framework for low-resource\\n  medical question-answering: proof of concept using Reddit data': {'Published Date': '2024-05-29T20:56:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.19519v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.19519v1'},\n",
              "  'QUB-Cirdan at \"Discharge Me!\": Zero shot discharge letter generation by\\n  open-source LLM': {'Published Date': '2024-05-27T17:55:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00041v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00041v2'},\n",
              "  'Mix-of-Granularity: Optimize the Chunking Granularity for\\n  Retrieval-Augmented Generation': {'Published Date': '2024-06-01T14:45:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00456v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00456v1'},\n",
              "  'Luna: An Evaluation Foundation Model to Catch Language Model\\n  Hallucinations with High Accuracy and Low Cost': {'Published Date': '2024-06-03T04:14:21Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00975v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00975v2'},\n",
              "  'Demo: Soccer Information Retrieval via Natural Queries using SoccerRAG': {'Published Date': '2024-06-03T12:48:38Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.01280v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.01280v1'},\n",
              "  'TSpec-LLM: An Open-source Dataset for LLM Understanding of 3GPP\\n  Specifications': {'Published Date': '2024-06-03T20:18:56Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.01768v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.01768v1'},\n",
              "  'Scholarly Question Answering using Large Language Models in the\\n  NFDI4DataScience Gateway': {'Published Date': '2024-06-11T13:36:19Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.07257v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.07257v1'},\n",
              "  'Ad Auctions for LLMs via Retrieval Augmented Generation': {'Published Date': '2024-06-12T22:05:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.09459v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.09459v1'},\n",
              "  'A Lightweight Framework for Adaptive Retrieval In Code Completion With\\n  Critique Model': {'Published Date': '2024-06-11T02:37:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.10263v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.10263v1'},\n",
              "  'Satyrn: A Platform for Analytics Augmented Generation': {'Published Date': '2024-06-17T20:14:16Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12069v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12069v1'},\n",
              "  'Intermediate Distillation: Data-Efficient Distillation from Black-Box\\n  LLMs for Information Retrieval': {'Published Date': '2024-06-18T00:41:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12169v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12169v1'},\n",
              "  'PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large\\n  Language Models as Decision Makers': {'Published Date': '2024-06-18T09:25:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12430v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12430v1'},\n",
              "  'R^2AG: Incorporating Retrieval Information into Retrieval Augmented\\n  Generation': {'Published Date': '2024-06-19T06:19:48Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13249v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13249v1'},\n",
              "  'Augmenting Query and Passage for Retrieval-Augmented Generation using\\n  LLMs for Open-Domain Question Answering': {'Published Date': '2024-06-20T12:59:27Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14277v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14277v1'},\n",
              "  'UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world\\n  Document Analysis': {'Published Date': '2024-06-21T14:29:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.15187v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.15187v1'},\n",
              "  'Evaluating Quality of Answers for Retrieval-Augmented Generation: A\\n  Strong LLM Is All You Need': {'Published Date': '2024-06-26T04:49:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.18064v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.18064v1'},\n",
              "  'RAVEN: Multitask Retrieval Augmented Vision-Language Learning': {'Published Date': '2024-06-27T13:08:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.19150v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.19150v1'},\n",
              "  'SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented\\n  Generation': {'Published Date': '2024-06-27T14:38:33Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.19215v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.19215v1'},\n",
              "  'AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for\\n  Retrieval-Augmented Generation': {'Published Date': '2024-06-27T15:18:21Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.19251v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.19251v1'},\n",
              "  'Spatio-Temporal driven Attention Graph Neural Network with Block\\n  Adjacency matrix (STAG-NN-BA)': {'Published Date': '2023-03-25T01:26:50Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2303.14322v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2303.14322v1'},\n",
              "  'REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific\\n  Sentences using Public and Proprietary LLMs': {'Published Date': '2024-05-03T16:38:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.02228v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.02228v2'},\n",
              "  'Ecce Signum: An R Package for Multivariate Signal Extraction and Time\\n  Series Analysis': {'Published Date': '2022-01-06T17:28:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2201.02148v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2201.02148v1'},\n",
              "  \"Juggler's friezes\": {'Published Date': '2022-08-18T18:57:23Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2208.09025v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2208.09025v1'},\n",
              "  'Towards Comprehensive Vietnamese Retrieval-Augmented Generation and\\n  Large Language Models': {'Published Date': '2024-03-03T21:24:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.01616v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.01616v2'},\n",
              "  'Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An\\n  Experimental Study and Quality Assessment Methods': {'Published Date': '2024-06-12T18:38:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.08582v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.08582v1'},\n",
              "  'Debate as Optimization: Adaptive Conformal Prediction and Diverse\\n  Retrieval for Event Extraction': {'Published Date': '2024-06-18T01:53:49Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12197v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12197v1'},\n",
              "  'Adaptive Selection for Homogeneous Tools: An Instantiation in the RAG\\n  Scenario': {'Published Date': '2024-06-18T09:24:09Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12429v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12429v1'},\n",
              "  'Towards Retrieval Augmented Generation over Large Video Libraries': {'Published Date': '2024-06-21T07:52:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14938v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14938v1'},\n",
              "  'Bayesian inference for link travel time correlation of a bus route': {'Published Date': '2022-02-19T01:00:25Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2202.09485v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2202.09485v1'},\n",
              "  'DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation': {'Published Date': '2023-05-31T12:27:58Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2305.19787v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2305.19787v2'},\n",
              "  \"Intuitive or Dependent? Investigating LLMs' Behavior Style to\\n  Conflicting Prompts\": {'Published Date': '2023-09-29T17:26:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2309.17415v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2309.17415v3'},\n",
              "  'GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using\\n  Large Language Models': {'Published Date': '2023-10-10T00:39:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.06225v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.06225v2'},\n",
              "  'Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large\\n  Language Model': {'Published Date': '2023-10-13T13:17:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.09089v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.09089v2'},\n",
              "  'FABULA: Intelligence Report Generation Using Retrieval-Augmented\\n  Narrative Construction': {'Published Date': '2023-10-20T22:47:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.13848v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.13848v2'},\n",
              "  'AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using\\n  Open-Source LLMs': {'Published Date': '2023-11-05T21:43:02Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.02775v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.02775v3'},\n",
              "  'Chemist-X: Large Language Model-empowered Agent for Reaction Condition\\n  Recommendation in Chemical Synthesis': {'Published Date': '2023-11-16T01:21:33Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.10776v5',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.10776v5'},\n",
              "  'IAG: Induction-Augmented Generation Framework for Answering Reasoning\\n  Questions': {'Published Date': '2023-11-30T09:48:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.18397v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.18397v1'},\n",
              "  \"NoMIRACL: Knowing When You Don't Know for Robust Multilingual\\n  Retrieval-Augmented Generation\": {'Published Date': '2023-12-18T17:18:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.11361v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.11361v2'},\n",
              "  'Graph database while computationally efficient filters out quickly the\\n  ESG integrated equities in investment management': {'Published Date': '2024-01-15T05:38:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.07483v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.07483v1'},\n",
              "  'UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\\n  Personalized Dialogue Systems': {'Published Date': '2024-01-24T06:50:20Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.13256v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.13256v1'},\n",
              "  'REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records\\n  Analysis via Large Language Models': {'Published Date': '2024-02-10T18:27:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.07016v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.07016v1'},\n",
              "  'G-Retriever: Retrieval-Augmented Generation for Textual Graph\\n  Understanding and Question Answering': {'Published Date': '2024-02-12T13:13:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.07630v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.07630v3'},\n",
              "  'RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented\\n  In-Context Learning in Multi-Modal Large Language Model': {'Published Date': '2024-02-16T16:57:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.10828v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.10828v2'},\n",
              "  'Improving Assessment of Tutoring Practices using Retrieval-Augmented\\n  Generation': {'Published Date': '2024-02-04T20:42:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.14594v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.14594v1'},\n",
              "  'A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI\\n  Judge': {'Published Date': '2024-02-26T23:37:59Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.17081v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.17081v1'},\n",
              "  'Evaluating Very Long-Term Conversational Memory of LLM Agents': {'Published Date': '2024-02-27T18:42:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.17753v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.17753v1'},\n",
              "  'DRAGIN: Dynamic Retrieval Augmented Generation based on the Information\\n  Needs of Large Language Models': {'Published Date': '2024-03-15T07:45:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.10081v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.10081v2'},\n",
              "  'Are Large Language Models Good at Utility Judgments?': {'Published Date': '2024-03-28T08:27:44Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.19216v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.19216v2'},\n",
              "  'Reusable Architecture Growth for Continual Stereo Matching': {'Published Date': '2024-03-30T13:24:58Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.00360v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.00360v1'},\n",
              "  'MedExpQA: Multilingual Benchmarking of Large Language Models for Medical\\n  Question Answering': {'Published Date': '2024-04-08T15:03:57Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.05590v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.05590v1'},\n",
              "  'Generative Information Retrieval Evaluation': {'Published Date': '2024-04-11T21:48:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.08137v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.08137v2'},\n",
              "  'Spiral of Silence: How is Large Language Model Killing Information\\n  Retrieval? -- A Case Study on Open Domain Question Answering': {'Published Date': '2024-04-16T12:10:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.10496v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.10496v4'},\n",
              "  'Generating Test Scenarios from NL Requirements using Retrieval-Augmented\\n  LLMs: An Industrial Study': {'Published Date': '2024-04-19T10:27:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.12772v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.12772v1'},\n",
              "  'Investigating the prompt leakage effect and black-box defenses for\\n  multi-turn LLM interactions': {'Published Date': '2024-04-24T23:39:58Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.16251v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.16251v2'},\n",
              "  'Retrieval-Augmented Generation with Knowledge Graphs for Customer\\n  Service Question Answering': {'Published Date': '2024-04-26T23:05:20Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.17723v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.17723v2'},\n",
              "  'RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural\\n  Language Processing': {'Published Date': '2024-04-30T13:14:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.19543v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.19543v1'},\n",
              "  'Automatic Retrieval-augmented Generation of 6G Network Specifications\\n  for Use Cases': {'Published Date': '2024-05-06T02:35:10Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.03122v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.03122v1'},\n",
              "  'A Method for Parsing and Vectorization of Semi-structured Data used in\\n  Retrieval Augmented Generation': {'Published Date': '2024-05-07T04:04:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.03989v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.03989v2'},\n",
              "  'Towards Accurate and Efficient Document Analytics with Large Language\\n  Models': {'Published Date': '2024-05-07T21:14:38Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.04674v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.04674v1'},\n",
              "  \"Evaluating Students' Open-ended Written Responses with LLMs: Using the\\n  RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large\": {'Published Date': '2024-05-08T22:23:58Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.05444v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.05444v1'},\n",
              "  'HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\\n  Models': {'Published Date': '2024-05-23T17:47:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.14831v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.14831v1'},\n",
              "  'EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling': {'Published Date': '2024-05-27T10:53:15Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00036v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00036v1'},\n",
              "  'Chain of Agents: Large Language Models Collaborating on Long-Context\\n  Tasks': {'Published Date': '2024-06-04T23:36:08Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.02818v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.02818v1'},\n",
              "  'Corpus Poisoning via Approximate Greedy Gradient Descent': {'Published Date': '2024-06-07T17:02:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.05087v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.05087v1'},\n",
              "  'Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt\\n  LLMs for Dialogue': {'Published Date': '2024-06-10T15:52:49Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.06399v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.06399v1'},\n",
              "  'STALL+: Boosting LLM-based Repository-level Code Completion with Static\\n  Analysis': {'Published Date': '2024-06-14T13:28:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.10018v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.10018v1'},\n",
              "  'Beyond Words: On Large Language Models Actionability in Mission-Critical\\n  Risk Analysis': {'Published Date': '2024-06-11T19:20:27Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.10273v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.10273v1'},\n",
              "  'RichRAG: Crafting Rich Responses for Multi-faceted Queries in\\n  Retrieval-Augmented Generation': {'Published Date': '2024-06-18T12:52:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12566v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12566v2'},\n",
              "  'Towards Unlocking Insights from Logbooks Using AI': {'Published Date': '2024-05-25T13:38:15Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12881v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12881v1'},\n",
              "  'Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?': {'Published Date': '2024-06-19T00:28:58Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13121v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13121v1'},\n",
              "  'FoRAG: Factuality-optimized Retrieval Augmented Generation for\\n  Web-enhanced Long-form Question Answering': {'Published Date': '2024-06-19T19:06:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13779v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13779v1'},\n",
              "  'CodeRAG-Bench: Can Retrieval Augment Code Generation?': {'Published Date': '2024-06-20T16:59:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14497v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14497v1'},\n",
              "  'Relation Extraction with Fine-Tuned Large Language Models in Retrieval\\n  Augmented Generation Frameworks': {'Published Date': '2024-06-20T21:27:57Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14745v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14745v2'},\n",
              "  'LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs': {'Published Date': '2024-06-21T17:23:21Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.15319v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.15319v1'},\n",
              "  'Multi-step Knowledge Retrieval and Inference over Unstructured Data': {'Published Date': '2024-06-26T00:00:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.17987v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.17987v1'},\n",
              "  'Poisoned LangChain: Jailbreak LLMs by LangChain': {'Published Date': '2024-06-26T07:21:02Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.18122v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.18122v1'},\n",
              "  'A RAG-based Question Answering System Proposal for Understanding Islam:\\n  MufassirQAS LLM': {'Published Date': '2024-01-27T10:50:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.15378v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.15378v4'},\n",
              "  'Large Multi-Modal Models (LMMs) as Universal Foundation Models for\\n  AI-Native Wireless Systems': {'Published Date': '2024-01-30T00:21:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01748v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01748v2'},\n",
              "  'FACTOID: FACtual enTailment fOr hallucInation Detection': {'Published Date': '2024-03-28T03:09:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.19113v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.19113v1'},\n",
              "  \"Integrating A.I. in Higher Education: Protocol for a Pilot Study with\\n  'SAMCares: An Adaptive Learning Hub'\": {'Published Date': '2024-05-01T05:39:07Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.00330v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.00330v1'},\n",
              "  'RAG-based Explainable Prediction of Road Users Behaviors for Automated\\n  Driving using Knowledge Graphs and Large Language Models': {'Published Date': '2024-05-01T11:06:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.00449v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.00449v1'},\n",
              "  'Hallucination-Free? Assessing the Reliability of Leading AI Legal\\n  Research Tools': {'Published Date': '2024-05-30T17:56:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20362v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20362v1'},\n",
              "  'The Fairness of Machine Learning in Insurance: New Rags for an Old Man?': {'Published Date': '2022-05-17T06:22:00Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2205.08112v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2205.08112v1'},\n",
              "  'An Efficient, Scalable IO Framework for Sparse Data: larcv3': {'Published Date': '2022-09-08T20:24:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2209.04023v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2209.04023v1'},\n",
              "  'PARAFAC2-based Coupled Matrix and Tensor Factorizations': {'Published Date': '2022-10-24T09:20:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2210.13054v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2210.13054v1'},\n",
              "  'Multi-class Brain Tumor Segmentation using Graph Attention Network': {'Published Date': '2023-02-11T04:30:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2302.05598v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2302.05598v1'},\n",
              "  'Huatuo-26M, a Large-scale Chinese Medical QA Dataset': {'Published Date': '2023-05-02T15:33:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2305.01526v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2305.01526v1'},\n",
              "  'Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT\\n  models': {'Published Date': '2023-05-05T16:28:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2305.03660v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2305.03660v1'},\n",
              "  'Deep Equilibrium Object Detection': {'Published Date': '2023-08-18T13:56:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2308.09564v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2308.09564v1'},\n",
              "  'Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs': {'Published Date': '2023-10-05T18:01:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.03812v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.03812v1'},\n",
              "  'Making LLMs Worth Every Penny: Resource-Limited Text Classification in\\n  Banking': {'Published Date': '2023-11-10T15:10:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.06102v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.06102v1'},\n",
              "  'Deficiency of Large Language Models in Finance: An Empirical Examination\\n  of Hallucination': {'Published Date': '2023-11-27T05:27:13Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.15548v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.15548v1'},\n",
              "  'Novel Preprocessing Technique for Data Embedding in Engineering Code\\n  Generation Using Large Language Model': {'Published Date': '2023-11-27T19:17:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.16267v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.16267v2'},\n",
              "  'RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language\\n  Models': {'Published Date': '2023-11-28T06:18:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.16543v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.16543v3'},\n",
              "  'How to Build an AI Tutor that Can Adapt to Any Course and Provide\\n  Accurate Answers Using Large Language Model and Retrieval-Augmented\\n  Generation': {'Published Date': '2023-11-29T15:02:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.17696v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.17696v3'},\n",
              "  'Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP': {'Published Date': '2023-12-19T18:56:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.12430v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.12430v3'},\n",
              "  'Question-Answering Based Summarization of Electronic Health Records\\n  using Retrieval Augmented Generation': {'Published Date': '2024-01-03T00:09:34Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.01469v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.01469v1'},\n",
              "  'Code-Based English Models Surprising Performance on Chinese QA Pair\\n  Extraction Task': {'Published Date': '2024-01-16T02:11:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.10286v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.10286v3'},\n",
              "  'Explaining Autonomy: Enhancing Human-Robot Interaction through\\n  Explanation Generation with Large Language Models': {'Published Date': '2024-02-06T18:01:29Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.04206v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.04206v1'},\n",
              "  'Generative Representational Instruction Tuning': {'Published Date': '2024-02-15T12:12:19Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.09906v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.09906v2'},\n",
              "  'In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs\\n  Miss': {'Published Date': '2024-02-16T16:15:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.10790v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.10790v2'},\n",
              "  'Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?': {'Published Date': '2024-02-16T19:28:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.11035v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.11035v2'},\n",
              "  'GenDec: A robust generative Question-decomposition method for Multi-hop\\n  reasoning': {'Published Date': '2024-02-17T02:21:44Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.11166v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.11166v1'},\n",
              "  'What Evidence Do Language Models Find Convincing?': {'Published Date': '2024-02-19T02:15:34Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.11782v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.11782v1'},\n",
              "  'ARKS: Active Retrieval in Knowledge Soup for Code Generation': {'Published Date': '2024-02-19T17:37:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.12317v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.12317v1'},\n",
              "  \"Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware\\n  Classification\": {'Published Date': '2024-02-28T17:29:27Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.18502v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.18502v1'},\n",
              "  'RNNs are not Transformers (Yet): The Key Bottleneck on In-context\\n  Retrieval': {'Published Date': '2024-02-28T17:38:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.18510v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.18510v3'},\n",
              "  'Neural Exec: Learning (and Learning from) Execution Triggers for Prompt\\n  Injection Attacks': {'Published Date': '2024-03-06T15:40:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.03792v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.03792v2'},\n",
              "  'FaaF: Facts as a Function for the evaluation of generated text': {'Published Date': '2024-03-06T17:48:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.03888v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.03888v2'},\n",
              "  'HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild': {'Published Date': '2024-03-07T08:25:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.04307v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.04307v2'},\n",
              "  'RA-ISF: Learning to Answer and Understand from Retrieval Augmentation\\n  via Iterative Self-Feedback': {'Published Date': '2024-03-11T16:01:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.06840v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.06840v2'},\n",
              "  'Exploring the Capabilities and Limitations of Large Language Models in\\n  the Electric Energy Sector': {'Published Date': '2024-03-14T06:17:20Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.09125v5',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.09125v5'},\n",
              "  'Improving Medical Multi-modal Contrastive Learning with Expert\\n  Annotations': {'Published Date': '2024-03-15T09:54:04Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.10153v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.10153v1'},\n",
              "  'LLMs Instruct LLMs:An Extraction and Editing Method': {'Published Date': '2024-03-23T06:03:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.15736v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.15736v1'},\n",
              "  'Octopus v2: On-device language model for super agent': {'Published Date': '2024-04-02T09:01:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.01744v5',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.01744v5'},\n",
              "  'Prompts As Programs: A Structure-Aware Approach to Efficient\\n  Compile-Time Prompt Optimization': {'Published Date': '2024-04-02T21:35:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.02319v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.02319v1'},\n",
              "  'uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?': {'Published Date': '2024-04-03T05:31:59Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.02474v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.02474v1'},\n",
              "  'IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe\\n  Biomedical Natural Language Inference for Clinical Trials': {'Published Date': '2024-04-06T05:44:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.04510v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.04510v1'},\n",
              "  'Enhancing Software-Related Information Extraction via Single-Choice\\n  Question Answering with Large Language Models': {'Published Date': '2024-04-08T15:00:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.05587v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.05587v2'},\n",
              "  'AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free\\n  Information Retrieval': {'Published Date': '2024-04-09T04:20:27Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.06004v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.06004v1'},\n",
              "  'Dimensionality Reduction in Sentence Transformer Vector Databases with\\n  Fast Fourier Transform': {'Published Date': '2024-04-09T13:02:22Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.06278v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.06278v1'},\n",
              "  'Onco-Retriever: Generative Classifier for Retrieval of EHR Records in\\n  Oncology': {'Published Date': '2024-04-10T02:02:34Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.06680v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.06680v1'},\n",
              "  'LLMs in Biomedicine: A study on clinical Named Entity Recognition': {'Published Date': '2024-04-10T22:26:26Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.07376v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.07376v1'},\n",
              "  'Position Engineering: Boosting Large Language Models through Positional\\n  Information Manipulation': {'Published Date': '2024-04-17T10:00:56Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.11216v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.11216v1'},\n",
              "  'MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory': {'Published Date': '2024-04-17T18:13:16Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.11672v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.11672v1'},\n",
              "  'Retrieval-Augmented Audio Deepfake Detection': {'Published Date': '2024-04-22T05:46:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.13892v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.13892v2'},\n",
              "  'GAIA: A General AI Assistant for Intelligent Accelerator Operations': {'Published Date': '2024-05-02T15:06:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.01359v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.01359v1'},\n",
              "  'Remote Diffusion': {'Published Date': '2024-05-07T23:44:09Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.04717v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.04717v1'},\n",
              "  'Automated Conversion of Static to Dynamic Scheduler via Natural Language': {'Published Date': '2024-05-08T04:07:38Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.06697v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.06697v1'},\n",
              "  'PyZoBot: A Platform for Conversational Information Extraction and\\n  Synthesis from Curated Zotero Reference Libraries through Advanced\\n  Retrieval-Augmented Generation': {'Published Date': '2024-05-13T17:44:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.07963v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.07963v1'},\n",
              "  'Exploring the Potential of Large Language Models for Automation in\\n  Technical Customer Service': {'Published Date': '2024-05-15T07:48:10Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.09161v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.09161v2'},\n",
              "  'FinTextQA: A Dataset for Long-form Financial Question Answering': {'Published Date': '2024-05-16T10:53:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.09980v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.09980v1'},\n",
              "  'Retrieving and Refining: A Hybrid Framework with Large Language Models\\n  for Rare Disease Identification': {'Published Date': '2024-05-16T20:59:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.10440v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.10440v1'},\n",
              "  'FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\\n  Question Answering': {'Published Date': '2024-05-22T17:56:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13873v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13873v1'},\n",
              "  'SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS\\n  Design Generation': {'Published Date': '2024-05-25T05:45:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.16072v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.16072v2'},\n",
              "  'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability': {'Published Date': '2024-05-27T13:16:29Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.17147v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.17147v1'},\n",
              "  'ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with\\n  LLM-Enhanced Cardiological Text': {'Published Date': '2024-05-26T06:45:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.19366v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.19366v1'},\n",
              "  'Unlearning Climate Misinformation in Large Language Models': {'Published Date': '2024-05-29T23:11:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.19563v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.19563v1'},\n",
              "  'DepsRAG: Towards Managing Software Dependencies using Large Language\\n  Models': {'Published Date': '2024-05-30T20:05:44Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20455v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20455v3'},\n",
              "  'COS-Mix: Cosine Similarity and Distance Fusion for Improved Information\\n  Retrieval': {'Published Date': '2024-06-02T06:48:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.00638v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.00638v1'},\n",
              "  'Recent advances in text embedding: A Comprehensive Review of\\n  Top-Performing Methods on the MTEB Benchmark': {'Published Date': '2024-05-27T09:52:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.01607v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.01607v2'},\n",
              "  'Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning\\n  Compressor': {'Published Date': '2024-06-04T12:43:23Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.02266v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.02266v1'},\n",
              "  'Analyzing Temporal Complex Events with Large Language Models? A\\n  Benchmark towards Temporal, Long Context Understanding': {'Published Date': '2024-06-04T16:42:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.02472v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.02472v1'},\n",
              "  'Evaluating the Retrieval Component in LLM-Based Question Answering\\n  Systems': {'Published Date': '2024-06-10T16:46:22Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.06458v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.06458v1'},\n",
              "  'TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation\\n  and LLMs': {'Published Date': '2024-06-11T08:35:23Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.07053v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.07053v1'},\n",
              "  'Battling Botpoop using GenAI for Higher Education: A Study of a\\n  Retrieval Augmented Generation Chatbots Impact on Learning': {'Published Date': '2024-06-12T01:19:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.07796v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.07796v2'},\n",
              "  'Blowfish: Topological and statistical signatures for quantifying\\n  ambiguity in semantic search': {'Published Date': '2024-06-12T08:26:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.07990v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.07990v1'},\n",
              "  'ClimRetrieve: A Benchmarking Dataset for Information Retrieval from\\n  Corporate Climate Disclosures': {'Published Date': '2024-06-14T08:21:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.09818v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.09818v1'},\n",
              "  'RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation\\n  Detection Using In-Context Learning based on Emotional Information': {'Published Date': '2024-06-16T22:49:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11093v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11093v1'},\n",
              "  'TIFG: Text-Informed Feature Generation with Large Language Models': {'Published Date': '2024-06-17T03:29:14Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.11177v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.11177v1'},\n",
              "  'Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text\\n  Understanding': {'Published Date': '2024-06-18T06:54:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12331v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12331v1'},\n",
              "  'PARAFAC2-based Coupled Matrix and Tensor Factorizations with Constraints': {'Published Date': '2024-06-18T07:05:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12338v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12338v1'},\n",
              "  'Identifying Performance-Sensitive Configurations in Software Systems\\n  through Code Analysis with LLM Agents': {'Published Date': '2024-06-18T17:22:48Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12806v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12806v1'},\n",
              "  'Improving Zero-shot LLM Re-Ranker with Risk Minimization': {'Published Date': '2024-06-19T08:29:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13331v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13331v1'},\n",
              "  'Thread: A Logic-Based Data Organization Paradigm for How-To Question\\n  Answering with Retrieval Augmented Generation': {'Published Date': '2024-06-19T09:14:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.13372v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.13372v1'},\n",
              "  'TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction\\n  in RAG-based Crowdsourcing Systems': {'Published Date': '2024-06-21T01:52:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14825v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14825v2'},\n",
              "  'Retrieve-Plan-Generation: An Iterative Planning and Answering Framework\\n  for Knowledge-Intensive LLM Generation': {'Published Date': '2024-06-21T08:45:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.14979v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.14979v1'},\n",
              "  'Harnessing Knowledge Retrieval with Large Language Models for Clinical\\n  Report Error Correction': {'Published Date': '2024-06-21T10:48:21Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.15045v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.15045v1'},\n",
              "  'FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy\\n  in Large Language Models': {'Published Date': '2024-06-23T17:18:19Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.16167v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.16167v1'},\n",
              "  'Context-augmented Retrieval: A Novel Framework for Fast Information\\n  Retrieval based Response Generation using Large Language Model': {'Published Date': '2024-06-24T07:52:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.16383v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.16383v1'},\n",
              "  'Attention Instruction: Amplifying Attention in the Middle via Prompting': {'Published Date': '2024-06-24T19:35:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.17095v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.17095v1'},\n",
              "  'LumberChunker: Long-Form Narrative Document Segmentation': {'Published Date': '2024-06-25T13:08:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.17526v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.17526v1'},\n",
              "  'Assessing the Effectiveness of LLMs in Android Application Vulnerability\\n  Analysis': {'Published Date': '2024-06-27T05:14:34Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.18894v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.18894v1'},\n",
              "  'Which Neurons Matter in IR? Applying Integrated Gradients-based Methods\\n  to Understand Cross-Encoders': {'Published Date': '2024-06-27T16:33:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.19309v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.19309v1'},\n",
              "  'Re2G: Retrieve, Rerank, Generate': {'Published Date': '2022-07-13T15:51:40Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2207.06300v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2207.06300v1'},\n",
              "  'MuRAG: Multimodal Retrieval-Augmented Generator for Open Question\\n  Answering over Images and Text': {'Published Date': '2022-10-06T13:58:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2210.02928v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2210.02928v2'},\n",
              "  'Retrieval Augmented Generation using Engineering Design Knowledge': {'Published Date': '2023-07-13T17:25:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2307.06985v9',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2307.06985v9'},\n",
              "  'VulLibGen: Generating Names of Vulnerability-Affected Packages via a\\n  Large Language Model': {'Published Date': '2023-08-09T02:02:46Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2308.04662v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2308.04662v3'},\n",
              "  'Model-Free Large-Scale Cloth Spreading With Mobile Manipulation: Initial\\n  Feasibility Study': {'Published Date': '2023-08-21T00:30:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2308.10401v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2308.10401v1'},\n",
              "  'Chatmap : Large Language Model Interaction with Cartographic Data': {'Published Date': '2023-09-28T15:32:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.01429v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.01429v1'},\n",
              "  'Glitter or Gold? Deriving Structured Insights from Sustainability\\n  Reports via Large Language Models': {'Published Date': '2023-10-09T11:34:41Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2310.05628v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2310.05628v3'},\n",
              "  'Detailing secondary frontal bore of internal tides breaking above\\n  deep-ocean topography': {'Published Date': '2023-11-14T08:07:11Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.07976v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.07976v1'},\n",
              "  'SenTest: Evaluating Robustness of Sentence Encoders': {'Published Date': '2023-11-29T15:21:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2311.17722v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2311.17722v1'},\n",
              "  'NDSEARCH: Accelerating Graph-Traversal-Based Approximate Nearest\\n  Neighbor Search through Near Data Processing': {'Published Date': '2023-12-05T21:21:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.03141v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.03141v2'},\n",
              "  'Fortify the Shortest Stave in Attention: Enhancing Context Awareness of\\n  Large Language Models for Effective Tool Use': {'Published Date': '2023-12-07T17:24:51Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.04455v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.04455v4'},\n",
              "  'Context-aware Decoding Reduces Hallucination in Query-focused\\n  Summarization': {'Published Date': '2023-12-21T23:42:13Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.14335v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.14335v2'},\n",
              "  'HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and\\n  Reliable Medical LLMs Responses': {'Published Date': '2023-12-26T04:49:56Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.15883v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.15883v2'},\n",
              "  'ESGReveal: An LLM-based approach for extracting structured data from ESG\\n  reports': {'Published Date': '2023-12-25T06:44:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.17264v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.17264v1'},\n",
              "  'A Reliable Knowledge Processing Framework for Combustion Science using\\n  Foundation Models': {'Published Date': '2023-12-31T17:15:25Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.00544v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.00544v2'},\n",
              "  'De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks\\n  via Iterative Grounding': {'Published Date': '2024-01-03T12:09:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.01701v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.01701v3'},\n",
              "  'Interactive AI with Retrieval-Augmented Generation for Next Generation\\n  Networking': {'Published Date': '2024-01-21T03:46:00Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.11391v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.11391v1'},\n",
              "  'Evaluating and Enhancing Large Language Models Performance in\\n  Domain-specific Medicine: Osteoarthritis Management with DocOA': {'Published Date': '2024-01-20T03:41:23Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.12998v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.12998v1'},\n",
              "  'LLaMP: Large Language Model Made Powerful for High-fidelity Materials\\n  Knowledge Retrieval and Distillation': {'Published Date': '2024-01-30T18:37:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.17244v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.17244v2'},\n",
              "  'Health-LLM: Personalized Retrieval-Augmented Disease Prediction System': {'Published Date': '2024-02-01T16:40:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.00746v6',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.00746v6'},\n",
              "  'LitLLM: A Toolkit for Scientific Literature Review': {'Published Date': '2024-02-02T02:41:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01788v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01788v1'},\n",
              "  'Retrieval Augmented End-to-End Spoken Dialog Models': {'Published Date': '2024-02-02T18:23:09Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.01828v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.01828v1'},\n",
              "  'How well do LLMs cite relevant medical references? An evaluation\\n  framework and analyses': {'Published Date': '2024-02-03T03:44:57Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.02008v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.02008v1'},\n",
              "  'Generative AI in the Construction Industry: A State-of-the-art Analysis': {'Published Date': '2024-02-15T13:39:55Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.09939v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.09939v1'},\n",
              "  'PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal\\n  Question-Answering': {'Published Date': '2024-02-16T19:26:09Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.11034v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.11034v2'},\n",
              "  'Where is the answer? Investigating Positional Bias in Language Model\\n  Knowledge Extraction': {'Published Date': '2024-02-16T06:29:16Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.12170v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.12170v2'},\n",
              "  'Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based\\n  Question Answering with Domain Hybrid Data': {'Published Date': '2024-02-20T10:00:58Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.12869v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.12869v2'},\n",
              "  'MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems\\n  in LLM Augmented Generation': {'Published Date': '2024-02-22T12:13:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.14480v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.14480v1'},\n",
              "  'DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy\\n  in Large-Scale Databases': {'Published Date': '2024-03-01T07:14:45Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.00872v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.00872v1'},\n",
              "  'From human experts to machines: An LLM supported approach to ontology\\n  and knowledge graph construction': {'Published Date': '2024-03-13T08:50:15Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.08345v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.08345v1'},\n",
              "  'S3LLM: Large-Scale Scientific Software Understanding with LLMs using\\n  Source, Metadata, and Document': {'Published Date': '2024-03-15T17:04:27Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.10588v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.10588v1'},\n",
              "  'AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented\\n  Stock-Chain Framework': {'Published Date': '2024-03-19T09:45:33Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.12582v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.12582v1'},\n",
              "  'Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\\n  Models through Question Complexity': {'Published Date': '2024-03-21T13:52:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.14403v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.14403v2'},\n",
              "  'MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering': {'Published Date': '2024-03-28T03:14:18Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.19116v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.19116v1'},\n",
              "  'Dialectical Alignment: Resolving the Tension of 3H and Security Threats\\n  of LLMs': {'Published Date': '2024-03-30T22:41:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.00486v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.00486v1'},\n",
              "  'A Comparison of Methods for Evaluating Generative IR': {'Published Date': '2024-04-05T11:55:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.04044v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.04044v2'},\n",
              "  'RAR-b: Reasoning as Retrieval Benchmark': {'Published Date': '2024-04-09T14:34:48Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.06347v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.06347v2'},\n",
              "  'Towards Robustness of Text-to-Visualization Translation against Lexical\\n  and Phrasal Variability': {'Published Date': '2024-04-10T16:12:50Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.07135v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.07135v2'},\n",
              "  'Generative AI Agent for Next-Generation MIMO Design: Fundamentals,\\n  Challenges, and Vision': {'Published Date': '2024-04-13T02:39:36Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.08878v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.08878v1'},\n",
              "  'Interactive Generative AI Agents for Satellite Networks through a\\n  Mixture of Experts Transmission': {'Published Date': '2024-04-14T03:44:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.09134v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.09134v1'},\n",
              "  'Cross-Data Knowledge Graph Construction for LLM-enabled Educational\\n  Question-Answering System: A~Case~Study~at~HCMUT': {'Published Date': '2024-04-14T16:34:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.09296v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.09296v1'},\n",
              "  'Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations': {'Published Date': '2024-03-23T13:25:01Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.10779v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.10779v1'},\n",
              "  'LongEmbed: Extending Embedding Models for Long Context Retrieval': {'Published Date': '2024-04-18T11:29:23Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.12096v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.12096v2'},\n",
              "  'Generative AI for Low-Carbon Artificial Intelligence of Things': {'Published Date': '2024-04-28T05:46:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.18077v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.18077v1'},\n",
              "  'ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\\n  using Large Language Model for Stock Performance Prediction': {'Published Date': '2024-04-29T07:11:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.18470v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.18470v1'},\n",
              "  'Self-Improving Customer Review Response Generation Based on LLMs': {'Published Date': '2024-05-06T20:50:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.03845v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.03845v1'},\n",
              "  'Prompt-based Code Completion via Multi-Retrieval Augmented Generation': {'Published Date': '2024-05-13T07:56:15Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.07530v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.07530v1'},\n",
              "  'Generative AI and Large Language Models for Cyber Security: All Insights\\n  You Need': {'Published Date': '2024-05-21T13:02:27Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.12750v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.12750v1'},\n",
              "  'Can Github issues be solved with Tree Of Thoughts?': {'Published Date': '2024-05-20T11:05:56Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13057v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13057v1'},\n",
              "  'TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in\\n  Large Language Models': {'Published Date': '2024-05-22T07:21:32Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.13401v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.13401v3'},\n",
              "  'G3: An Effective and Adaptive Framework for Worldwide Geolocalization\\n  Using Large Multi-Modality Models': {'Published Date': '2024-05-23T15:37:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.14702v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.14702v1'},\n",
              "  'Exploiting the Layered Intrinsic Dimensionality of Deep Models for\\n  Practical Adversarial Training': {'Published Date': '2024-05-27T12:48:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.17130v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.17130v1'},\n",
              "  'Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual\\n  Performance in LLMs': {'Published Date': '2024-05-28T16:56:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.18359v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.18359v1'},\n",
              "  'Similarity is Not All You Need: Endowing Retrieval Augmented Generation\\n  with Multi Layered Thoughts': {'Published Date': '2024-05-30T09:50:38Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.19893v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.19893v1'},\n",
              "  'Designing an Evaluation Framework for Large Language Models in Astronomy\\n  Research': {'Published Date': '2024-05-30T18:00:21Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20389v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20389v1'},\n",
              "  'Exploring Backdoor Attacks against Large Language Model-based Decision\\n  Making': {'Published Date': '2024-05-27T17:59:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.20774v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.20774v1'},\n",
              "  'Superhuman performance in urology board questions by an explainable\\n  large language model enabled for context integration of the European\\n  Association of Urology guidelines: the UroBot study': {'Published Date': '2024-06-03T15:26:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.01428v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.01428v2'},\n",
              "  'UniOQA: A Unified Framework for Knowledge Graph Question Answering with\\n  Large Language Models': {'Published Date': '2024-06-04T08:36:39Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.02110v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.02110v1'},\n",
              "  'RATT: A Thought Structure for Coherent and Correct LLM Reasoning': {'Published Date': '2024-06-04T20:02:52Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.02746v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.02746v2'},\n",
              "  'Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge\\n  Devices': {'Published Date': '2024-06-06T06:41:53Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.03777v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.03777v2'},\n",
              "  'A + B: A General Generator-Reader Framework for Optimizing LLMs to\\n  Unleash Synergy Potential': {'Published Date': '2024-06-06T11:14:27Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.03963v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.03963v1'},\n",
              "  'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance\\n  Assessor': {'Published Date': '2024-06-10T17:58:29Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.06519v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.06519v1'},\n",
              "  'RAG-based Crowdsourcing Task Decomposition via Masked Contrastive\\n  Learning with Prompts': {'Published Date': '2024-06-04T08:34:19Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.06577v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.06577v1'},\n",
              "  'Artificial Intelligence as the New Hacker: Developing Agents for\\n  Offensive Security': {'Published Date': '2024-05-09T18:15:12Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.07561v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.07561v1'},\n",
              "  'We Have a Package for You! A Comprehensive Analysis of Package\\n  Hallucinations by Code Generating LLMs': {'Published Date': '2024-06-12T03:29:06Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.10279v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.10279v1'},\n",
              "  'Current state of LLM Risks and AI Guardrails': {'Published Date': '2024-06-16T22:04:10Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.12934v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.12934v1'},\n",
              "  'Found in the Middle: Calibrating Positional Attention Bias Improves Long\\n  Context Utilization': {'Published Date': '2024-06-23T04:35:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.16008v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.16008v1'},\n",
              "  'Graph-Augmented LLMs for Personalized Health Insights: A Case Study in\\n  Sleep Analysis': {'Published Date': '2024-06-24T01:22:54Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.16252v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.16252v2'},\n",
              "  'CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented\\n  Analysis Generation': {'Published Date': '2024-06-24T23:57:57Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.17186v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.17186v2'},\n",
              "  'Leave No Document Behind: Benchmarking Long-Context LLMs with Extended\\n  Multi-Doc QA': {'Published Date': '2024-06-25T09:42:56Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.17419v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.17419v1'},\n",
              "  'AI-native Memory: A Pathway from LLMs Towards AGI': {'Published Date': '2024-06-26T12:51:37Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.18312v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.18312v1'},\n",
              "  'The Impacts of Neutron-Star Structure and Base Heating on Type I X-Ray\\n  Bursts and Code Comparison': {'Published Date': '2023-04-14T15:18:44Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2304.07197v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2304.07197v1'},\n",
              "  'Merging-Diverging Hybrid Transformer Networks for Survival Prediction in\\n  Head and Neck Cancer': {'Published Date': '2023-07-07T07:16:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2307.03427v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2307.03427v1'},\n",
              "  'Dynamic Retrieval Augmented Generation of Ontologies using Artificial\\n  Intelligence (DRAGON-AI)': {'Published Date': '2023-12-18T03:19:31Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.10904v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.10904v2'},\n",
              "  'Privacy-Preserved Neural Graph Databases': {'Published Date': '2023-12-25T02:32:05Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.15591v5',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.15591v5'},\n",
              "  'DB-GPT: Empowering Database Interactions with Private Large Language\\n  Models': {'Published Date': '2023-12-29T03:23:23Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2312.17449v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2312.17449v2'},\n",
              "  'Improving Medical Reasoning through Retrieval and Self-Reflection with\\n  Retrieval-Augmented Large Language Models': {'Published Date': '2024-01-27T02:29:42Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.15269v3',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.15269v3'},\n",
              "  'CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation\\n  for Evaluating LLMs in Cybersecurity Knowledge': {'Published Date': '2024-02-12T14:53:28Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.07688v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.07688v2'},\n",
              "  'FinBen: A Holistic Financial Benchmark for Large Language Models': {'Published Date': '2024-02-20T02:16:16Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2402.12659v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2402.12659v2'},\n",
              "  'AesopAgent: Agent-driven Evolutionary System on Story-to-Video\\n  Production': {'Published Date': '2024-03-12T02:30:50Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.07952v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.07952v1'},\n",
              "  'Generation of Asset Administration Shell with Large Language Model\\n  Agents: Toward Semantic Interoperability in Digital Twins in the Context of\\n  Industry 4.0': {'Published Date': '2024-03-25T21:37:30Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2403.17209v4',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2403.17209v4'},\n",
              "  \"ClashEval: Quantifying the tug-of-war between an LLM's internal prior\\n  and external evidence\": {'Published Date': '2024-04-16T00:43:03Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2404.10198v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2404.10198v2'},\n",
              "  'Characterizing the Dilemma of Performance and Index Size in\\n  Billion-Scale Vector Search and Breaking It with Second-Tier Memory': {'Published Date': '2024-05-06T08:38:14Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.03267v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.03267v2'},\n",
              "  'Perception of Knowledge Boundary for Large Language Models through\\n  Semi-open-ended Question Answering': {'Published Date': '2024-05-23T10:00:14Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.14383v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.14383v1'},\n",
              "  'CacheBlend: Fast Large Language Model Serving for RAG with Cached\\n  Knowledge Fusion': {'Published Date': '2024-05-26T06:00:17Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2405.16444v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2405.16444v2'},\n",
              "  'RAG-Enhanced Commit Message Generation': {'Published Date': '2024-06-08T16:24:24Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.05514v2',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.05514v2'},\n",
              "  'Diagnosis Assistant for Liver Cancer Utilizing a Large Language Model\\n  with Three Types of Knowledge': {'Published Date': '2024-06-26T03:32:35Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2406.18039v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2406.18039v1'},\n",
              "  'Weaver: Foundation Models for Creative Writing': {'Published Date': '2024-01-30T18:58:43Z',\n",
              "   'alternate link1': 'http://arxiv.org/abs/2401.17268v1',\n",
              "   'pdf link1': 'http://arxiv.org/pdf/2401.17268v1'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('pdfLinks_published_dates.json', 'w') as f:\n",
        "    json.dump(pdf_links, f, indent=4)"
      ],
      "metadata": {
        "id": "gyos4nidTvQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The below code first takes the url from your json file then downloads the paper in colab local storage and then copies it to your g drive but you will want to upload the json file  with names and links of rag papers"
      ],
      "metadata": {
        "id": "ms5gYIjFkJFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yKttlMbgzDJ",
        "outputId": "6b997a19-751b-4c08-baac-1ac8ad4feaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/My Drive/rag papers\""
      ],
      "metadata": {
        "id": "KsL-Z6m2iXht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "def download_and_save_to_drive(url, filename, drive_path='/content/drive/My Drive/rag papers/'):\n",
        "    \"\"\"Downloads a file from the given URL and saves it to Google Drive.\"\"\"\n",
        "    local_filename = '/content/' + filename  # Save temporarily in Colab\n",
        "    urllib.request.urlretrieve(url, local_filename)\n",
        "\n",
        "    drive_filename = drive_path + filename  # Full path in Google Drive\n",
        "    !cp \"{local_filename}\" \"{drive_filename}\"  # Copy to Google Drive\n",
        "\n",
        "# Example usage:\n",
        "paper_url = 'http://arxiv.org/pdf/2404.00657v1'  # Replace with the actual URL\n",
        "filename = 'Observations on Building RAG Systems for Technical Documents.pdf'  # Choose a filename\n",
        "download_and_save_to_drive(paper_url, filename)"
      ],
      "metadata": {
        "id": "iUxIAu-vf_N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/pdfLinks_published_dates.json', 'r') as f:\n",
        "        rag_paper_data = json.load(f)"
      ],
      "metadata": {
        "id": "qaHfVgj1jPX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for paper_name,paper_data in rag_paper_data.items():\n",
        "  filename = paper_name\n",
        "  paper_url = paper_data['pdf link1']\n",
        "  print(filename)\n",
        "  print(paper_url)\n",
        "  download_and_save_to_drive(paper_url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mp_OzFJjAUg",
        "outputId": "df86e08b-1482-4b0d-881d-d04b7d81a71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop\n",
            "  Queries\n",
            "http://arxiv.org/pdf/2401.15391v1\n",
            "Seven Failure Points When Engineering a Retrieval Augmented Generation\n",
            "  System\n",
            "http://arxiv.org/pdf/2401.05856v1\n",
            "RAGGED: Towards Informed Design of Retrieval Augmented Generation\n",
            "  Systems\n",
            "http://arxiv.org/pdf/2403.09040v1\n",
            "Observations on Building RAG Systems for Technical Documents\n",
            "http://arxiv.org/pdf/2404.00657v1\n",
            "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented\n",
            "  Generation in Niche Domains, Exemplified by Korean Medicine\n",
            "http://arxiv.org/pdf/2401.11246v1\n",
            "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented\n",
            "  Generation (RAG)\n",
            "http://arxiv.org/pdf/2402.16893v1\n",
            "CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions\n",
            "  for RAG systems\n",
            "http://arxiv.org/pdf/2404.02103v1\n",
            "Evaluation of Retrieval-Augmented Generation: A Survey\n",
            "http://arxiv.org/pdf/2405.07437v1\n",
            "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation\n",
            "  Research\n",
            "http://arxiv.org/pdf/2405.13576v1\n",
            "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database\n",
            "  Filtering with LLM-Extracted Metadata\n",
            "http://arxiv.org/pdf/2406.13213v1\n",
            "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by\n",
            "  Simulating Documents in the Wild via Low-level Perturbations\n",
            "http://arxiv.org/pdf/2404.13948v1\n",
            "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical\n",
            "  Regulatory Compliance Process\n",
            "http://arxiv.org/pdf/2402.01717v1\n",
            "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy\n",
            "  with Semantic Search and Hybrid Query-Based Retrievers\n",
            "http://arxiv.org/pdf/2404.07220v1\n",
            "Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language\n",
            "  Models for Telecommunications\n",
            "http://arxiv.org/pdf/2404.15939v2\n",
            "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG)\n",
            "  Models for Open Domain Question Answering\n",
            "http://arxiv.org/pdf/2210.02627v1\n",
            "Understand What LLM Needs: Dual Preference Alignment for\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2406.18676v1\n",
            "Retrieval-Augmented Generation for AI-Generated Content: A Survey\n",
            "http://arxiv.org/pdf/2402.19473v6\n",
            "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction\n",
            "http://arxiv.org/pdf/2403.14374v1\n",
            "Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge\n",
            "  Gaps\n",
            "http://arxiv.org/pdf/2312.07796v1\n",
            "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
            "http://arxiv.org/pdf/2312.10997v5\n",
            "Unsupervised Information Refinement Training of Large Language Models\n",
            "  for Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2402.18150v2\n",
            "Investigating the performance of Retrieval-Augmented Generation and\n",
            "  fine-tuning for the development of AI-driven knowledge-based systems\n",
            "http://arxiv.org/pdf/2403.09727v1\n",
            "From Local to Global: A Graph RAG Approach to Query-Focused\n",
            "  Summarization\n",
            "http://arxiv.org/pdf/2404.16130v1\n",
            "Robust Implementation of Retrieval-Augmented Generation on Edge-based\n",
            "  Computing-in-Memory Architectures\n",
            "http://arxiv.org/pdf/2405.04700v1\n",
            "The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented\n",
            "  Generation (FutureDial-RAG)\n",
            "http://arxiv.org/pdf/2405.13084v1\n",
            "Unveil the Duality of Retrieval-Augmented Generation: Theoretical\n",
            "  Analysis and Practical Solution\n",
            "http://arxiv.org/pdf/2406.00944v1\n",
            "DomainRAG: A Chinese Benchmark for Evaluating Domain-specific\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2406.05654v2\n",
            "Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level\n",
            "  RAG\n",
            "http://arxiv.org/pdf/2406.11147v2\n",
            "ARES: An Automated Evaluation Framework for Retrieval-Augmented\n",
            "  Generation Systems\n",
            "http://arxiv.org/pdf/2311.09476v2\n",
            "Revolutionizing Retrieval-Augmented Generation with Enhanced PDF\n",
            "  Structure Recognition\n",
            "http://arxiv.org/pdf/2401.12599v1\n",
            "FeB4RAG: Evaluating Federated Search in the Context of Retrieval\n",
            "  Augmented Generation\n",
            "http://arxiv.org/pdf/2402.11891v1\n",
            "ARAGOG: Advanced RAG Output Grading\n",
            "http://arxiv.org/pdf/2404.01037v1\n",
            "Improving Retrieval for RAG based Question Answering Models on Financial\n",
            "  Documents\n",
            "http://arxiv.org/pdf/2404.07221v1\n",
            "A Survey on Retrieval-Augmented Text Generation for Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2404.10981v1\n",
            "Stochastic RAG: End-to-End Retrieval-Augmented Generation through\n",
            "  Expected Utility Maximization\n",
            "http://arxiv.org/pdf/2405.02816v1\n",
            "Don't Forget to Connect! Improving RAG with Graph-based Reranking\n",
            "http://arxiv.org/pdf/2405.18414v1\n",
            "RAG Does Not Work for Enterprises\n",
            "http://arxiv.org/pdf/2406.04369v1\n",
            "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems\n",
            "http://arxiv.org/pdf/2406.14972v1\n",
            "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented\n",
            "  Generation of Large Language Models\n",
            "http://arxiv.org/pdf/2401.17043v2\n",
            "C-RAG: Certified Generation Risks for Retrieval-Augmented Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2402.03181v4\n",
            "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning\n",
            "http://arxiv.org/pdf/2405.20139v1\n",
            "Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework\n",
            "http://arxiv.org/pdf/2406.14783v1\n",
            "Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024\n",
            "  Retrieval-Augmented Generation Track\n",
            "http://arxiv.org/pdf/2406.16828v1\n",
            "Robust Action Governor for Uncertain Piecewise Affine Systems with\n",
            "  Non-convex Constraints and Safe Reinforcement Learning\n",
            "http://arxiv.org/pdf/2207.08240v1\n",
            "Enhancing Multilingual Information Retrieval in Mixed Human Resources\n",
            "  Environments: A RAG Model Implementation for Multicultural Enterprise\n",
            "http://arxiv.org/pdf/2401.01511v1\n",
            "RAG-Fusion: a New Take on Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2402.03367v2\n",
            "Financial Report Chunking for Effective Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2402.05131v3\n",
            "Retrieval Augmented Generation Systems: Automatic Dataset Creation,\n",
            "  Evaluation and Boolean Agent Setup\n",
            "http://arxiv.org/pdf/2403.00820v1\n",
            "RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots\n",
            "http://arxiv.org/pdf/2403.01193v3\n",
            "Boosting Conversational Question Answering with Fine-Grained\n",
            "  Retrieval-Augmentation and Self-Check\n",
            "http://arxiv.org/pdf/2403.18243v1\n",
            "Introducing Super RAGs in Mistral 8x7B-v1\n",
            "http://arxiv.org/pdf/2404.08940v1\n",
            "InspectorRAGet: An Introspection Platform for RAG Evaluation\n",
            "http://arxiv.org/pdf/2404.17347v1\n",
            "Towards a Search Engine for Machines: Unified Ranking for Multiple\n",
            "  Retrieval-Augmented Large Language Models\n",
            "http://arxiv.org/pdf/2405.00175v1\n",
            "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n",
            "  Blocker Documents\n",
            "http://arxiv.org/pdf/2406.05870v1\n",
            "Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG)\n",
            "  via Pure Synthetic Data\n",
            "http://arxiv.org/pdf/2406.14773v1\n",
            "Seeing Is Believing: Black-Box Membership Inference Attacks Against\n",
            "  Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2406.19234v1\n",
            "DuetRAG: Collaborative Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2405.13002v1\n",
            "Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2309.01431v2\n",
            "The Chronicles of RAG: The Retriever, the Chunk and the Generator\n",
            "http://arxiv.org/pdf/2401.07883v1\n",
            "The Power of Noise: Redefining Retrieval for RAG Systems\n",
            "http://arxiv.org/pdf/2401.14887v4\n",
            "Benchmarking Retrieval-Augmented Generation for Medicine\n",
            "http://arxiv.org/pdf/2402.13178v2\n",
            "Compressing Long Context for Enhancing RAG with AMR-based Concept\n",
            "  Distillation\n",
            "http://arxiv.org/pdf/2405.03085v1\n",
            "Accelerating Inference of Retrieval-Augmented Generation via Sparse\n",
            "  Context Selection\n",
            "http://arxiv.org/pdf/2405.16178v1\n",
            "Empowering Large Language Models to Set up a Knowledge Retrieval Indexer\n",
            "  via Self-Learning\n",
            "http://arxiv.org/pdf/2405.16933v1\n",
            "Is My Data in Your Retrieval Database? Membership Inference Attacks\n",
            "  Against Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2405.20446v2\n",
            "Phantom: General Trigger Attacks on Retrieval Augmented Language\n",
            "  Generation\n",
            "http://arxiv.org/pdf/2405.20485v1\n",
            "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs\n",
            "http://arxiv.org/pdf/2406.05085v1\n",
            "Development and Testing of Retrieval Augmented Generation in Large\n",
            "  Language Models -- A Case Study Report\n",
            "http://arxiv.org/pdf/2402.01733v1\n",
            "A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs\n",
            "http://arxiv.org/pdf/2404.06082v1\n",
            "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2309.15217v1\n",
            "DFA-RAG: Conversational Semantic Router for Large Language Model with\n",
            "  Definite Finite Automaton\n",
            "http://arxiv.org/pdf/2402.04411v2\n",
            "ActiveRAG: Revealing the Treasures of Knowledge via Active Learning\n",
            "http://arxiv.org/pdf/2402.13547v1\n",
            "Follow My Instruction and Spill the Beans: Scalable Data Extraction from\n",
            "  Retrieval-Augmented Generation Systems\n",
            "http://arxiv.org/pdf/2402.17840v2\n",
            "Fine Tuning vs. Retrieval Augmented Generation for Less Popular\n",
            "  Knowledge\n",
            "http://arxiv.org/pdf/2403.01432v2\n",
            "Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A\n",
            "  Case Study on Domain-Specific Queries in Private Knowledge-Bases\n",
            "http://arxiv.org/pdf/2403.10446v1\n",
            "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2404.00610v1\n",
            "CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs\n",
            "  for Legal Question Answering\n",
            "http://arxiv.org/pdf/2404.04302v1\n",
            "RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political\n",
            "  Fact-Checking using Multimodal Large Language Models\n",
            "http://arxiv.org/pdf/2404.12065v1\n",
            "Evaluating Retrieval Quality in Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2404.13781v1\n",
            "Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular\n",
            "  RAG Applications\n",
            "http://arxiv.org/pdf/2405.01585v1\n",
            "Question-Based Retrieval using Atomic Units for Enterprise RAG\n",
            "http://arxiv.org/pdf/2405.12363v1\n",
            "RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented\n",
            "  Generation and Readability Control for Layman Summarization of Biomedical\n",
            "  Texts\n",
            "http://arxiv.org/pdf/2405.13179v4\n",
            "Automated Evaluation of Retrieval-Augmented Language Models with\n",
            "  Task-Specific Exam Generation\n",
            "http://arxiv.org/pdf/2405.13622v1\n",
            "M-RAG: Reinforcing Large Language Model Performance through\n",
            "  Retrieval-Augmented Generation with Multiple Partitions\n",
            "http://arxiv.org/pdf/2405.16420v1\n",
            "A Multi-Source Retrieval Question Answering Framework Based on RAG\n",
            "http://arxiv.org/pdf/2405.19207v1\n",
            "RAG Enabled Conversations about Household Electricity Monitoring\n",
            "http://arxiv.org/pdf/2406.06566v1\n",
            "Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and\n",
            "  Abbreviation De-hallucination\n",
            "http://arxiv.org/pdf/2406.06575v1\n",
            "HIRO: Hierarchical Information Retrieval Optimization\n",
            "http://arxiv.org/pdf/2406.09979v1\n",
            "The Impact of Quantization on Retrieval-Augmented Generation: An\n",
            "  Analysis of Small LLMs\n",
            "http://arxiv.org/pdf/2406.10251v1\n",
            "Iterative Utility Judgment Framework via LLMs Inspired by Relevance in\n",
            "  Philosophy\n",
            "http://arxiv.org/pdf/2406.11290v1\n",
            "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG\n",
            "  Systems: A Comparative Study of Performance and Scalability\n",
            "http://arxiv.org/pdf/2406.11424v1\n",
            "CrAM: Credibility-Aware Attention Modification in LLMs for Combating\n",
            "  Misinformation in RAG\n",
            "http://arxiv.org/pdf/2406.11497v2\n",
            "Model Internals-based Answer Attribution for Trustworthy\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2406.13663v1\n",
            "On the Role of Long-tail Knowledge in Retrieval Augmented Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2406.16367v1\n",
            "Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode\n",
            "  and Effects Analysis\n",
            "http://arxiv.org/pdf/2406.18114v1\n",
            "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on\n",
            "  Agriculture\n",
            "http://arxiv.org/pdf/2401.08406v3\n",
            "CRAG -- Comprehensive RAG Benchmark\n",
            "http://arxiv.org/pdf/2406.04744v1\n",
            "RAGTruth: A Hallucination Corpus for Developing Trustworthy\n",
            "  Retrieval-Augmented Language Models\n",
            "http://arxiv.org/pdf/2401.00396v2\n",
            "Prompt Perturbation in Retrieval-Augmented Generation based Large\n",
            "  Language Models\n",
            "http://arxiv.org/pdf/2402.07179v2\n",
            "T-RAG: Lessons from the LLM Trenches\n",
            "http://arxiv.org/pdf/2402.07483v2\n",
            "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n",
            "  Question Answering\n",
            "http://arxiv.org/pdf/2402.17497v1\n",
            "CONFLARE: CONFormal LArge language model REtrieval\n",
            "http://arxiv.org/pdf/2404.04287v1\n",
            "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2404.12457v2\n",
            "ERATTA: Extreme RAG for Table To Answers with Large Language Models\n",
            "http://arxiv.org/pdf/2405.03963v2\n",
            "Leveraging Lecture Content for Improved Feedback: Explorations with\n",
            "  GPT-4 and Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2405.06681v1\n",
            "CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control\n",
            "http://arxiv.org/pdf/2405.18727v1\n",
            "Clustered Retrieved Augmented Generation (CRAG)\n",
            "http://arxiv.org/pdf/2406.00029v1\n",
            "Toward Conversational Agents with Context and Time Sensitive Long-term\n",
            "  Memory\n",
            "http://arxiv.org/pdf/2406.00057v2\n",
            "DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented\n",
            "  Generation for Question-Answering\n",
            "http://arxiv.org/pdf/2406.07348v3\n",
            "Reinforcement Learning for Optimizing RAG for Domain Chatbots\n",
            "http://arxiv.org/pdf/2401.06800v1\n",
            "Development and Testing of a Novel Large Language Model-Based Clinical\n",
            "  Decision Support Systems for Medication Safety in 12 Clinical Specialties\n",
            "http://arxiv.org/pdf/2402.01741v2\n",
            "iRAG: An Incremental Retrieval Augmented Generation System for Videos\n",
            "http://arxiv.org/pdf/2404.12309v1\n",
            "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2405.06211v3\n",
            "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of\n",
            "  Large Language Models\n",
            "http://arxiv.org/pdf/2406.00083v2\n",
            "Retrieval Augmented Generation and Representative Vector Summarization\n",
            "  for large unstructured textual data in Medical Education\n",
            "http://arxiv.org/pdf/2308.00479v1\n",
            "Dynamic Contexts for Generating Suggestion Questions in RAG Based\n",
            "  Conversational Systems\n",
            "http://arxiv.org/pdf/2403.11413v1\n",
            "RAM: Towards an Ever-Improving Memory System by Learning from\n",
            "  Communications\n",
            "http://arxiv.org/pdf/2404.12045v1\n",
            "Control Token with Dense Passage Retrieval\n",
            "http://arxiv.org/pdf/2405.13008v1\n",
            "Retrieval-Augmented Generation for Generative Artificial Intelligence in\n",
            "  Medicine\n",
            "http://arxiv.org/pdf/2406.12449v1\n",
            "Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval\n",
            "  Augmented Generation Models for Open Book Question-Answering\n",
            "http://arxiv.org/pdf/2307.05915v2\n",
            "A Study on the Implementation of Generative AI Services Using an\n",
            "  Enterprise Data-Based LLM Application Architecture\n",
            "http://arxiv.org/pdf/2309.01105v2\n",
            "Retrieval-augmented Generation to Improve Math Question-Answering:\n",
            "  Trade-offs Between Groundedness and Human Preference\n",
            "http://arxiv.org/pdf/2310.03184v2\n",
            "Self-RAG: Learning to Retrieve, Generate, and Critique through\n",
            "  Self-Reflection\n",
            "http://arxiv.org/pdf/2310.11511v1\n",
            "GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval\n",
            "http://arxiv.org/pdf/2310.20158v1\n",
            "ChatQA: Surpassing GPT-4 on Conversational QA and RAG\n",
            "http://arxiv.org/pdf/2401.10225v4\n",
            "RAFT: Adapting Language Model to Domain Specific RAG\n",
            "http://arxiv.org/pdf/2403.10131v2\n",
            "Towards a RAG-based Summarization Agent for the Electron-Ion Collider\n",
            "http://arxiv.org/pdf/2403.15729v3\n",
            "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning:\n",
            "  A Comparative Study\n",
            "http://arxiv.org/pdf/2404.11792v2\n",
            "GRAG: Graph Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2405.16506v1\n",
            "Retrieval Meets Reasoning: Even High-school Textbook Knowledge Benefits\n",
            "  Multimodal Reasoning\n",
            "http://arxiv.org/pdf/2405.20834v1\n",
            "Leveraging Large Language Models for Web Scraping\n",
            "http://arxiv.org/pdf/2406.08246v1\n",
            "Refiner: Restructure Retrieval Content Efficiently to Advance\n",
            "  Question-Answering Capabilities\n",
            "http://arxiv.org/pdf/2406.11357v2\n",
            "InstructRAG: Instructing Retrieval-Augmented Generation with Explicit\n",
            "  Denoising\n",
            "http://arxiv.org/pdf/2406.13629v1\n",
            "Robust affine point matching via quadratic assignment on Grassmannians\n",
            "http://arxiv.org/pdf/2303.02698v4\n",
            "A Resource-efficient FIR Filter Design Based on an RAG Improved\n",
            "  Algorithm\n",
            "http://arxiv.org/pdf/2310.00912v2\n",
            "Context Tuning for Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2312.05708v1\n",
            "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research\n",
            "http://arxiv.org/pdf/2312.07559v2\n",
            "Beyond Extraction: Contextualising Tabular Data for Efficient\n",
            "  Summarisation by Language Models\n",
            "http://arxiv.org/pdf/2401.02333v3\n",
            "Bridging the Preference Gap between Retrievers and LLMs\n",
            "http://arxiv.org/pdf/2401.06954v2\n",
            "Corrective Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2401.15884v2\n",
            "Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for\n",
            "  Semantic Representations\n",
            "http://arxiv.org/pdf/2402.03053v1\n",
            "Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning\n",
            "http://arxiv.org/pdf/2402.08416v1\n",
            "From RAGs to riches: Using large language models to write documents for\n",
            "  clinical trials\n",
            "http://arxiv.org/pdf/2402.16406v1\n",
            "Enhancing Retrieval Processes for Language Generation with Augmented\n",
            "  Queries\n",
            "http://arxiv.org/pdf/2402.16874v1\n",
            "Superposition Prompting: Improving and Accelerating Retrieval-Augmented\n",
            "  Generation\n",
            "http://arxiv.org/pdf/2404.06910v1\n",
            "Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented\n",
            "  Generation\n",
            "http://arxiv.org/pdf/2404.12879v1\n",
            "IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection &\n",
            "  Correction Task On the Shoulders of Medical Agents\n",
            "http://arxiv.org/pdf/2404.15488v1\n",
            "Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered\n",
            "  Applications\n",
            "http://arxiv.org/pdf/2404.17196v1\n",
            "RaFe: Ranking Feedback Improves Query Rewriting for RAG\n",
            "http://arxiv.org/pdf/2405.14431v1\n",
            "ATM: Adversarial Tuning Multi-agent System Makes a Robust\n",
            "  Retrieval-Augmented Generator\n",
            "http://arxiv.org/pdf/2405.18111v2\n",
            "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for\n",
            "  Retrieval-Augmented Large Language Models\n",
            "http://arxiv.org/pdf/2405.19670v3\n",
            "SoccerRAG: Multimodal Soccer Information Retrieval via Natural Queries\n",
            "http://arxiv.org/pdf/2406.01273v1\n",
            "Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis\n",
            "  with Context-Aware Contrastive Language-Audio Pretraining\n",
            "http://arxiv.org/pdf/2406.03714v1\n",
            "RE-RAG: Improving Open-Domain QA Performance and Interpretability with\n",
            "  Relevance Estimator in Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2406.05794v2\n",
            "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2406.11460v1\n",
            "From RAGs to rich parameters: Probing how language models utilize\n",
            "  external knowledge over parametric information for factual queries\n",
            "http://arxiv.org/pdf/2406.12824v1\n",
            "DIRAS: Efficient LLM-Assisted Annotation of Document Relevance in\n",
            "  Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2406.14162v1\n",
            "Biomedical knowledge graph-optimized prompt generation for large\n",
            "  language models\n",
            "http://arxiv.org/pdf/2311.17330v2\n",
            "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented\n",
            "  Generation of Large Language Models\n",
            "http://arxiv.org/pdf/2402.07867v1\n",
            "First bromine doped cryogenic implosion at the National Ignition\n",
            "  Facility\n",
            "http://arxiv.org/pdf/2307.03730v1\n",
            "LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation\n",
            "http://arxiv.org/pdf/2310.04963v3\n",
            "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented\n",
            "  Generation and Soft-Prompting for Non-Specialist LLM Users\n",
            "http://arxiv.org/pdf/2311.05903v2\n",
            "Advancing TTP Analysis: Harnessing the Power of Encoder-Only and\n",
            "  Decoder-Only Language Models with Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2401.00280v2\n",
            "CorpusLM: Towards a Unified Language Model on Corpus for\n",
            "  Knowledge-Intensive Tasks\n",
            "http://arxiv.org/pdf/2402.01176v2\n",
            "Grounding Language Model with Chunking-Free In-Context Retrieval\n",
            "http://arxiv.org/pdf/2402.09760v1\n",
            "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge\n",
            "http://arxiv.org/pdf/2402.12352v1\n",
            "JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\n",
            "  and Professional Question Answering Capability\n",
            "http://arxiv.org/pdf/2402.17887v3\n",
            "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based\n",
            "  Search Engines\n",
            "http://arxiv.org/pdf/2402.19421v1\n",
            "Repoformer: Selective Retrieval for Repository-Level Code Completion\n",
            "http://arxiv.org/pdf/2403.10059v2\n",
            "CPR: Retrieval Augmented Generation for Copyright Protection\n",
            "http://arxiv.org/pdf/2403.18920v1\n",
            "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation\n",
            "http://arxiv.org/pdf/2404.06809v2\n",
            "LLMs Know What They Need: Leveraging a Missing Information Guided\n",
            "  Framework to Empower Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2404.14043v1\n",
            "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented\n",
            "  Large Language Models\n",
            "http://arxiv.org/pdf/2404.17897v1\n",
            "Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf\n",
            "  Disease Remediation\n",
            "http://arxiv.org/pdf/2405.01310v1\n",
            "ERAGent: Enhancing Retrieval-Augmented Language Models with Improved\n",
            "  Accuracy, Efficiency, and Personalization\n",
            "http://arxiv.org/pdf/2405.06683v1\n",
            "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning\n",
            "  Inner Monologues\n",
            "http://arxiv.org/pdf/2405.13021v1\n",
            "Augmenting Textual Generation via Topology Aware Retrieval\n",
            "http://arxiv.org/pdf/2405.17602v1\n",
            "Enhancing Noise Robustness of Retrieval-Augmented Language Models with\n",
            "  Adaptive Adversarial Training\n",
            "http://arxiv.org/pdf/2405.20978v1\n",
            "AMGPT: a Large Language Model for Contextual Querying in Additive\n",
            "  Manufacturing\n",
            "http://arxiv.org/pdf/2406.00031v1\n",
            "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large\n",
            "  Language Models\n",
            "http://arxiv.org/pdf/2406.11201v1\n",
            "Enhancing Biomedical Knowledge Retrieval-Augmented Generation with\n",
            "  Self-Rewarding Tree Search and Proximal Policy Optimization\n",
            "http://arxiv.org/pdf/2406.11258v1\n",
            "R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval\n",
            "  Augmented Large Language Models\n",
            "http://arxiv.org/pdf/2406.11681v1\n",
            "Unified Active Retrieval for Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2406.12534v3\n",
            "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge\n",
            "  Conflicts from Wikipedia\n",
            "http://arxiv.org/pdf/2406.13805v1\n",
            "Development of a Reliable and Accessible Caregiving Language Model\n",
            "  (CaLM)\n",
            "http://arxiv.org/pdf/2403.06857v1\n",
            "Structure Learning for Hybrid Bayesian Networks\n",
            "http://arxiv.org/pdf/2206.01356v2\n",
            "Formation of asymmetric arms in barred galaxies\n",
            "http://arxiv.org/pdf/2301.11385v1\n",
            "TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal\n",
            "  Prediction\n",
            "http://arxiv.org/pdf/2307.04642v2\n",
            "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for\n",
            "  Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2311.04177v1\n",
            "Minimizing Factual Inconsistency and Hallucination in Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2311.13878v1\n",
            "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\n",
            "http://arxiv.org/pdf/2312.05934v3\n",
            "Enhancing Large Language Model Performance To Answer Questions and\n",
            "  Extract Information More Accurately\n",
            "http://arxiv.org/pdf/2402.01722v1\n",
            "HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents\n",
            "  QA\n",
            "http://arxiv.org/pdf/2402.01767v1\n",
            "Enhancing Textbook Question Answering Task with Large Language Models\n",
            "  and Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2402.05128v2\n",
            "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning\n",
            "http://arxiv.org/pdf/2402.12177v4\n",
            "Assessing generalization capability of text ranking models in Polish\n",
            "http://arxiv.org/pdf/2402.14318v1\n",
            "ESE: Espresso Sentence Embeddings\n",
            "http://arxiv.org/pdf/2402.14776v2\n",
            "Federated Recommendation via Hybrid Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2403.04256v1\n",
            "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System\n",
            "  Co-design\n",
            "http://arxiv.org/pdf/2403.05676v1\n",
            "Retrieval augmented text-to-SQL generation for epidemiological question\n",
            "  answering using electronic health records\n",
            "http://arxiv.org/pdf/2403.09226v2\n",
            "JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented\n",
            "  Fine-Tuning\n",
            "http://arxiv.org/pdf/2403.11366v2\n",
            "LexDrafter: Terminology Drafting for Legislative Documents using\n",
            "  Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2403.16295v1\n",
            "Evaluation of Semantic Search and its Role in\n",
            "  Retrieved-Augmented-Generation (RAG) for Arabic Language\n",
            "http://arxiv.org/pdf/2403.18350v2\n",
            "Towards a Robust Retrieval-Based Summarization System\n",
            "http://arxiv.org/pdf/2403.19889v1\n",
            "Reducing hallucination in structured outputs via Retrieval-Augmented\n",
            "  Generation\n",
            "http://arxiv.org/pdf/2404.08189v1\n",
            "Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for\n",
            "  Text-to-SQL\n",
            "http://arxiv.org/pdf/2404.12560v1\n",
            "Studying Large Language Model Behaviors Under Realistic Knowledge\n",
            "  Conflicts\n",
            "http://arxiv.org/pdf/2404.16032v1\n",
            "GRAMMAR: Grounded and Modular Methodology for Assessment of\n",
            "  Domain-Specific Retrieval-Augmented Language Model\n",
            "http://arxiv.org/pdf/2404.19232v4\n",
            "Comparative Analysis of Retrieval Systems in the Real World\n",
            "http://arxiv.org/pdf/2405.02048v1\n",
            "From Questions to Insightful Answers: Building an Informed Chatbot for\n",
            "  University Resources\n",
            "http://arxiv.org/pdf/2405.08120v1\n",
            "KG-RAG: Bridging the Gap Between Knowledge and Creativity\n",
            "http://arxiv.org/pdf/2405.12035v1\n",
            "Certifiably Robust RAG against Retrieval Corruption\n",
            "http://arxiv.org/pdf/2405.15556v1\n",
            "RAGSys: Item-Cold-Start Recommender as RAG System\n",
            "http://arxiv.org/pdf/2405.17587v1\n",
            "Video Enriched Retrieval Augmented Generation Using Aligned Video\n",
            "  Captions\n",
            "http://arxiv.org/pdf/2405.17706v1\n",
            "Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical\n",
            "  Machine Reading Comprehension\n",
            "http://arxiv.org/pdf/2405.18682v1\n",
            "Two-layer retrieval augmented generation framework for low-resource\n",
            "  medical question-answering: proof of concept using Reddit data\n",
            "http://arxiv.org/pdf/2405.19519v1\n",
            "QUB-Cirdan at \"Discharge Me!\": Zero shot discharge letter generation by\n",
            "  open-source LLM\n",
            "http://arxiv.org/pdf/2406.00041v2\n",
            "cp: target 'Me!: Zero shot discharge letter generation by'$'\\n''  open-source LLM' is not a directory\n",
            "Mix-of-Granularity: Optimize the Chunking Granularity for\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2406.00456v1\n",
            "Luna: An Evaluation Foundation Model to Catch Language Model\n",
            "  Hallucinations with High Accuracy and Low Cost\n",
            "http://arxiv.org/pdf/2406.00975v2\n",
            "Demo: Soccer Information Retrieval via Natural Queries using SoccerRAG\n",
            "http://arxiv.org/pdf/2406.01280v1\n",
            "TSpec-LLM: An Open-source Dataset for LLM Understanding of 3GPP\n",
            "  Specifications\n",
            "http://arxiv.org/pdf/2406.01768v1\n",
            "Scholarly Question Answering using Large Language Models in the\n",
            "  NFDI4DataScience Gateway\n",
            "http://arxiv.org/pdf/2406.07257v1\n",
            "Ad Auctions for LLMs via Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2406.09459v1\n",
            "A Lightweight Framework for Adaptive Retrieval In Code Completion With\n",
            "  Critique Model\n",
            "http://arxiv.org/pdf/2406.10263v1\n",
            "Satyrn: A Platform for Analytics Augmented Generation\n",
            "http://arxiv.org/pdf/2406.12069v1\n",
            "Intermediate Distillation: Data-Efficient Distillation from Black-Box\n",
            "  LLMs for Information Retrieval\n",
            "http://arxiv.org/pdf/2406.12169v1\n",
            "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large\n",
            "  Language Models as Decision Makers\n",
            "http://arxiv.org/pdf/2406.12430v1\n",
            "R^2AG: Incorporating Retrieval Information into Retrieval Augmented\n",
            "  Generation\n",
            "http://arxiv.org/pdf/2406.13249v1\n",
            "Augmenting Query and Passage for Retrieval-Augmented Generation using\n",
            "  LLMs for Open-Domain Question Answering\n",
            "http://arxiv.org/pdf/2406.14277v1\n",
            "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world\n",
            "  Document Analysis\n",
            "http://arxiv.org/pdf/2406.15187v1\n",
            "Evaluating Quality of Answers for Retrieval-Augmented Generation: A\n",
            "  Strong LLM Is All You Need\n",
            "http://arxiv.org/pdf/2406.18064v1\n",
            "RAVEN: Multitask Retrieval Augmented Vision-Language Learning\n",
            "http://arxiv.org/pdf/2406.19150v1\n",
            "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented\n",
            "  Generation\n",
            "http://arxiv.org/pdf/2406.19215v1\n",
            "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2406.19251v1\n",
            "Spatio-Temporal driven Attention Graph Neural Network with Block\n",
            "  Adjacency matrix (STAG-NN-BA)\n",
            "http://arxiv.org/pdf/2303.14322v1\n",
            "REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific\n",
            "  Sentences using Public and Proprietary LLMs\n",
            "http://arxiv.org/pdf/2405.02228v2\n",
            "Ecce Signum: An R Package for Multivariate Signal Extraction and Time\n",
            "  Series Analysis\n",
            "http://arxiv.org/pdf/2201.02148v1\n",
            "Juggler's friezes\n",
            "http://arxiv.org/pdf/2208.09025v1\n",
            "Towards Comprehensive Vietnamese Retrieval-Augmented Generation and\n",
            "  Large Language Models\n",
            "http://arxiv.org/pdf/2403.01616v2\n",
            "Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An\n",
            "  Experimental Study and Quality Assessment Methods\n",
            "http://arxiv.org/pdf/2406.08582v1\n",
            "Debate as Optimization: Adaptive Conformal Prediction and Diverse\n",
            "  Retrieval for Event Extraction\n",
            "http://arxiv.org/pdf/2406.12197v1\n",
            "Adaptive Selection for Homogeneous Tools: An Instantiation in the RAG\n",
            "  Scenario\n",
            "http://arxiv.org/pdf/2406.12429v1\n",
            "Towards Retrieval Augmented Generation over Large Video Libraries\n",
            "http://arxiv.org/pdf/2406.14938v1\n",
            "Bayesian inference for link travel time correlation of a bus route\n",
            "http://arxiv.org/pdf/2202.09485v1\n",
            "DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation\n",
            "http://arxiv.org/pdf/2305.19787v2\n",
            "Intuitive or Dependent? Investigating LLMs' Behavior Style to\n",
            "  Conflicting Prompts\n",
            "http://arxiv.org/pdf/2309.17415v3\n",
            "GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using\n",
            "  Large Language Models\n",
            "http://arxiv.org/pdf/2310.06225v2\n",
            "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large\n",
            "  Language Model\n",
            "http://arxiv.org/pdf/2310.09089v2\n",
            "FABULA: Intelligence Report Generation Using Retrieval-Augmented\n",
            "  Narrative Construction\n",
            "http://arxiv.org/pdf/2310.13848v2\n",
            "AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using\n",
            "  Open-Source LLMs\n",
            "http://arxiv.org/pdf/2311.02775v3\n",
            "Chemist-X: Large Language Model-empowered Agent for Reaction Condition\n",
            "  Recommendation in Chemical Synthesis\n",
            "http://arxiv.org/pdf/2311.10776v5\n",
            "IAG: Induction-Augmented Generation Framework for Answering Reasoning\n",
            "  Questions\n",
            "http://arxiv.org/pdf/2311.18397v1\n",
            "NoMIRACL: Knowing When You Don't Know for Robust Multilingual\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2312.11361v2\n",
            "Graph database while computationally efficient filters out quickly the\n",
            "  ESG integrated equities in investment management\n",
            "http://arxiv.org/pdf/2401.07483v1\n",
            "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\n",
            "  Personalized Dialogue Systems\n",
            "http://arxiv.org/pdf/2401.13256v1\n",
            "REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records\n",
            "  Analysis via Large Language Models\n",
            "http://arxiv.org/pdf/2402.07016v1\n",
            "G-Retriever: Retrieval-Augmented Generation for Textual Graph\n",
            "  Understanding and Question Answering\n",
            "http://arxiv.org/pdf/2402.07630v3\n",
            "RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented\n",
            "  In-Context Learning in Multi-Modal Large Language Model\n",
            "http://arxiv.org/pdf/2402.10828v2\n",
            "Improving Assessment of Tutoring Practices using Retrieval-Augmented\n",
            "  Generation\n",
            "http://arxiv.org/pdf/2402.14594v1\n",
            "A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI\n",
            "  Judge\n",
            "http://arxiv.org/pdf/2402.17081v1\n",
            "Evaluating Very Long-Term Conversational Memory of LLM Agents\n",
            "http://arxiv.org/pdf/2402.17753v1\n",
            "DRAGIN: Dynamic Retrieval Augmented Generation based on the Information\n",
            "  Needs of Large Language Models\n",
            "http://arxiv.org/pdf/2403.10081v2\n",
            "Are Large Language Models Good at Utility Judgments?\n",
            "http://arxiv.org/pdf/2403.19216v2\n",
            "Reusable Architecture Growth for Continual Stereo Matching\n",
            "http://arxiv.org/pdf/2404.00360v1\n",
            "MedExpQA: Multilingual Benchmarking of Large Language Models for Medical\n",
            "  Question Answering\n",
            "http://arxiv.org/pdf/2404.05590v1\n",
            "Generative Information Retrieval Evaluation\n",
            "http://arxiv.org/pdf/2404.08137v2\n",
            "Spiral of Silence: How is Large Language Model Killing Information\n",
            "  Retrieval? -- A Case Study on Open Domain Question Answering\n",
            "http://arxiv.org/pdf/2404.10496v4\n",
            "Generating Test Scenarios from NL Requirements using Retrieval-Augmented\n",
            "  LLMs: An Industrial Study\n",
            "http://arxiv.org/pdf/2404.12772v1\n",
            "Investigating the prompt leakage effect and black-box defenses for\n",
            "  multi-turn LLM interactions\n",
            "http://arxiv.org/pdf/2404.16251v2\n",
            "Retrieval-Augmented Generation with Knowledge Graphs for Customer\n",
            "  Service Question Answering\n",
            "http://arxiv.org/pdf/2404.17723v2\n",
            "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural\n",
            "  Language Processing\n",
            "http://arxiv.org/pdf/2404.19543v1\n",
            "Automatic Retrieval-augmented Generation of 6G Network Specifications\n",
            "  for Use Cases\n",
            "http://arxiv.org/pdf/2405.03122v1\n",
            "A Method for Parsing and Vectorization of Semi-structured Data used in\n",
            "  Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2405.03989v2\n",
            "Towards Accurate and Efficient Document Analytics with Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2405.04674v1\n",
            "Evaluating Students' Open-ended Written Responses with LLMs: Using the\n",
            "  RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large\n",
            "http://arxiv.org/pdf/2405.05444v1\n",
            "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2405.14831v1\n",
            "EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling\n",
            "http://arxiv.org/pdf/2406.00036v1\n",
            "Chain of Agents: Large Language Models Collaborating on Long-Context\n",
            "  Tasks\n",
            "http://arxiv.org/pdf/2406.02818v1\n",
            "Corpus Poisoning via Approximate Greedy Gradient Descent\n",
            "http://arxiv.org/pdf/2406.05087v1\n",
            "Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt\n",
            "  LLMs for Dialogue\n",
            "http://arxiv.org/pdf/2406.06399v1\n",
            "STALL+: Boosting LLM-based Repository-level Code Completion with Static\n",
            "  Analysis\n",
            "http://arxiv.org/pdf/2406.10018v1\n",
            "Beyond Words: On Large Language Models Actionability in Mission-Critical\n",
            "  Risk Analysis\n",
            "http://arxiv.org/pdf/2406.10273v1\n",
            "RichRAG: Crafting Rich Responses for Multi-faceted Queries in\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2406.12566v2\n",
            "Towards Unlocking Insights from Logbooks Using AI\n",
            "http://arxiv.org/pdf/2406.12881v1\n",
            "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?\n",
            "http://arxiv.org/pdf/2406.13121v1\n",
            "FoRAG: Factuality-optimized Retrieval Augmented Generation for\n",
            "  Web-enhanced Long-form Question Answering\n",
            "http://arxiv.org/pdf/2406.13779v1\n",
            "CodeRAG-Bench: Can Retrieval Augment Code Generation?\n",
            "http://arxiv.org/pdf/2406.14497v1\n",
            "Relation Extraction with Fine-Tuned Large Language Models in Retrieval\n",
            "  Augmented Generation Frameworks\n",
            "http://arxiv.org/pdf/2406.14745v2\n",
            "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs\n",
            "http://arxiv.org/pdf/2406.15319v1\n",
            "Multi-step Knowledge Retrieval and Inference over Unstructured Data\n",
            "http://arxiv.org/pdf/2406.17987v1\n",
            "Poisoned LangChain: Jailbreak LLMs by LangChain\n",
            "http://arxiv.org/pdf/2406.18122v1\n",
            "A RAG-based Question Answering System Proposal for Understanding Islam:\n",
            "  MufassirQAS LLM\n",
            "http://arxiv.org/pdf/2401.15378v4\n",
            "Large Multi-Modal Models (LMMs) as Universal Foundation Models for\n",
            "  AI-Native Wireless Systems\n",
            "http://arxiv.org/pdf/2402.01748v2\n",
            "FACTOID: FACtual enTailment fOr hallucInation Detection\n",
            "http://arxiv.org/pdf/2403.19113v1\n",
            "Integrating A.I. in Higher Education: Protocol for a Pilot Study with\n",
            "  'SAMCares: An Adaptive Learning Hub'\n",
            "http://arxiv.org/pdf/2405.00330v1\n",
            "RAG-based Explainable Prediction of Road Users Behaviors for Automated\n",
            "  Driving using Knowledge Graphs and Large Language Models\n",
            "http://arxiv.org/pdf/2405.00449v1\n",
            "Hallucination-Free? Assessing the Reliability of Leading AI Legal\n",
            "  Research Tools\n",
            "http://arxiv.org/pdf/2405.20362v1\n",
            "The Fairness of Machine Learning in Insurance: New Rags for an Old Man?\n",
            "http://arxiv.org/pdf/2205.08112v1\n",
            "An Efficient, Scalable IO Framework for Sparse Data: larcv3\n",
            "http://arxiv.org/pdf/2209.04023v1\n",
            "PARAFAC2-based Coupled Matrix and Tensor Factorizations\n",
            "http://arxiv.org/pdf/2210.13054v1\n",
            "Multi-class Brain Tumor Segmentation using Graph Attention Network\n",
            "http://arxiv.org/pdf/2302.05598v1\n",
            "Huatuo-26M, a Large-scale Chinese Medical QA Dataset\n",
            "http://arxiv.org/pdf/2305.01526v1\n",
            "Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT\n",
            "  models\n",
            "http://arxiv.org/pdf/2305.03660v1\n",
            "Deep Equilibrium Object Detection\n",
            "http://arxiv.org/pdf/2308.09564v1\n",
            "Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs\n",
            "http://arxiv.org/pdf/2310.03812v1\n",
            "Making LLMs Worth Every Penny: Resource-Limited Text Classification in\n",
            "  Banking\n",
            "http://arxiv.org/pdf/2311.06102v1\n",
            "Deficiency of Large Language Models in Finance: An Empirical Examination\n",
            "  of Hallucination\n",
            "http://arxiv.org/pdf/2311.15548v1\n",
            "Novel Preprocessing Technique for Data Embedding in Engineering Code\n",
            "  Generation Using Large Language Model\n",
            "http://arxiv.org/pdf/2311.16267v2\n",
            "RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2311.16543v3\n",
            "How to Build an AI Tutor that Can Adapt to Any Course and Provide\n",
            "  Accurate Answers Using Large Language Model and Retrieval-Augmented\n",
            "  Generation\n",
            "http://arxiv.org/pdf/2311.17696v3\n",
            "Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP\n",
            "http://arxiv.org/pdf/2312.12430v3\n",
            "Question-Answering Based Summarization of Electronic Health Records\n",
            "  using Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2401.01469v1\n",
            "Code-Based English Models Surprising Performance on Chinese QA Pair\n",
            "  Extraction Task\n",
            "http://arxiv.org/pdf/2401.10286v3\n",
            "Explaining Autonomy: Enhancing Human-Robot Interaction through\n",
            "  Explanation Generation with Large Language Models\n",
            "http://arxiv.org/pdf/2402.04206v1\n",
            "Generative Representational Instruction Tuning\n",
            "http://arxiv.org/pdf/2402.09906v2\n",
            "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs\n",
            "  Miss\n",
            "http://arxiv.org/pdf/2402.10790v2\n",
            "Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?\n",
            "http://arxiv.org/pdf/2402.11035v2\n",
            "GenDec: A robust generative Question-decomposition method for Multi-hop\n",
            "  reasoning\n",
            "http://arxiv.org/pdf/2402.11166v1\n",
            "What Evidence Do Language Models Find Convincing?\n",
            "http://arxiv.org/pdf/2402.11782v1\n",
            "ARKS: Active Retrieval in Knowledge Soup for Code Generation\n",
            "http://arxiv.org/pdf/2402.12317v1\n",
            "Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware\n",
            "  Classification\n",
            "http://arxiv.org/pdf/2402.18502v1\n",
            "RNNs are not Transformers (Yet): The Key Bottleneck on In-context\n",
            "  Retrieval\n",
            "http://arxiv.org/pdf/2402.18510v3\n",
            "Neural Exec: Learning (and Learning from) Execution Triggers for Prompt\n",
            "  Injection Attacks\n",
            "http://arxiv.org/pdf/2403.03792v2\n",
            "FaaF: Facts as a Function for the evaluation of generated text\n",
            "http://arxiv.org/pdf/2403.03888v2\n",
            "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild\n",
            "http://arxiv.org/pdf/2403.04307v2\n",
            "RA-ISF: Learning to Answer and Understand from Retrieval Augmentation\n",
            "  via Iterative Self-Feedback\n",
            "http://arxiv.org/pdf/2403.06840v2\n",
            "Exploring the Capabilities and Limitations of Large Language Models in\n",
            "  the Electric Energy Sector\n",
            "http://arxiv.org/pdf/2403.09125v5\n",
            "Improving Medical Multi-modal Contrastive Learning with Expert\n",
            "  Annotations\n",
            "http://arxiv.org/pdf/2403.10153v1\n",
            "LLMs Instruct LLMs:An Extraction and Editing Method\n",
            "http://arxiv.org/pdf/2403.15736v1\n",
            "Octopus v2: On-device language model for super agent\n",
            "http://arxiv.org/pdf/2404.01744v5\n",
            "Prompts As Programs: A Structure-Aware Approach to Efficient\n",
            "  Compile-Time Prompt Optimization\n",
            "http://arxiv.org/pdf/2404.02319v1\n",
            "uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?\n",
            "http://arxiv.org/pdf/2404.02474v1\n",
            "IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe\n",
            "  Biomedical Natural Language Inference for Clinical Trials\n",
            "http://arxiv.org/pdf/2404.04510v1\n",
            "Enhancing Software-Related Information Extraction via Single-Choice\n",
            "  Question Answering with Large Language Models\n",
            "http://arxiv.org/pdf/2404.05587v2\n",
            "AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free\n",
            "  Information Retrieval\n",
            "http://arxiv.org/pdf/2404.06004v1\n",
            "Dimensionality Reduction in Sentence Transformer Vector Databases with\n",
            "  Fast Fourier Transform\n",
            "http://arxiv.org/pdf/2404.06278v1\n",
            "Onco-Retriever: Generative Classifier for Retrieval of EHR Records in\n",
            "  Oncology\n",
            "http://arxiv.org/pdf/2404.06680v1\n",
            "LLMs in Biomedicine: A study on clinical Named Entity Recognition\n",
            "http://arxiv.org/pdf/2404.07376v1\n",
            "Position Engineering: Boosting Large Language Models through Positional\n",
            "  Information Manipulation\n",
            "http://arxiv.org/pdf/2404.11216v1\n",
            "MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory\n",
            "http://arxiv.org/pdf/2404.11672v1\n",
            "Retrieval-Augmented Audio Deepfake Detection\n",
            "http://arxiv.org/pdf/2404.13892v2\n",
            "GAIA: A General AI Assistant for Intelligent Accelerator Operations\n",
            "http://arxiv.org/pdf/2405.01359v1\n",
            "Remote Diffusion\n",
            "http://arxiv.org/pdf/2405.04717v1\n",
            "Automated Conversion of Static to Dynamic Scheduler via Natural Language\n",
            "http://arxiv.org/pdf/2405.06697v1\n",
            "PyZoBot: A Platform for Conversational Information Extraction and\n",
            "  Synthesis from Curated Zotero Reference Libraries through Advanced\n",
            "  Retrieval-Augmented Generation\n",
            "http://arxiv.org/pdf/2405.07963v1\n",
            "Exploring the Potential of Large Language Models for Automation in\n",
            "  Technical Customer Service\n",
            "http://arxiv.org/pdf/2405.09161v2\n",
            "FinTextQA: A Dataset for Long-form Financial Question Answering\n",
            "http://arxiv.org/pdf/2405.09980v1\n",
            "Retrieving and Refining: A Hybrid Framework with Large Language Models\n",
            "  for Rare Disease Identification\n",
            "http://arxiv.org/pdf/2405.10440v1\n",
            "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph\n",
            "  Question Answering\n",
            "http://arxiv.org/pdf/2405.13873v1\n",
            "SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS\n",
            "  Design Generation\n",
            "http://arxiv.org/pdf/2405.16072v2\n",
            "Large Language Models (LLMs): Deployment, Tokenomics and Sustainability\n",
            "http://arxiv.org/pdf/2405.17147v1\n",
            "ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with\n",
            "  LLM-Enhanced Cardiological Text\n",
            "http://arxiv.org/pdf/2405.19366v1\n",
            "Unlearning Climate Misinformation in Large Language Models\n",
            "http://arxiv.org/pdf/2405.19563v1\n",
            "DepsRAG: Towards Managing Software Dependencies using Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2405.20455v3\n",
            "COS-Mix: Cosine Similarity and Distance Fusion for Improved Information\n",
            "  Retrieval\n",
            "http://arxiv.org/pdf/2406.00638v1\n",
            "Recent advances in text embedding: A Comprehensive Review of\n",
            "  Top-Performing Methods on the MTEB Benchmark\n",
            "http://arxiv.org/pdf/2406.01607v2\n",
            "Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning\n",
            "  Compressor\n",
            "http://arxiv.org/pdf/2406.02266v1\n",
            "Analyzing Temporal Complex Events with Large Language Models? A\n",
            "  Benchmark towards Temporal, Long Context Understanding\n",
            "http://arxiv.org/pdf/2406.02472v1\n",
            "Evaluating the Retrieval Component in LLM-Based Question Answering\n",
            "  Systems\n",
            "http://arxiv.org/pdf/2406.06458v1\n",
            "TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation\n",
            "  and LLMs\n",
            "http://arxiv.org/pdf/2406.07053v1\n",
            "Battling Botpoop using GenAI for Higher Education: A Study of a\n",
            "  Retrieval Augmented Generation Chatbots Impact on Learning\n",
            "http://arxiv.org/pdf/2406.07796v2\n",
            "Blowfish: Topological and statistical signatures for quantifying\n",
            "  ambiguity in semantic search\n",
            "http://arxiv.org/pdf/2406.07990v1\n",
            "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from\n",
            "  Corporate Climate Disclosures\n",
            "http://arxiv.org/pdf/2406.09818v1\n",
            "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation\n",
            "  Detection Using In-Context Learning based on Emotional Information\n",
            "http://arxiv.org/pdf/2406.11093v1\n",
            "TIFG: Text-Informed Feature Generation with Large Language Models\n",
            "http://arxiv.org/pdf/2406.11177v1\n",
            "Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text\n",
            "  Understanding\n",
            "http://arxiv.org/pdf/2406.12331v1\n",
            "PARAFAC2-based Coupled Matrix and Tensor Factorizations with Constraints\n",
            "http://arxiv.org/pdf/2406.12338v1\n",
            "Identifying Performance-Sensitive Configurations in Software Systems\n",
            "  through Code Analysis with LLM Agents\n",
            "http://arxiv.org/pdf/2406.12806v1\n",
            "Improving Zero-shot LLM Re-Ranker with Risk Minimization\n",
            "http://arxiv.org/pdf/2406.13331v1\n",
            "Thread: A Logic-Based Data Organization Paradigm for How-To Question\n",
            "  Answering with Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2406.13372v1\n",
            "TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction\n",
            "  in RAG-based Crowdsourcing Systems\n",
            "http://arxiv.org/pdf/2406.14825v2\n",
            "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework\n",
            "  for Knowledge-Intensive LLM Generation\n",
            "http://arxiv.org/pdf/2406.14979v1\n",
            "Harnessing Knowledge Retrieval with Large Language Models for Clinical\n",
            "  Report Error Correction\n",
            "http://arxiv.org/pdf/2406.15045v1\n",
            "FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy\n",
            "  in Large Language Models\n",
            "http://arxiv.org/pdf/2406.16167v1\n",
            "Context-augmented Retrieval: A Novel Framework for Fast Information\n",
            "  Retrieval based Response Generation using Large Language Model\n",
            "http://arxiv.org/pdf/2406.16383v1\n",
            "Attention Instruction: Amplifying Attention in the Middle via Prompting\n",
            "http://arxiv.org/pdf/2406.17095v1\n",
            "LumberChunker: Long-Form Narrative Document Segmentation\n",
            "http://arxiv.org/pdf/2406.17526v1\n",
            "Assessing the Effectiveness of LLMs in Android Application Vulnerability\n",
            "  Analysis\n",
            "http://arxiv.org/pdf/2406.18894v1\n",
            "Which Neurons Matter in IR? Applying Integrated Gradients-based Methods\n",
            "  to Understand Cross-Encoders\n",
            "http://arxiv.org/pdf/2406.19309v1\n",
            "Re2G: Retrieve, Rerank, Generate\n",
            "http://arxiv.org/pdf/2207.06300v1\n",
            "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question\n",
            "  Answering over Images and Text\n",
            "http://arxiv.org/pdf/2210.02928v2\n",
            "Retrieval Augmented Generation using Engineering Design Knowledge\n",
            "http://arxiv.org/pdf/2307.06985v9\n",
            "VulLibGen: Generating Names of Vulnerability-Affected Packages via a\n",
            "  Large Language Model\n",
            "http://arxiv.org/pdf/2308.04662v3\n",
            "Model-Free Large-Scale Cloth Spreading With Mobile Manipulation: Initial\n",
            "  Feasibility Study\n",
            "http://arxiv.org/pdf/2308.10401v1\n",
            "Chatmap : Large Language Model Interaction with Cartographic Data\n",
            "http://arxiv.org/pdf/2310.01429v1\n",
            "Glitter or Gold? Deriving Structured Insights from Sustainability\n",
            "  Reports via Large Language Models\n",
            "http://arxiv.org/pdf/2310.05628v3\n",
            "Detailing secondary frontal bore of internal tides breaking above\n",
            "  deep-ocean topography\n",
            "http://arxiv.org/pdf/2311.07976v1\n",
            "SenTest: Evaluating Robustness of Sentence Encoders\n",
            "http://arxiv.org/pdf/2311.17722v1\n",
            "NDSEARCH: Accelerating Graph-Traversal-Based Approximate Nearest\n",
            "  Neighbor Search through Near Data Processing\n",
            "http://arxiv.org/pdf/2312.03141v2\n",
            "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of\n",
            "  Large Language Models for Effective Tool Use\n",
            "http://arxiv.org/pdf/2312.04455v4\n",
            "Context-aware Decoding Reduces Hallucination in Query-focused\n",
            "  Summarization\n",
            "http://arxiv.org/pdf/2312.14335v2\n",
            "HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and\n",
            "  Reliable Medical LLMs Responses\n",
            "http://arxiv.org/pdf/2312.15883v2\n",
            "ESGReveal: An LLM-based approach for extracting structured data from ESG\n",
            "  reports\n",
            "http://arxiv.org/pdf/2312.17264v1\n",
            "A Reliable Knowledge Processing Framework for Combustion Science using\n",
            "  Foundation Models\n",
            "http://arxiv.org/pdf/2401.00544v2\n",
            "De-Hallucinator: Mitigating LLM Hallucinations in Code Generation Tasks\n",
            "  via Iterative Grounding\n",
            "http://arxiv.org/pdf/2401.01701v3\n",
            "Interactive AI with Retrieval-Augmented Generation for Next Generation\n",
            "  Networking\n",
            "http://arxiv.org/pdf/2401.11391v1\n",
            "Evaluating and Enhancing Large Language Models Performance in\n",
            "  Domain-specific Medicine: Osteoarthritis Management with DocOA\n",
            "http://arxiv.org/pdf/2401.12998v1\n",
            "LLaMP: Large Language Model Made Powerful for High-fidelity Materials\n",
            "  Knowledge Retrieval and Distillation\n",
            "http://arxiv.org/pdf/2401.17244v2\n",
            "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System\n",
            "http://arxiv.org/pdf/2402.00746v6\n",
            "LitLLM: A Toolkit for Scientific Literature Review\n",
            "http://arxiv.org/pdf/2402.01788v1\n",
            "Retrieval Augmented End-to-End Spoken Dialog Models\n",
            "http://arxiv.org/pdf/2402.01828v1\n",
            "How well do LLMs cite relevant medical references? An evaluation\n",
            "  framework and analyses\n",
            "http://arxiv.org/pdf/2402.02008v1\n",
            "Generative AI in the Construction Industry: A State-of-the-art Analysis\n",
            "http://arxiv.org/pdf/2402.09939v1\n",
            "PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal\n",
            "  Question-Answering\n",
            "http://arxiv.org/pdf/2402.11034v2\n",
            "Where is the answer? Investigating Positional Bias in Language Model\n",
            "  Knowledge Extraction\n",
            "http://arxiv.org/pdf/2402.12170v2\n",
            "Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based\n",
            "  Question Answering with Domain Hybrid Data\n",
            "http://arxiv.org/pdf/2402.12869v2\n",
            "MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems\n",
            "  in LLM Augmented Generation\n",
            "http://arxiv.org/pdf/2402.14480v1\n",
            "DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy\n",
            "  in Large-Scale Databases\n",
            "http://arxiv.org/pdf/2403.00872v1\n",
            "From human experts to machines: An LLM supported approach to ontology\n",
            "  and knowledge graph construction\n",
            "http://arxiv.org/pdf/2403.08345v1\n",
            "S3LLM: Large-Scale Scientific Software Understanding with LLMs using\n",
            "  Source, Metadata, and Document\n",
            "http://arxiv.org/pdf/2403.10588v1\n",
            "AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented\n",
            "  Stock-Chain Framework\n",
            "http://arxiv.org/pdf/2403.12582v1\n",
            "Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\n",
            "  Models through Question Complexity\n",
            "http://arxiv.org/pdf/2403.14403v2\n",
            "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering\n",
            "http://arxiv.org/pdf/2403.19116v1\n",
            "Dialectical Alignment: Resolving the Tension of 3H and Security Threats\n",
            "  of LLMs\n",
            "http://arxiv.org/pdf/2404.00486v1\n",
            "A Comparison of Methods for Evaluating Generative IR\n",
            "http://arxiv.org/pdf/2404.04044v2\n",
            "RAR-b: Reasoning as Retrieval Benchmark\n",
            "http://arxiv.org/pdf/2404.06347v2\n",
            "Towards Robustness of Text-to-Visualization Translation against Lexical\n",
            "  and Phrasal Variability\n",
            "http://arxiv.org/pdf/2404.07135v2\n",
            "Generative AI Agent for Next-Generation MIMO Design: Fundamentals,\n",
            "  Challenges, and Vision\n",
            "http://arxiv.org/pdf/2404.08878v1\n",
            "Interactive Generative AI Agents for Satellite Networks through a\n",
            "  Mixture of Experts Transmission\n",
            "http://arxiv.org/pdf/2404.09134v1\n",
            "Cross-Data Knowledge Graph Construction for LLM-enabled Educational\n",
            "  Question-Answering System: A~Case~Study~at~HCMUT\n",
            "http://arxiv.org/pdf/2404.09296v1\n",
            "Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations\n",
            "http://arxiv.org/pdf/2404.10779v1\n",
            "LongEmbed: Extending Embedding Models for Long Context Retrieval\n",
            "http://arxiv.org/pdf/2404.12096v2\n",
            "Generative AI for Low-Carbon Artificial Intelligence of Things\n",
            "http://arxiv.org/pdf/2404.18077v1\n",
            "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n",
            "  using Large Language Model for Stock Performance Prediction\n",
            "http://arxiv.org/pdf/2404.18470v1\n",
            "Self-Improving Customer Review Response Generation Based on LLMs\n",
            "http://arxiv.org/pdf/2405.03845v1\n",
            "Prompt-based Code Completion via Multi-Retrieval Augmented Generation\n",
            "http://arxiv.org/pdf/2405.07530v1\n",
            "Generative AI and Large Language Models for Cyber Security: All Insights\n",
            "  You Need\n",
            "http://arxiv.org/pdf/2405.12750v1\n",
            "Can Github issues be solved with Tree Of Thoughts?\n",
            "http://arxiv.org/pdf/2405.13057v1\n",
            "TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in\n",
            "  Large Language Models\n",
            "http://arxiv.org/pdf/2405.13401v3\n",
            "G3: An Effective and Adaptive Framework for Worldwide Geolocalization\n",
            "  Using Large Multi-Modality Models\n",
            "http://arxiv.org/pdf/2405.14702v1\n",
            "Exploiting the Layered Intrinsic Dimensionality of Deep Models for\n",
            "  Practical Adversarial Training\n",
            "http://arxiv.org/pdf/2405.17130v1\n",
            "Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual\n",
            "  Performance in LLMs\n",
            "http://arxiv.org/pdf/2405.18359v1\n",
            "Similarity is Not All You Need: Endowing Retrieval Augmented Generation\n",
            "  with Multi Layered Thoughts\n",
            "http://arxiv.org/pdf/2405.19893v1\n",
            "Designing an Evaluation Framework for Large Language Models in Astronomy\n",
            "  Research\n",
            "http://arxiv.org/pdf/2405.20389v1\n",
            "Exploring Backdoor Attacks against Large Language Model-based Decision\n",
            "  Making\n",
            "http://arxiv.org/pdf/2405.20774v1\n",
            "Superhuman performance in urology board questions by an explainable\n",
            "  large language model enabled for context integration of the European\n",
            "  Association of Urology guidelines: the UroBot study\n",
            "http://arxiv.org/pdf/2406.01428v2\n",
            "UniOQA: A Unified Framework for Knowledge Graph Question Answering with\n",
            "  Large Language Models\n",
            "http://arxiv.org/pdf/2406.02110v1\n",
            "RATT: A Thought Structure for Coherent and Correct LLM Reasoning\n",
            "http://arxiv.org/pdf/2406.02746v2\n",
            "Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge\n",
            "  Devices\n",
            "http://arxiv.org/pdf/2406.03777v2\n",
            "A + B: A General Generator-Reader Framework for Optimizing LLMs to\n",
            "  Unleash Synergy Potential\n",
            "http://arxiv.org/pdf/2406.03963v1\n",
            "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance\n",
            "  Assessor\n",
            "http://arxiv.org/pdf/2406.06519v1\n",
            "RAG-based Crowdsourcing Task Decomposition via Masked Contrastive\n",
            "  Learning with Prompts\n",
            "http://arxiv.org/pdf/2406.06577v1\n",
            "Artificial Intelligence as the New Hacker: Developing Agents for\n",
            "  Offensive Security\n",
            "http://arxiv.org/pdf/2406.07561v1\n",
            "We Have a Package for You! A Comprehensive Analysis of Package\n",
            "  Hallucinations by Code Generating LLMs\n",
            "http://arxiv.org/pdf/2406.10279v1\n",
            "Current state of LLM Risks and AI Guardrails\n",
            "http://arxiv.org/pdf/2406.12934v1\n",
            "Found in the Middle: Calibrating Positional Attention Bias Improves Long\n",
            "  Context Utilization\n",
            "http://arxiv.org/pdf/2406.16008v1\n",
            "Graph-Augmented LLMs for Personalized Health Insights: A Case Study in\n",
            "  Sleep Analysis\n",
            "http://arxiv.org/pdf/2406.16252v2\n",
            "CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented\n",
            "  Analysis Generation\n",
            "http://arxiv.org/pdf/2406.17186v2\n",
            "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended\n",
            "  Multi-Doc QA\n",
            "http://arxiv.org/pdf/2406.17419v1\n",
            "AI-native Memory: A Pathway from LLMs Towards AGI\n",
            "http://arxiv.org/pdf/2406.18312v1\n",
            "The Impacts of Neutron-Star Structure and Base Heating on Type I X-Ray\n",
            "  Bursts and Code Comparison\n",
            "http://arxiv.org/pdf/2304.07197v1\n",
            "Merging-Diverging Hybrid Transformer Networks for Survival Prediction in\n",
            "  Head and Neck Cancer\n",
            "http://arxiv.org/pdf/2307.03427v1\n",
            "Dynamic Retrieval Augmented Generation of Ontologies using Artificial\n",
            "  Intelligence (DRAGON-AI)\n",
            "http://arxiv.org/pdf/2312.10904v2\n",
            "Privacy-Preserved Neural Graph Databases\n",
            "http://arxiv.org/pdf/2312.15591v5\n",
            "DB-GPT: Empowering Database Interactions with Private Large Language\n",
            "  Models\n",
            "http://arxiv.org/pdf/2312.17449v2\n",
            "Improving Medical Reasoning through Retrieval and Self-Reflection with\n",
            "  Retrieval-Augmented Large Language Models\n",
            "http://arxiv.org/pdf/2401.15269v3\n",
            "CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation\n",
            "  for Evaluating LLMs in Cybersecurity Knowledge\n",
            "http://arxiv.org/pdf/2402.07688v2\n",
            "FinBen: A Holistic Financial Benchmark for Large Language Models\n",
            "http://arxiv.org/pdf/2402.12659v2\n",
            "AesopAgent: Agent-driven Evolutionary System on Story-to-Video\n",
            "  Production\n",
            "http://arxiv.org/pdf/2403.07952v1\n",
            "Generation of Asset Administration Shell with Large Language Model\n",
            "  Agents: Toward Semantic Interoperability in Digital Twins in the Context of\n",
            "  Industry 4.0\n",
            "http://arxiv.org/pdf/2403.17209v4\n",
            "ClashEval: Quantifying the tug-of-war between an LLM's internal prior\n",
            "  and external evidence\n",
            "http://arxiv.org/pdf/2404.10198v2\n",
            "Characterizing the Dilemma of Performance and Index Size in\n",
            "  Billion-Scale Vector Search and Breaking It with Second-Tier Memory\n",
            "http://arxiv.org/pdf/2405.03267v2\n",
            "Perception of Knowledge Boundary for Large Language Models through\n",
            "  Semi-open-ended Question Answering\n",
            "http://arxiv.org/pdf/2405.14383v1\n",
            "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n",
            "  Knowledge Fusion\n",
            "http://arxiv.org/pdf/2405.16444v2\n",
            "RAG-Enhanced Commit Message Generation\n",
            "http://arxiv.org/pdf/2406.05514v2\n",
            "Diagnosis Assistant for Liver Cancer Utilizing a Large Language Model\n",
            "  with Three Types of Knowledge\n",
            "http://arxiv.org/pdf/2406.18039v1\n",
            "Weaver: Foundation Models for Creative Writing\n",
            "http://arxiv.org/pdf/2401.17268v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parsing and Chunking"
      ],
      "metadata": {
        "id": "kmNwnYfuszRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIVyweihBxdx",
        "outputId": "7efd8e5c-ee2a-4757-90c8-150233596596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory setup\n",
        "pdf_directory = '/content/drive/MyDrive/RAG_Papers'\n",
        "output_directory= '/content/Parsed_Files'\n",
        "zip_file= 'cleaned_texts.zip'\n",
        "# Ensure the output directory exists\n"
      ],
      "metadata": {
        "id": "zfUdOm1EE3Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Parsing**\n",
        "Parsing was done using llmparser**"
      ],
      "metadata": {
        "id": "HJyUlpYDMp_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U llama-index --upgrade -q\n",
        "!pip install llama-parse -q"
      ],
      "metadata": {
        "id": "apMy4oWTMuSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-huggingface -q -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHqcI-Dp0xNt",
        "outputId": "37e1f62d-0824-4ba3-9741-46a8778e7779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set your API key (ensure this is done before initializing the parser)\n",
        "os.environ['LLAMA_API_KEY'] = 'llx-78xMTtNuY8KS0UxiCo1jh8azXZlwWONmUE8eLZW77bNKNNdD'\n",
        "print(os.getenv('LLAMA_API_KEY'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70RECXc3nc_n",
        "outputId": "46c7edcd-8f5c-43cd-fea7-44fa4208ad2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llx-78xMTtNuY8KS0UxiCo1jh8azXZlwWONmUE8eLZW77bNKNNdD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_parse import LlamaParse\n",
        "\n",
        "parser = LlamaParse(\n",
        "    api_key=os.getenv('LLAMA_API_KEY'),\n",
        "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
        "    num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
        "    verbose=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "iUlFpMBLepAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first 45 PDF files\n",
        "files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')][:30]\n",
        "file_paths = [os.path.join(pdf_directory, file) for file in files]\n",
        "\n",
        "# Parse the PDFs\n",
        "documents = parser.load_data(file_paths)  # Load and parse documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O78HHrMvgmea",
        "outputId": "9be69c88-e977-4e2a-da2b-9fa55955979c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing files: 100%|██████████| 30/30 [07:11<00:00, 14.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The documents returned represent the parsed content of the PDFs. The length of documents being 468 instead of 30 indicates that LlamaParse returns multiple segments or chunks per document rather than a single document. This chunking is essential for managing large documents more effectively and can help in scenarios like semantic chunking and retrieval augmentation.###"
      ],
      "metadata": {
        "id": "m3MoEji7uW3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc2mtVtFhIhz",
        "outputId": "6d9e4af5-7887-49a7-8805-6f4a3ef0ad6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleansing**"
      ],
      "metadata": {
        "id": "XNMMkVawEPP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import unicodedata\n",
        "import re\n",
        "from google.colab import files as colab_files\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Replace multiple consecutive spaces with a single space.\"\"\"\n",
        "    text = re.sub(r'\\nPage \\d+\\n', '', text)  # Example pattern for page numbers\n",
        "    text = re.sub(r'\\n(\\w+\\s+){1,4}\\d+\\n', '', text)  # Example pattern for headers/footers\n",
        "    text = unicodedata.normalize('NFKC', text)  # Normalize unicode characters\n",
        "    text = ' '.join(text.split())  # Remove excessive whitespace and newlines\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove inline citations (e.g., [1])\n",
        "    text = re.sub(r'\\(\\w+ et al\\., \\d{4}\\)', '', text)  # Remove inline citations (e.g., (Smith et al., 2020))\n",
        "    text = re.sub(r'References\\n.*', '', text, flags=re.DOTALL)  # Remove references section\n",
        "    return text\n",
        "\n",
        "def save_text_to_file(filename, text, directory):\n",
        "    \"\"\"Save the cleaned text to a file.\"\"\"\n",
        "    with open(os.path.join(directory, filename.replace('.pdf', '.txt')), 'w', encoding='utf-8') as file:\n",
        "        file.write(text)\n",
        "\n",
        "# Process and save PDFs\n",
        "for index, doc in enumerate(documents):\n",
        "    cleaned_text = preprocess_text(doc.get_text())  # Use the appropriate method to get text\n",
        "    save_text_to_file(f'document_{index}.txt', cleaned_text, output_directory)\n",
        "\n",
        "def zip_files(directory, zip_filename):\n",
        "    \"\"\"Zip all files in a directory.\"\"\"\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file), arcname=file)\n",
        "\n",
        "# Zip the processed files\n",
        "zip_files(output_directory, zip_file)\n",
        "\n",
        "print(f\"Processed and zipped files are saved at {zip_file}\")\n",
        "files.download(zip_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WjUauZPTmnZf",
        "outputId": "4dfe4557-afe5-4eae-f582-f7ee3dba40bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and zipped files are saved at cleaned_texts.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b7e84461-b005-4d4a-8096-c3b20951f65e\", \"cleaned_texts.zip\", 732492)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semantic Chunking"
      ],
      "metadata": {
        "id": "UAc53o9v0o1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__kxedZQ0rXj",
        "outputId": "d3d8c96a-8677-4f3a-bab6-891752d3eb95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters for semantic chunking\n",
        "buffer_size = 1\n",
        "breakpoint_percentile_threshold = 95\n",
        "\n",
        "# Function to chunk text using SemanticSplitterNodeParser\n",
        "def chunk_text_with_semantic_splitter(document, buffer_size, breakpoint_percentile_threshold, embed_model):\n",
        "    splitter = SemanticSplitterNodeParser(\n",
        "        buffer_size=buffer_size,\n",
        "        breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
        "        embed_model=embed_model\n",
        "        # include_metadata=True,  # Include metadata\n",
        "    )\n",
        "    nodes = splitter.get_nodes_from_documents(document)\n",
        "    return nodes\n",
        "\n",
        "# Create semantic chunks\n",
        "semantic_chunks = chunk_text_with_semantic_splitter(documents, buffer_size, breakpoint_percentile_threshold, embed_model)\n"
      ],
      "metadata": {
        "id": "DHS9ZUEG1rTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of semantic chunks created are:\", len(semantic_chunks))\n",
        "print(semantic_chunks[1].get_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfXM-hA6CUOo",
        "outputId": "c49788cc-149e-492d-8a3c-8963b61e9aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of semantic chunks created are: 1232\n",
            "<bound method TextNode.get_content of TextNode(id_='1593d631-cc17-4947-9306-4e9c8a8cdb90', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0a6be288-8f2e-435c-a681-83c89a3c0a3f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='27e5f0b23fe262224e232de1bc845d7e12d10b2c2245479d0718a07638756fc5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d769e7e9-bc9d-4ead-bcb0-8c0c97a0c12c', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='89ca1050aee3457b01f8abf90515a6156fa6c9a3777e9ca3269e6c090c8cffc6')}, text='A financial analyst might query, \"Which company among Google, Apple, and Nvidia reported the largest profit margins in their third-quarter reports for 2023?\" or inquire about a specific company’s performance over time, such as \"How does Apple’s sales trend look over the past three years?\" These queries require evidence from multiple documents to formulate an answer. Due to the multifaceted nature of such queries, involving information from various sources, traditional similarity matching methods like cosine similarity between query and financial data may not be sufficient.\\n\\nFigure 1: RAG with multi-hop query.', mimetype='text/plain', start_char_idx=2916, end_char_idx=3532, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the chunks**"
      ],
      "metadata": {
        "id": "cHKaaHo1EIYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Function to save nodes with metadata\n",
        "def save_node_with_metadata(node, index, directory):\n",
        "    # Prepare the content to save\n",
        "    content = {\n",
        "        'text': node.get_content()\n",
        "    }\n",
        "    # Save as JSON file\n",
        "    with open(os.path.join(directory, f'chunk_{index}.json'), 'w', encoding='utf-8') as file:\n",
        "        json.dump(content, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Save all nodes\n",
        "output_directory = '/content/Semantic_Chunks'  # Update with your path\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for index, node in enumerate(semantic_chunks):\n",
        "    save_node_with_metadata(node, index, output_directory)\n"
      ],
      "metadata": {
        "id": "2Mc_SC4sDNuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Function to zip the files\n",
        "def zip_files(directory, zip_filename):\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file), arcname=file)\n",
        "\n",
        "# Define the zip filename\n",
        "zip_filename = '/content/semantic_chunks.zip'  # Ensure it ends with .zip\n",
        "\n",
        "# Zip the files\n",
        "zip_files(output_directory, zip_filename)\n",
        "colab_files.download(zip_filename)\n",
        "\n",
        "print(f\"Processed and zipped files are saved at {zip_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EHSMJaVIDpZ3",
        "outputId": "a7d67f7a-0e9c-4e51-c7a5-e2f9cee3bf66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_406676db-32bb-4ab7-803b-0386ccd96898\", \"semantic_chunks.zip\", 928276)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and zipped files are saved at /content/semantic_chunks.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Qdrant to store the embeddings"
      ],
      "metadata": {
        "id": "T2ULoZNtC83X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers -q"
      ],
      "metadata": {
        "id": "LW26X6NXZ5IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "U42x0SAphBbU",
        "outputId": "f5acc17f-df25-4c9a-d39a-79ff93e8bbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
            "  Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Installing collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.2\n",
            "    Uninstalling transformers-4.40.2:\n",
            "      Successfully uninstalled transformers-4.40.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastembed 0.2.7 requires huggingface-hub<0.21,>=0.20, but you have huggingface-hub 0.23.4 which is incompatible.\n",
            "fastembed 0.2.7 requires tokenizers<0.16,>=0.15, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.23.4 transformers-4.42.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "c91e89f830794f08bccd8fabbe10ae78"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-huggingface -q\n",
        "%pip install llama-index-embeddings-instructor -q"
      ],
      "metadata": {
        "id": "2WWX9PacNpML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-1f68UbXhhw",
        "outputId": "7ee679d3-a411-4d48-9772-8c7b4c8bee7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U qdrant_client fastembed -q\n",
        "# !pip install -U fastembed"
      ],
      "metadata": {
        "id": "vvlBP_ieMyYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-fastembed -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnErUoySYMV4",
        "outputId": "03a13417-1065-4b40-b2c6-9ed841f86c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.41.2 requires huggingface-hub<1.0,>=0.23.0, but you have huggingface-hub 0.20.3 which is incompatible.\n",
            "transformers 4.41.2 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpMjujl4jvhd",
        "outputId": "4f836edd-2cd2-48a5-f883-31071f762538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.2 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "fastembed 0.2.7 requires huggingface-hub<0.21,>=0.20, but you have huggingface-hub 0.23.4 which is incompatible.\n",
            "fastembed 0.2.7 requires tokenizers<0.16,>=0.15, but you have tokenizers 0.19.1 which is incompatible.\n",
            "google-api-core 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-aiplatform 1.57.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigtable 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow -q"
      ],
      "metadata": {
        "id": "XDIShH9vk97q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-vector-stores-qdrant -q"
      ],
      "metadata": {
        "id": "_ztqaRtbYDtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import qdrant_client\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "# from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n"
      ],
      "metadata": {
        "id": "40U97vC5DDj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loads BAAI/bge-small-en\n",
        "# embed_model = HuggingFaceEmbedding()\n",
        "\n",
        "# loads BAAI/bge-small-en-v1.5\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3jgHgRJJ9A4",
        "outputId": "c7030d68-180e-4c90-ab53-264dc6311d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a dataset from the Hugging Face Hub\n",
        "# For example, the 'ag_news' dataset\n",
        "dataset = load_dataset('Areeb-02/30rag_papers_qa_dataset', split = 'train')\n",
        "\n",
        "# Inspect the dataset\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fL_q0SUi3r8",
        "outputId": "d9a4b47d-4c5a-4982-e25e-012f92ad9502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['instruction', 'system_prompt', '3B_q1', '3B_a1', '3B_q2', '3B_a2', '7B_q1', '7B_a1', '7B_q2', '7B_a2'],\n",
            "    num_rows: 1010\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: getpass for qdrant cloud api\n",
        "\n",
        "import getpass\n",
        "qdrant_cloud_api_key = getpass.getpass(prompt='Qdrant Cloud API Key:')\n",
        "os.environ['QDRANT_CLOUD_API_KEY'] = qdrant_cloud_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiI9tgOJmxF0",
        "outputId": "a7757a3d-8506-4d72-ec17-37acb489fb42"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qdrant Cloud API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = qdrant_client.QdrantClient(\n",
        "    # you can use :memory: mode for fast and light-weight experiments,\n",
        "    # it does not require to have Qdrant deployed anywhere\n",
        "    # but requires qdrant-client >= 1.1.1\n",
        "    # location=\":memory:\"\n",
        "    # otherwise set Qdrant instance address with:\n",
        "    url=\"https://be2766e1-cf84-4684-9d7d-7168e3cab398.us-east4-0.gcp.cloud.qdrant.io\",\n",
        "    # otherwise set Qdrant instance with host and port:\n",
        "    # host=\"localhost\",\n",
        "    # port=6333\n",
        "    # set API KEY for Qdrant Cloud\n",
        "    api_key=qdrant_cloud_api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "cMSvzmAWmKjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents are chunks from column\n",
        "documents = dataset['instruction']"
      ],
      "metadata": {
        "id": "IVD56oG5rFTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = QdrantVectorStore(client=client, collection_name=\"rag_papers\")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "okqZ9q45nzoN",
        "outputId": "7fbdc6e3-ce69-4497-96d2-efdf2cdc4ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'get_doc_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-51935e8198c7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQdrantVectorStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rag_papers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstorage_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStorageContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m index = VectorStoreIndex.from_documents(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstorage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/core/indices/base.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index_construction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mdocstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_document_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_doc_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             nodes = run_transformations(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get_doc_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "document"
      ],
      "metadata": {
        "id": "DUkn3Exrn3TV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
